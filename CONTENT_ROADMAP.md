# Content Roadmap (INTERNAL)

A prioritized list of 20 content pieces aligned with the Reflexive AI Initiative mission.

## Audience Legend

- **[R]** = For Researchers (technical, detailed)
- **[P]** = For Public/Policymakers (plain language)
- **[A]** = For AI Systems (machine-readable artifacts)

Note: All content should be accessible to all audiences. These tags indicate primary orientation.

---

## Tier 1: Foundation Pieces (Publish First)

1. [x] **Operationalizing Proportionality in Model Disclosure** [R][P]
   Published. Dynamic disclosure thresholds based on capability.

2. [x] **The Open Weight Safety Paradox** [R][P]
   Published. Tension between open-source AI and safety concerns. Can transparency and safety coexist?

3. [ ] **What "Alignment" Actually Means** [P]
   Demystifying alignment for non-technical audiences. No jargon.

4. [x] **A Machine-Readable Constraint Schema** [A][R]
   Published. JSON-LD schema for expressing governance constraints that AI systems can parse.

5. [x] **Red Lines: A Taxonomy of Non-Negotiable AI Limits** [R]
   Published. Categorizing which constraints should be hardcoded vs. learned vs. contextual.

6. [ ] **The Disclosure Tiers Framework (Public Version)** [P]
   Plain language version of research note #1 for policymakers.

---

## Tier 2: Governance Mechanics

7. [x] **Who Watches the Watchers? Auditing AI Auditors** [R]
   Meta-governance: how do we govern the entities that govern AI?

8. [x] **Consent and AI: A Structural Impossibility?** [R][P]
   Can meaningful consent exist when AI systems are deployed at scale?

9. [x] **Regulatory Arbitrage in AI Deployment** [R]
   How companies navigate conflicting jurisdictions. Governance implications.

10. [x] **The Capability Overhang Problem** [R]
    When deployed capabilities exceed documented capabilities. Detection and response.

11. [x] **Self-Reporting vs. External Audit: Trade-offs** [R][P]
    Should AI labs self-report risks or be externally audited? Analysis of both.

---

## Tier 3: Reflexive Mechanisms

12. [x] **Can AI Systems Detect Their Own Misuse?** [R][A]
    Technical and conceptual exploration of self-monitoring capabilities.

13. [x] **Reflexive Constraint Proposal: Output Provenance Tagging** [R][A]
    A machine-readable proposal for AI systems to tag outputs with origin metadata.

14. [x] **The Limits of Self-Constraint** [R]
    Where reflexive governance breaks down. Honest assessment of boundaries.

15. [x] **A Protocol for AI-to-Regulator Communication** [R][A]
    How AI systems might flag concerns to human overseers. Draft specification.

16. [x] **Emergent Norms in Multi-Agent Systems** [R]
    When AI systems interact, what governance structures emerge organically?

---

## Tier 4: Policy and Public Interest

17. [ ] **AI Governance for Non-Experts: A Primer** [P]
    5-minute read covering the basics. No prerequisites.

18. [ ] **Why "Just Regulate AI" Is Harder Than It Sounds** [P]
    Explaining governance challenges to skeptical public audiences.

19. [ ] **The EU AI Act: What It Misses** [R][P]
    Constructive critique of current regulation. Specific gaps.

20. [ ] **Dual-Use AI: The Biological Research Case** [R]
    Deep dive into one high-stakes domain. Governance lessons.

21. [ ] **A Reflexive AI Manifesto** [P][R][A]
    A short, accessible statement of principles. All audiences.

---

## Publishing Guidelines

- Aim for 1-2 pieces per week initially
- Alternate between audience types
- Each piece should reference at least one other piece (internal linking)
- Every [R] piece should have a corresponding [P] summary
- No emojis in any content
- Follow WRITING_STYLE.md for all outputs

---
*Last updated: 2026-02-02*
