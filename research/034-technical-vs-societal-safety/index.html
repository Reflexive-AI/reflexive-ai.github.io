<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."><meta name="generator" content="Astro v5.17.1"><!-- Fonts --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Source+Serif+4:ital,wght@0,400;0,500;0,600;1,400&display=swap" rel="stylesheet"><!-- Favicon --><link rel="icon" type="image/png" href="/logo.png"><!-- Open Graph --><meta property="og:title" content="Technical Safety vs. Societal Safety: Different Problems | Reflexive AI Initiative"><meta property="og:description" content="Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."><meta property="og:type" content="website"><meta property="og:site_name" content="Reflexive AI Initiative"><title>Technical Safety vs. Societal Safety: Different Problems | Reflexive AI Initiative</title><link rel="stylesheet" href="/_astro/_slug_.BtA_RPCv.css"></head> <body>  <header class="header"> <div class="container header__inner"> <a href="/" class="header__brand"> <img src="/logo.png" alt="" class="header__logo"> <div> <span class="header__title">Reflexive AI</span> <span class="header__tagline">Governance · Reflexivity · Constraint</span> </div> </a> <nav class="header__nav"> <a href="/about" class="header__link">About</a> <a href="/research" class="header__link">Research</a> <a href="/contribute" class="header__link">Contribute</a> </nav> </div> </header>  <main>  <div class="article-layout"> <article class="article"> <header class="article__header"> <div class="article__meta"> <a href="/research" class="article__category">Governance Analysis</a><a href="/research" class="article__category">Public</a> <span>·</span> <time datetime="2026-01-16T00:00:00.000Z"> January 16, 2026 </time> </div> <h1 class="article__title">Technical Safety vs. Societal Safety: Different Problems</h1> <p class="article__excerpt">Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance.</p> </header>  <div class="toc-mobile"> <button class="toc-mobile__button" aria-expanded="false" onclick="this.setAttribute('aria-expanded', this.getAttribute('aria-expanded') === 'true' ? 'false' : 'true'); this.nextElementSibling.classList.toggle('open');"> <span>On This Page</span> <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"> <path d="M4 6L8 10L12 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </button> <nav class="toc-mobile__content"> <ul class="toc-mobile__list"> <li style="margin-left: 0px"> <a href="#two-meanings-of-ai-safety">Two Meanings of “AI Safety”</a> </li><li style="margin-left: 0px"> <a href="#the-technical-safety-problem">The Technical Safety Problem</a> </li><li style="margin-left: 12px"> <a href="#key-questions">Key Questions</a> </li><li style="margin-left: 12px"> <a href="#who-works-on-it">Who Works on It</a> </li><li style="margin-left: 12px"> <a href="#what-success-looks-like">What Success Looks Like</a> </li><li style="margin-left: 12px"> <a href="#limitations">Limitations</a> </li><li style="margin-left: 0px"> <a href="#the-societal-safety-problem">The Societal Safety Problem</a> </li><li style="margin-left: 12px"> <a href="#key-questions-1">Key Questions</a> </li><li style="margin-left: 12px"> <a href="#who-works-on-it-1">Who Works on It</a> </li><li style="margin-left: 12px"> <a href="#what-success-looks-like-1">What Success Looks Like</a> </li><li style="margin-left: 12px"> <a href="#limitations-1">Limitations</a> </li><li style="margin-left: 0px"> <a href="#how-conflation-causes-problems">How Conflation Causes Problems</a> </li><li style="margin-left: 12px"> <a href="#technical-researchers-claiming-to-solve-political-problems">Technical Researchers Claiming to Solve Political Problems</a> </li><li style="margin-left: 12px"> <a href="#policymakers-deferring-to-technical-solutions">Policymakers Deferring to Technical “Solutions”</a> </li><li style="margin-left: 12px"> <a href="#safety-teams-doing-ethics-and-vice-versa">Safety Teams Doing Ethics (and Vice Versa)</a> </li><li style="margin-left: 12px"> <a href="#missing-the-intersection">Missing the Intersection</a> </li><li style="margin-left: 0px"> <a href="#where-they-intersect">Where They Intersect</a> </li><li style="margin-left: 12px"> <a href="#power-and-control">Power and Control</a> </li><li style="margin-left: 12px"> <a href="#transparency-and-accountability">Transparency and Accountability</a> </li><li style="margin-left: 12px"> <a href="#capability-and-risk">Capability and Risk</a> </li><li style="margin-left: 12px"> <a href="#concentration">Concentration</a> </li><li style="margin-left: 0px"> <a href="#implications-for-governance">Implications for Governance</a> </li><li style="margin-left: 12px"> <a href="#separate-but-coordinated-oversight">Separate but Coordinated Oversight</a> </li><li style="margin-left: 12px"> <a href="#different-expertise-for-different-problems">Different Expertise for Different Problems</a> </li><li style="margin-left: 12px"> <a href="#multiple-interventions-at-multiple-points">Multiple Interventions at Multiple Points</a> </li><li style="margin-left: 12px"> <a href="#dont-let-one-substitute-for-the-other">Don’t Let One Substitute for the Other</a> </li><li style="margin-left: 0px"> <a href="#the-reflexive-connection">The Reflexive Connection</a> </li><li style="margin-left: 0px"> <a href="#further-reading">Further Reading</a> </li> </ul> </nav> </div> <div class="article__content">  <h2 id="two-meanings-of-ai-safety">Two Meanings of “AI Safety”</h2>
<p>When someone says they work on “AI safety,” they might mean two very different things.</p>
<p><strong>Technical safety</strong> focuses on making AI systems behave as intended. This includes preventing systems from producing harmful outputs, avoiding unintended behaviors, ensuring robustness to adversarial inputs, and building AI that reliably does what its operators want.</p>
<p><strong>Societal safety</strong> focuses on making AI development beneficial for humanity. This includes distributing AI’s benefits fairly, preventing concentration of power, protecting rights and democracy, and ensuring AI serves collective wellbeing rather than narrow interests.</p>
<p>These problems overlap but are fundamentally distinct. Conflating them leads to confused discourse and poor governance.</p>
<p>This analysis examines why the distinction matters, how conflation causes problems, and what it means for AI governance.</p>
<h2 id="the-technical-safety-problem">The Technical Safety Problem</h2>
<p>Technical safety is primarily an engineering challenge. The goal is reliable, predictable, controllable AI systems.</p>
<h3 id="key-questions">Key Questions</h3>
<ul>
<li>How do we specify what we want AI to do precisely enough that it actually does it? (The alignment problem we explored in <a href="/research/016-what-alignment-means/">what alignment actually means</a>.)</li>
<li>How do we prevent AI from producing harmful outputs like dangerous instructions or manipulative content?</li>
<li>How do we ensure AI systems behave consistently across different contexts and inputs?</li>
<li>How do we maintain human oversight as systems become more autonomous?</li>
</ul>
<h3 id="who-works-on-it">Who Works on It</h3>
<p>Technical safety is primarily worked on by ML researchers, often within AI labs. It’s a technical research agenda with papers, benchmarks, and experiments.</p>
<h3 id="what-success-looks-like">What Success Looks Like</h3>
<p>A technically safe AI system does what its operators intend, doesn’t produce harmful outputs, and can be reliably controlled and overseen. It works as designed.</p>
<h3 id="limitations">Limitations</h3>
<p>A technically safe system can still cause societal harm. A surveillance system that perfectly identifies dissidents is technically successful but socially harmful. A recommendation algorithm that efficiently maximizes engagement may be technically robust while undermining mental health or democratic discourse.</p>
<p>Technical safety is necessary but not sufficient for beneficial AI.</p>
<h2 id="the-societal-safety-problem">The Societal Safety Problem</h2>
<p>Societal safety is primarily a political and institutional challenge. The goal is AI development that benefits humanity broadly.</p>
<h3 id="key-questions-1">Key Questions</h3>
<ul>
<li>Who controls AI development and deployment decisions?</li>
<li>How are AI’s benefits and costs distributed across society?</li>
<li>What protections exist for those harmed by AI systems?</li>
<li>How do we prevent AI from being used to concentrate power or undermine democracy?</li>
<li>What institutional structures ensure AI serves collective interests?</li>
</ul>
<h3 id="who-works-on-it-1">Who Works on It</h3>
<p>Societal safety is worked on by a diverse community: policymakers, civil society advocates, legal scholars, ethicists, social scientists, and increasingly, technical researchers who recognize that technical solutions alone are insufficient.</p>
<h3 id="what-success-looks-like-1">What Success Looks Like</h3>
<p>Socially safe AI development produces broad benefits, includes robust protections for the vulnerable, maintains democratic governance, and prevents dangerous concentration of power. The technology serves humanity rather than the reverse.</p>
<h3 id="limitations-1">Limitations</h3>
<p>Societal safety measures cannot substitute for technical capabilities. No amount of governance can make an unreliable system reliable. And societal interventions often struggle to keep pace with technical change.</p>
<h2 id="how-conflation-causes-problems">How Conflation Causes Problems</h2>
<p>When we fail to distinguish technical and societal safety, several problems emerge.</p>
<h3 id="technical-researchers-claiming-to-solve-political-problems">Technical Researchers Claiming to Solve Political Problems</h3>
<p>Technical safety work is valuable, but it cannot answer questions like “who should AI serve?” or “how should benefits be distributed?” When technical researchers imply that alignment research solves AI governance, they overreach—and potentially distract from necessary political engagement.</p>
<p>This is related to what we identified in <a href="/research/029-honest-ai/">the honest AI problem</a>: technical solutions embed value judgments, but those judgments should be made explicitly through democratic processes, not implicitly through engineering decisions.</p>
<h3 id="policymakers-deferring-to-technical-solutions">Policymakers Deferring to Technical “Solutions”</h3>
<p>Policymakers sometimes treat AI governance as a technical problem that technical experts should solve. This abdicates democratic responsibility. Questions about acceptable risk, distribution of benefits, and fundamental rights are political questions requiring political decisions.</p>
<p>As we discussed in <a href="/research/033-policymaker-misconceptions/">what policymakers get wrong about AI risk</a>, technical expertise should inform governance but cannot substitute for democratic deliberation.</p>
<h3 id="safety-teams-doing-ethics-and-vice-versa">Safety Teams Doing Ethics (and Vice Versa)</h3>
<p>AI companies often conflate safety and ethics teams, giving them overlapping mandates. This can work well when problems genuinely overlap. But it can also mean that neither problem gets proper attention—ethics concerns get treated as engineering problems, while engineering problems get diluted by broader social considerations.</p>
<h3 id="missing-the-intersection">Missing the Intersection</h3>
<p>Sometimes problems require both technical and societal solutions. Algorithmic bias, for instance, has technical aspects (how systems amplify biases in training data) and societal aspects (which biases matter, what fairness requires, who decides). Addressing only one dimension leaves the problem unsolved.</p>
<h2 id="where-they-intersect">Where They Intersect</h2>
<p>The problems aren’t entirely separate. Several issues require addressing both dimensions.</p>
<h3 id="power-and-control">Power and Control</h3>
<p>Technical questions about AI controllability connect to societal questions about who controls AI. A system that is technically controllable but controlled by harmful actors isn’t safe in any meaningful sense.</p>
<h3 id="transparency-and-accountability">Transparency and Accountability</h3>
<p>Technical interpretability research connects to societal demands for accountability. But transparent systems can still be misused, and accountability requires institutional infrastructure beyond technical capabilities.</p>
<h3 id="capability-and-risk">Capability and Risk</h3>
<p>Technical capability levels have societal implications. More capable systems create greater societal risks if governance is inadequate. Technical choices about what capabilities to develop are thus also societal choices.</p>
<h3 id="concentration">Concentration</h3>
<p>Technical economies of scale in AI development connect to societal concerns about power concentration. The fact that frontier AI requires massive resources is both a technical reality and a governance challenge.</p>
<h2 id="implications-for-governance">Implications for Governance</h2>
<p>This distinction has practical implications for how we govern AI.</p>
<h3 id="separate-but-coordinated-oversight">Separate but Coordinated Oversight</h3>
<p>Technical safety and societal impact require different oversight mechanisms. Technical standards bodies should address system behavior. Democratic institutions should address deployment conditions, rights protections, and benefit distribution. These bodies should coordinate but not merge.</p>
<h3 id="different-expertise-for-different-problems">Different Expertise for Different Problems</h3>
<p>Technical safety requires ML expertise. Societal safety requires diverse expertise: law, economics, ethics, social science, domain knowledge. Governance structures should incorporate both without confusing them.</p>
<h3 id="multiple-interventions-at-multiple-points">Multiple Interventions at Multiple Points</h3>
<p>Technical interventions during development (testing, verification, monitoring) address technical safety. Societal interventions around deployment (licensing, auditing, liability) address societal safety. Both are necessary; neither is sufficient.</p>
<h3 id="dont-let-one-substitute-for-the-other">Don’t Let One Substitute for the Other</h3>
<p>AI developers shouldn’t claim that technical safety work addresses societal concerns. Policymakers shouldn’t assume that governance can substitute for technical robustness. Each problem requires its own solutions.</p>
<h2 id="the-reflexive-connection">The Reflexive Connection</h2>
<p>Our work on reflexive governance attempts to bridge these domains. The idea that <a href="/research/030-manifesto/">AI systems can participate in their own governance</a> is both a technical proposal (systems reporting constraints, explaining limits) and a societal one (making AI more legible and accountable).</p>
<p><a href="/research/003-machine-readable-constraint-schema/">Machine-readable constraints</a> are a technical artifact that serves societal purposes. <a href="/research/014-ai-regulator-protocol/">AI-to-regulator communication protocols</a> require both technical infrastructure and institutional capacity.</p>
<p>This integration is the distinctive contribution of reflexive approaches: recognizing that the technical and societal are distinct but must be connected, and building bridges that respect both domains.</p>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="/research/016-what-alignment-means/">What Alignment Actually Means</a></li>
<li><a href="/research/029-honest-ai/">The Honest AI Problem</a></li>
<li><a href="/research/030-manifesto/">A Reflexive AI Manifesto</a></li>
<li><a href="/research/033-policymaker-misconceptions/">What Policymakers Get Wrong About AI Risk</a></li>
</ul>  </div> <footer class="article__footer" style="margin-top: var(--space-12); padding-top: var(--space-6); border-top: 1px solid var(--color-border);"> <div class="article-card__meta"> <a href="/tags" class="article-card__tag">safety</a><a href="/tags" class="article-card__tag">alignment</a><a href="/tags" class="article-card__tag">governance</a><a href="/tags" class="article-card__tag">ethics</a> </div> </footer> </article>  <aside class="toc-sidebar" id="toc-sidebar"> <div class="toc-sidebar__inner"> <div class="toc-sidebar__header"> <span class="toc-sidebar__title">On This Page</span> <button class="toc-sidebar__toggle" aria-label="Toggle table of contents" onclick="document.getElementById('toc-sidebar').classList.toggle('collapsed');"> <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg"> <path d="M4 6L8 10L12 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </button> </div> <ul class="toc-sidebar__list"> <li style="margin-left: 0px"> <a href="#two-meanings-of-ai-safety">Two Meanings of “AI Safety”</a> </li><li style="margin-left: 0px"> <a href="#the-technical-safety-problem">The Technical Safety Problem</a> </li><li style="margin-left: 12px"> <a href="#key-questions">Key Questions</a> </li><li style="margin-left: 12px"> <a href="#who-works-on-it">Who Works on It</a> </li><li style="margin-left: 12px"> <a href="#what-success-looks-like">What Success Looks Like</a> </li><li style="margin-left: 12px"> <a href="#limitations">Limitations</a> </li><li style="margin-left: 0px"> <a href="#the-societal-safety-problem">The Societal Safety Problem</a> </li><li style="margin-left: 12px"> <a href="#key-questions-1">Key Questions</a> </li><li style="margin-left: 12px"> <a href="#who-works-on-it-1">Who Works on It</a> </li><li style="margin-left: 12px"> <a href="#what-success-looks-like-1">What Success Looks Like</a> </li><li style="margin-left: 12px"> <a href="#limitations-1">Limitations</a> </li><li style="margin-left: 0px"> <a href="#how-conflation-causes-problems">How Conflation Causes Problems</a> </li><li style="margin-left: 12px"> <a href="#technical-researchers-claiming-to-solve-political-problems">Technical Researchers Claiming to Solve Political Problems</a> </li><li style="margin-left: 12px"> <a href="#policymakers-deferring-to-technical-solutions">Policymakers Deferring to Technical “Solutions”</a> </li><li style="margin-left: 12px"> <a href="#safety-teams-doing-ethics-and-vice-versa">Safety Teams Doing Ethics (and Vice Versa)</a> </li><li style="margin-left: 12px"> <a href="#missing-the-intersection">Missing the Intersection</a> </li><li style="margin-left: 0px"> <a href="#where-they-intersect">Where They Intersect</a> </li><li style="margin-left: 12px"> <a href="#power-and-control">Power and Control</a> </li><li style="margin-left: 12px"> <a href="#transparency-and-accountability">Transparency and Accountability</a> </li><li style="margin-left: 12px"> <a href="#capability-and-risk">Capability and Risk</a> </li><li style="margin-left: 12px"> <a href="#concentration">Concentration</a> </li><li style="margin-left: 0px"> <a href="#implications-for-governance">Implications for Governance</a> </li><li style="margin-left: 12px"> <a href="#separate-but-coordinated-oversight">Separate but Coordinated Oversight</a> </li><li style="margin-left: 12px"> <a href="#different-expertise-for-different-problems">Different Expertise for Different Problems</a> </li><li style="margin-left: 12px"> <a href="#multiple-interventions-at-multiple-points">Multiple Interventions at Multiple Points</a> </li><li style="margin-left: 12px"> <a href="#dont-let-one-substitute-for-the-other">Don’t Let One Substitute for the Other</a> </li><li style="margin-left: 0px"> <a href="#the-reflexive-connection">The Reflexive Connection</a> </li><li style="margin-left: 0px"> <a href="#further-reading">Further Reading</a> </li> </ul> </div> </aside> </div> <div class="scroll-progress" id="scroll-progress"></div> <script type="module">const s=document.getElementById("scroll-progress");s&&window.addEventListener("scroll",()=>{const o=window.scrollY,e=document.documentElement.scrollHeight-window.innerHeight,t=o/e*100;s.style.width=`${t}%`});const r=document.querySelectorAll(".toc-sidebar__list a"),n=document.querySelectorAll(".article__content h2, .article__content h3");if(r.length>0&&n.length>0){const o=new IntersectionObserver(e=>{e.forEach(t=>{if(t.isIntersecting){r.forEach(i=>i.classList.remove("active"));const c=document.querySelector(`.toc-sidebar__list a[href="#${t.target.id}"]`);c&&c.classList.add("active")}})},{rootMargin:"-20% 0% -60% 0%"});n.forEach(e=>o.observe(e))}</script>  </main> <footer class="footer"> <div class="container footer__inner"> <a href="/" class="footer__brand"> <img src="/logo.png" alt="" class="footer__logo"> <span>Reflexive AI Initiative</span> </a> <div class="footer__links"> <a href="https://github.com/Reflexive-AI" class="footer__link" target="_blank" rel="noopener">GitHub</a> <a href="/research" class="footer__link">Research</a> <a href="/tags" class="footer__link">Tags</a> <a href="/contribute" class="footer__link">Contribute</a> </div> <div class="footer__meta"> <span>Maintained by <a href="https://eugenekondratov.eu" target="_blank" rel="noopener">Eugene Kondratov</a></span> <span>·</span> <a href="https://creativecommons.org/licenses/by/4.0/" class="footer__license" target="_blank" rel="noopener">CC BY 4.0</a> </div> </div> </footer> </body></html>