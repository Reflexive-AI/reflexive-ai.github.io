---
title: "The Small Actor Problem: How AI Regulation Shapes Market Structure"
excerpt: "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
date: 2026-02-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - regulation
  - market-structure
  - small-actors
  - competition
  - power-concentration
---

## The Hidden Cost of Compliance

Every regulation has compliance costs. Documentation requirements take lawyer time. Audits require auditor fees. Safety testing demands infrastructure. Certification involves bureaucracy.

For large organizations, these costs are manageable. A company with billions in revenue can absorb millions in compliance. The cost is a line item.

For small organizations, the same absolute costs may be prohibitive. A startup with ten employees cannot hire a compliance team. A nonprofit cannot afford enterprise-grade auditing.

This creates a structural bias in regulation: rules designed for large actors may screen out small ones, not because small actors are less safe but because they cannot navigate compliance.

## How This Manifests in AI

AI governance proposals often contain elements that disproportionately burden small actors.

### Compute Thresholds

Proposals that trigger oversight above certain compute levels seem neutral. They apply equally to anyone using that compute.

But compute correlates with organizational resources. Large actors can more easily meet oversight requirements because they already have legal, compliance, and policy infrastructure. Small actors crossing thresholds face new bureaucratic demands they may not be structured to meet.

### Documentation Requirements

Extensive documentation requirements, model cards, risk assessments, training data disclosures, take time to produce. Large organizations have technical writers and policy teams. Small organizations have engineers who would rather be building.

Even if documentation improves transparency, the burden falls unequally.

### Audit Requirements

[Third-party audits](/research/010-self-reporting-vs-audit/) are valuable but expensive. Audit fees that represent rounding errors for large companies may equal a small company's entire compliance budget.

If audits are mandatory, small actors pay proportionally more. If audits are optional but advantageous (creating trust signals), small actors are disadvantaged.

### Liability Frameworks

[Liability frameworks](/research/020-liability-frameworks/) that enable litigation over AI harms can disproportionately burden small actors. Large companies can afford legal teams and litigation insurance. Small companies may be bankrupted by a single lawsuit.

This does not mean liability is bad. It means liability risk is borne unequally.

### Standards Bodies

Participating in standards development takes time and expertise. Standards bodies that shape AI governance require sustained engagement. Large companies send professional standards delegates. Small companies cannot spare the personnel.

Standards end up reflecting the interests of those who participate: predominantly large organizations.

## Why This Matters

The small actor problem matters for several reasons.

**Competition and innovation.** Small actors are often innovation sources. Regulatory barriers that lock out small actors reduce competition and may slow beneficial innovation.

**Power concentration.** If regulation favors incumbents, it concentrates power further. Large AI companies may lobby for regulations they can navigate but competitors cannot. Regulation becomes a competitive weapon.

**Legitimacy.** Governance that entrenches existing power structures undermines its own legitimacy. Democratic governance should not systematically favor the powerful.

**Safety itself.** If only large actors can operate, safety depends on whether large actors are, in fact, safer. Large organizations have their own failure modes: bureaucracy, misaligned incentives, slower adaptation. Small actors may be more nimble in responding to emerging risks.

## The Counterargument

There is a counterargument: perhaps small actors should be burdened.

**Resource requirements are real.** Safety requires investment. If small actors cannot invest adequately, perhaps they should not operate. The costs of compliance may represent genuine costs of responsible operation.

**Larger actors have more capability.** If large actors can cause more harm due to greater capability, regulating them more heavily makes sense. Small actors may need less oversight because they pose less risk.

**Market incentives for scale.** If the AI market naturally favors scale (via compute costs, data requirements, network effects), regulation merely follows market structure.

These points have merit. But they do not eliminate the problem. They suggest the tradeoff must be managed, not that small actors do not matter.

## Design Principles for Equitable Regulation

Regulation can be designed to reduce disproportionate burden on small actors.

### Tiered Requirements

Requirements can scale with organizational size, revenue, or capability. Small actors might face lighter documentation requirements, less frequent audits, or simplified compliance pathways.

Risk-based tiering is a common approach: higher-risk applications face stricter requirements regardless of actor size, while lower-risk applications have reduced burden.

### Safe Harbors

Certain practices could qualify for safe harbor from liability or oversight. Small actors who follow established guidelines could receive protection without full compliance burden.

This creates pathways for responsible operation without requiring the infrastructure of large organizations.

### Subsidized Compliance

Governments or industry associations could subsidize compliance for small actors: providing free or reduced-cost audits, shared legal resources, or compliance tooling.

Public benefit requires public investment. If society benefits from small-actor participation, society should share compliance costs.

### Simplified Standards

Standards can be designed with small actors in mind. Default templates, automated checklists, and streamlined processes reduce the specialized expertise required.

Not every requirement needs to be complex. Simplicity is a design choice.

### Participation Structures

Standards bodies and governance processes can actively support small-actor participation. This might include reserved seats, remote participation options, or funding for travel and time.

If the table is dominated by large actors, restructure the table.

## The Regulatory Capture Risk

There is a mirror problem: regulations might be designed by large actors specifically to burden small competitors. This is regulatory capture.

Large AI companies participate intensively in policy development. They have lobbyists, policy teams, and relationships with regulators. When regulations emerge that happen to favor large actors, coincidence is not the only explanation.

Detecting capture is difficult. Regulations may genuinely serve safety goals while also serving incumbent interests. The challenge is ensuring that safety justifications are real rather than pretextual.

Safeguards include:

- **Diverse input.** Policy development should include small actors, civil society, and public interest groups, not just large companies.
- **Transparency.** Lobbying and policy influence should be documented and public.
- **Impact analysis.** Proposed regulations should include analysis of differential impacts by organization size.
- **Sunset and review.** Regulations should be reviewed for unintended effects and revised when differential burdens are identified.

## Conclusion

The small actor problem does not have a clean solution. Regulation inevitably has compliance costs. Those costs inevitably burden small actors more. Some burden may be appropriate if it reflects genuine safety investments.

But disproportionate burden that screens out small actors without safety benefit is pure cost. It concentrates power, reduces competition, and undermines governance legitimacy.

Designing AI governance with the small actor problem in mind requires active attention. Tiered requirements, safe harbors, subsidized compliance, and inclusive participation can mitigate the problem without eliminating necessary safety measures.

The question is whether governance designers prioritize this. Given that governance designers are often associated with large organizations, the answer is not guaranteed.

## Related Research

- [Regulatory Arbitrage in AI Deployment](/research/008-regulatory-arbitrage/)
- [Liability Frameworks for AI Harm](/research/020-liability-frameworks/)
- [Self-Reporting vs. External Audit: Trade-offs](/research/010-self-reporting-vs-audit/)
- [The Role of Standards Bodies in AI Governance](/research/039-standards-bodies/)
