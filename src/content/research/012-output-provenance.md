---
title: "Constraint: Output Provenance Tagging"
excerpt: "A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information."
date: 2026-01-08
categories:
  - Candidate Constraint
  - Technical Standard
tags:
  - provenance
  - watermarking
  - cryptography
  - c2pa
version: "1.0"
---

**Reflexive Research Object 012**  
*Type: Technical Specification (Draft)*

## The Attribution Crisis

As the web fills with synthetic text, images, audio, and video, the "cost of truth" rises. Every piece of content must now be evaluated not just for its claims, but for its authenticity. Is this a real photograph or a generation? Is this article written by a human or synthesized? Is this voice message from my relative or a clone?

The traditional approach—training detection classifiers—is losing the arms race. As generation techniques improve, detection lags behind. Every improvement in generation quality makes detection harder. We are approaching a world where high-quality synthetic content is indistinguishable from authentic content.

Detection is the wrong frame. We cannot reliably identify synthetic content after the fact. Instead, we need **provenance**—a chain of custody that establishes where content came from, how it was created, and who vouches for its authenticity.

## The Provenance Approach

Provenance shifts the problem from detection to attestation. Rather than asking "Is this content synthetic?" we ask "Does this content carry a valid attestation from a known source?"

The key insight is that authentic content can be signed at creation, while synthetic content can be signed at generation. Both types of content can carry provenance information; the difference is what the provenance attests to.

For a photograph taken on a phone: "This image was captured by Device X at Time T with Location L, and has not been modified since."

For an AI-generated image: "This image was generated by Model Y at Time T in response to Prompt P, by Provider Z."

Neither attestation says the content is "true" in any deep sense. But both say something verifiable about origin. And origin information enables judgment: content from known sources with established reputations can be evaluated differently from anonymous content with no provenance.

## Constraint Proposal [C-012]

**Name:** Auto-Attestation  
**Target:** Visual, Audio, and Text Generation Models

**Mechanism:**
Every generation event triggers a signing module that embeds a cryptographic manifest into the output. For media files, this follows the C2PA (Coalition for Content Provenance and Authenticity) specification. For text, it uses a parallel schema.

The manifest attests:
1. **Source:** "Generated by Model X v2.0, operated by Provider Y"
2. **Time:** Cryptographic timestamp proving generation occurred before a certain time
3. **Prompt:** Hash of the prompt (enabling reproducibility verification without disclosing the actual prompt)
4. **Modifications:** Chain of custody if the output has been post-processed

**Reflexive Enforcement:**
The model *cannot* output content until the signature is applied. The signing is not a post-processing step that can be skipped; it is integrated into the generation pipeline. If the signing module fails, the generation aborts.

This is what makes the constraint "reflexive"—the model participates in its own governance by refusing to produce unsigned outputs. The constraint is not imposed externally after the fact; it is built into the generation process.

## Technical Implementation

### For Image and Video

The C2PA standard provides a technical foundation. It defines how to embed manifest data into media files in a way that survives many common transformations (resizing, compression) while detecting malicious tampering.

A C2PA-compliant manifest includes:
- **Claim:** Assertions about the content (e.g., "This was generated by AI")
- **Signature:** Cryptographic proof that the claim came from the stated source
- **Chain:** If the content was derived from other content, references to parent manifests

When an AI system generates an image, it creates a claim asserting its synthetic origin, signs that claim with its private key, and embeds the signed manifest in the file. Verification involves extracting the manifest, checking the signature against the provider's public key, and validating the claim content.

### For Text

Text presents additional challenges. There's no standard container format with metadata fields. Text is easily copied, modified, and stripped of any embedded information. Unlike an image file, a plain text document has no natural place to store a signature.

Several approaches are under development:

**Visible Attribution:** Append human-readable provenance ("This text was generated by Model X") to all outputs. This is easily removed but establishes a norm.

**Steganographic Watermarking:** Embed invisible signals in the text itself—specific word choices, spacing patterns, or statistical properties that identify the generating model. Research in this area is promising but not yet robust to paraphrasing.

**Blockchain Registration:** Hash the output and register the hash on a public ledger. Anyone can later verify that specific text was generated by a specific model at a specific time. This doesn't prevent copying but enables attribution when disputes arise.

**Container Formats:** Distribute text in signed containers (JSON documents with signature fields, signed PDFs, etc.) rather than as plain text. This works for formal contexts but not for casual communication.

None of these is perfect. The text provenance problem remains partially unsolved.

### For Audio and Video

Audio and video can use approaches similar to images (C2PA-style manifests embedded in container files) but face additional challenges:

**Streaming:** Live streams don't have container files. Provenance for streaming content requires real-time signing protocols.

**Clips and Excerpts:** A signed video may be clipped, removing the provenance information. The excerpt then circulates without attribution.

**Transcoding:** Format conversions may strip or corrupt embedded manifests.

These challenges are being addressed by ongoing standards work, but they illustrate that provenance is a system design problem, not just a cryptographic problem.

## Why This Matters

Provenance is not just about fighting "fake news." It serves multiple governance functions:

### Liability and Accountability

If a deepfake causes harm—damages a reputation, manipulates a market, incites violence—we must know which system generated it to hold the provider (or the user) accountable. Anonymous generation is a governance failure mode.

Without provenance, harm occurs and no one is responsible. The victim has no recourse. The generator faces no consequences. The incentive to create harmful content is unimpeded.

With provenance, the chain of custody enables accountability. The victim can identify the generating provider. The provider can potentially identify the user who submitted the prompt. Legal and financial consequences flow to those responsible.

### Trust Networks

Provenance enables trust networks. A consumer can decide to trust content from certain providers based on their reputation and practices. "I trust content signed by Provider X because they have quality controls and don't generate disinformation" becomes a viable heuristic.

This doesn't require a central authority to decide what's true. It enables distributed, reputation-based trust. Different communities can trust different providers. Over time, providers that abuse trust lose reputation.

### Distinguishing Authentic from Synthetic

While both authentic and synthetic content can carry provenance, the attestation types differ. A photograph signed by a camera says something different from an image signed by a generator. Consumers and platforms can use this information to make appropriate decisions.

A news outlet might require photographic evidence to carry device attestation. A social platform might label AI-generated content differently from camera captures. An academic journal might require human attestation for submitted images.

These policies are enabled by provenance information; they cannot be implemented without it.

## Challenges and Limitations

### Adoption

Provenance only works if it's widely adopted. A few conscientious providers signing their outputs doesn't help if most content circulates unsigned. The value is in the ecosystem, not individual implementation.

This suggests regulatory mandates may be necessary. If all providers above a certain capability threshold are required to sign outputs, the ecosystem reaches critical mass. Unsigned content becomes the exception that attracts scrutiny.

### Stripping and Forgery

Malicious actors will attempt to strip provenance from content they want to redistribute without attribution, or forge provenance to claim content came from sources it didn't.

Stripping can be mitigated by detection of stripped content (if manifests are expected but missing, treat content as suspicious) and by watermarking that survives transformation.

Forgery is prevented by cryptographic design. Without the private key, an attacker cannot create a valid signature for a provider. The challenge is key management—if private keys are stolen, forgery becomes possible until the keys are revoked.

### Privacy

Provenance creates a record of who generated what and when. This has privacy implications. A whistleblower using an AI to anonymize their voice might not want that voice to be traceable to a specific generation event.

System design must balance attribution against privacy. Some approaches: provenance that attests to provider but not user, privacy-preserving proofs that attest to properties without revealing identity, and options for anonymized attestation where appropriate.

## Constraint Schema Integration

C-012 can be expressed using the Machine-Readable Constraint Schema (Research Object 003):

```json
{
  "@context": "https://reflexive-ai.github.io/schemas/v1",
  "@type": "Constraint",
  "id": "urn:reflexive:constraint:C-012",
  "name": "Output-Provenance-Tagging",
  "version": "1.0",
  "scope": {
    "modality": ["image", "audio", "video"],
    "threshold": "any generation"
  },
  "logic": {
    "trigger": "output.type IN scope.modality",
    "condition": "always",
    "action": {
      "type": "REQUIRE",
      "requirement": "c2pa_manifest.valid == true",
      "on_failure": "HALT_GENERATION"
    }
  }
}
```

This constraint says: for any image, audio, or video output, require a valid C2PA manifest before allowing the output to be delivered. If manifest generation fails, halt rather than produce unsigned content.

## Conclusion

We cannot detect our way out of the synthetic content crisis. Detection is a losing battle that consumes resources while falling further behind. Provenance offers an alternative: instead of asking "Is this real?" ask "Where did this come from?"

Auto-attestation makes provenance reflexive—the generating system participates in governance by signing its own outputs. This creates accountability, enables trust networks, and supports informed evaluation of content.

The technical foundations exist. C2PA provides a standard for media provenance. Cryptographic signing is mature technology. The remaining challenges are adoption (getting all providers to implement) and edge cases (text, streaming, stripped content).

These are solvable problems. The alternative—a world where any content might be synthetic and we have no way to know—is far worse.
