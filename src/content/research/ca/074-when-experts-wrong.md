---
title: "Quan els experts es van equivocar: humilitat epistèmica en les prediccions sobre IA"
excerpt: "Els experts en IA tenen un historial deficient de prediccions. Terminis, capacitats i impactes socials s'han jutjat malament de manera sistemàtica. Què ens hauria d'ensenyar això sobre la confiança en les afirmacions actuals?"
date: 2026-02-04
categories:
  - Governance Analysis
  - Public
tags:
  - predictions
  - epistemic-humility
  - history
  - uncertainty
  - expert-judgment
---

## Una història d'errors segurs

El 1965, Herbert Simon va predir que en vint anys les màquines serien capaces de fer qualsevol feina que un ésser humà pugui fer. El 1970, Marvin Minsky va dir que en tres a vuit anys tindríem una màquina amb la intel·ligència general d'un ésser humà mitjà.

No eren figures marginals fent conjectures esbojarrades. Eren fundadors del camp, parlant des d'una profunda experiència.

Es van equivocar.

La història de la IA està plena de prediccions segures que van fracassar estrepitosament. Comprendre aquesta història és essencial per avaluar les afirmacions actuals sobre terminis, capacitats i impactes de la IA. Aconsella humilitat epistèmica: no escepticisme, sinó cautela apropiada davant afirmacions segures.

## Categories de prediccions fallides

Els errors dels experts s'agrupen en patrons recognoscibles.

### Errors en els terminis de capacitats

L'error més comú: predir que les capacitats arribarien abans del que ho van fer.

- **1958:** Simon i Newell: "En deu anys un ordinador digital serà el campió mundial d'escacs."
  - *Realitat:* Deep Blue va vèncer Kasparov el 1997, gairebé 40 anys després.

- **Anys 70:** La comprensió del llenguatge natural estava "gairebé resolta."
  - *Realitat:* El rendiment acceptable no va arribar fins a la dècada del 2020.

- **Anys 80:** Els sistemes experts revolucionarien la medicina i el dret en pocs anys.
  - *Realitat:* La majoria dels sistemes experts van ser abandonats a principis dels anys 90.

Aquests fracassos comparteixen un patró: el progrés primerenc va crear un optimisme que es va extrapolar incorrectament. El primer 80% del rendiment es va aconseguir amb relativa facilitat; el 20% restant va trigar dècades.

### Errors en la predicció d'impactes

Els experts també van jutjar malament els impactes socials.

- **Anys 60-70:** L'automatització eliminaria en gran mesura el treball, requerint una reestructuració econòmica important.
  - *Realitat:* L'ocupació va continuar creixent durant dècades. L'automatització va desplaçar feines específiques mentre en creava d'altres.

- **Anys 90:** La IA continuaria sent una eina limitada sense una rellevància més àmplia.
  - *Realitat:* La IA es va convertir en central per a l'economia, la política i la vida quotidiana.

- **Anys 2000:** Internet havia arribat a un altiplà en gran mesura; millores incrementals per endavant.
  - *Realitat:* Els dispositius mòbils, les xarxes socials i la IA ho van transformar tot.

Les prediccions d'impacte van fallar en ambdues direccions: de vegades sobreestimant la disrupció, de vegades subestimant-la.

### Errors en la direcció de les capacitats

Els experts van jutjar malament quins problemes resultarien tractables.

- **La paradoxa de Moravec.** Els investigadors esperaven que el raonament d'alt nivell (escacs, matemàtiques) fos difícil i la percepció de baix nivell (visió, caminar) fos fàcil. Va succeir el contrari.

- **Llenguatge contra raonament.** Molts esperaven que els sistemes de raonament formal conduïssin a la comprensió del llenguatge. En canvi, els models de llenguatge estadístics van desenvolupar capacitats que van sorprendre els investigadors clàssics d'IA.

- **Aprenentatge profund.** Al llarg dels anys 2000, el corrent principal de la IA va descartar en gran mesura l'aprenentatge profund com una direcció poc prometedora. Ara domina el camp.

## Per què els experts s'equivoquen

El fracàs predictiu dels experts té causes sistemàtiques.

**Confusió de classe de referència.** Els experts extrapolen a partir del progrés recent. Però la classe de referència rellevant pot diferir del període d'extrapolació. El progrés s'accelera o desacelera de manera impredictible.

**Les parts fàcils arriben primer.** El progrés primerenc sol ser més fàcil que el progrés posterior. Confondre la dificultat de les fites inicials amb la dificultat de la tasca completa produeix excés de confiança.

**Distorsions d'incentius.** Les prediccions optimistes atrauen finançament, atenció i prestigi. Les prediccions pessimistes són avorrides. Els experts estan pressionats per ser interessants, no precisos.

**Ancoratge.** Un cop una predicció s'ha fet pública, els experts es resisteixen a actualitzar-la. Admetre l'error és costós. Les prediccions es converteixen en compromisos.

**Experiència estreta.** Els experts en IA coneixen la IA. Pot ser que no coneguin l'economia, la política o la sociologia prou bé com per predir impactes socials.

**Determinisme tecnològic.** Els experts solen assumir que la trajectòria de la tecnologia és fixa i que les forces externes s'ajusten. En realitat, forces socials, econòmiques i polítiques donen forma al desenvolupament tecnològic.

## El moment actual

El discurs actual sobre IA inclou prediccions segures en tot l'espectre.

**Prediccions catastrofistes.** Alguns experts prediuen que la IA transformadora planteja un risc existencial en anys o dècades. Expressen alta confiança.

**Prediccions desdenyoses.** Altres experts desestimen aquestes preocupacions per infundades, expressant alta confiança que els sistemes actuals estan lluny de ser perillosos.

**Prediccions de capacitats.** Les prediccions sobre quan la IA assolirà capacitats específiques (AGI, superintel·ligència) van des d'imminent fins a mai.

Donat el registre històric, què hauríem de fer amb aquestes afirmacions?

## Lliçons per a les prediccions actuals

La història de les prediccions fallides no ens diu quines de les prediccions actuals són errònies. Ens diu com gestionar les prediccions de manera adequada.

**Descomptar la confiança.** Quan un expert expressa alta confiança en una predicció sobre IA, descompteu-la. La calibració històrica és deficient. La incertesa apropiada és més àmplia del que els experts solen expressar.

**Considerar trajectòries.** Alguns pronosticadors tenen millors trajectòries que d'altres. Però fins i tot les bones trajectòries en un domini poden no transferir-se a situacions noves.

**Ponderar les cues.** Les prediccions sobre medianes (quan arribarà la capacitat X?) poden ser menys importants que les prediccions sobre les cues (quins són els millors i pitjors escenaris?). Els cignes negres importen.

**Distingir capacitat d'impacte.** Predir el que la IA pot fer i predir quin efecte tindrà són problemes diferents. El segon és més difícil.

**Observar les estructures d'incentius.** Qui es beneficia que es cregui la predicció? Les prediccions que s'alineen amb els interessos del predictor mereixen un escrutini addicional.

**Cercar el desacord.** Quan els experts discrepen, no tots poden tenir raó. Comprendre per què persisteix el desacord és més informatiu que prendre partit.

## Què significa això per a la governança

La humilitat epistèmica té implicacions per a la governança.

**No esperar la certesa.** Si esperem fins que les prediccions siguin segures, esperem massa. La governança ha d'avançar sota incertesa.

**Construir sistemes robustos.** Una governança dissenyada per a un únic futur previst pot fallar si aquesta predicció és errònia. La governança robusta funciona raonablement en una varietat d'escenaris.

**Preservar l'opcionalitat.** Comprometre's irrevocablement amb una predicció tanca opcions si aquesta predicció falla. La governança hauria de mantenir flexibilitat.

**Monitoritzar i adaptar.** Atès que les prediccions seran errònies, la governança ha de monitoritzar la realitat i adaptar-s'hi. Les regles estàtiques dissenyades per a condicions previstes divergiran de les condicions reals.

**Institucionalitzar la humilitat epistèmica.** Els processos de governança haurien d'incloure mecanismes per visibilitzar la incertesa, qüestionar les afirmacions segures i actualitzar amb base en l'evidència.

**Distingir nivells de certesa.** Algunes afirmacions mereixen més confiança que d'altres. "La IA pot generar text" és més segur que "la IA causarà l'extinció humana per al 2040". La governança hauria de rastrejar aquestes distincions.

## La paradoxa de la humilitat

La humilitat epistèmica comporta els seus propis riscos.

Si ens tornem massa incerts sobre les trajectòries de la IA, podem no actuar quan l'acció està justificada. La paràlisi també és un mode de fallada.

Si descartem les prediccions dels experts completament, perdem el valor de l'experiència. Els experts s'equivoquen sovint, però continuen sabent més que els no experts.

La solució no és ignorar les prediccions sinó gestionar-les adequadament: com a conjectures informades, no com a certeses; com a entrades per a les decisions, no com els seus determinants; com a hipòtesis que monitoritzar, no com a fets que assumir.

## Conclusió

La història de les prediccions sobre IA és una història d'errors segurs. Els fundadors del camp, treballant des d'una profunda experiència, van fer prediccions que van fallar per dècades.

Aquesta història no ens diu que les prediccions actuals siguin errònies. Ens diu que les prediccions actuals podrien ser errònies, fins i tot quan s'expressen amb confiança per part d'experts acreditats.

Per a la governança, això aconsella robustesa per sobre d'optimització, adaptació per sobre de planificació estàtica, i humilitat per sobre de certesa. Hauríem d'actuar malgrat la incertesa, però actuar de maneres que reconeguin que les nostres prediccions poden fallar.

La mateixa humilitat epistèmica s'aplica reflexivament: aquesta anàlisi també podria estar equivocada. Potser les prediccions actuals són més fiables que les passades. Potser el camp ha après. Mantenir fins i tot aquesta meta-afirmació amb la incertesa apropiada és el que la humilitat exigeix.

## Recerca relacionada

- [Why AI Safety Researchers Disagree: A Taxonomy of Worldviews](/research/064-ai-safety-worldviews/)
- [The Capability Overhang Problem](/research/009-capability-overhang/)
- [Simulating Governance: Using AI to Stress-Test AI Regulations](/research/072-simulating-governance/)
