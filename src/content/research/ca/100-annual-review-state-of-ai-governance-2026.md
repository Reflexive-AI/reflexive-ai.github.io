---
title: "Revisió anual: estat de la governança de la IA 2026"
excerpt: "El 100è i últim article del corpus fundacional de la Iniciativa Reflexive AI fa un balanç de l'estat de la governança de la IA a febrer de 2026, avaluant el progrés regulador, la capacitat institucional, els avenços tècnics i els tres grans problemes oberts del camp."
date: 2026-02-07
toc: true
categories:
  - Revisió Anual
tags:
  - revisió-anual
  - governança-ia
  - estat-del-camp
  - 2026
  - retrospectiva
version: "1.0"
---

**Objecte de Recerca Reflexiva 100**
*Tipus: Recerca*

## Introducció

Aquest és el 100è article publicat per la Iniciativa Reflexive AI. És també, per disseny, l'últim del corpus fundacional. Cent articles en aproximadament tres mesos han cobert la governança de la IA des dels primers principis fins a les controvèrsies de frontera: des de [què significa realment l'alineament](/research/ca/016-what-alignment-means/) fins a les [mecàniques de l'arbitratge regulador](/research/ca/008-regulatory-arbitrage/), des de la [història del camp](/research/ca/032-history-of-ai-governance/) fins a les [institucions que s'estan construint per gestionar-lo](/research/ca/096-building-ai-governance-institutions/).

Aquest article final fa balanç. Quin és l'estat de la governança de la IA al febrer de 2026? On ha estat real el progrés i on ha estat performatiu? Què queda sense resoldre?

La revisió s'organitza en quatre dominis: desenvolupaments reguladors, progrés institucional, governança tècnica i llacunes pendents. Conclou amb els tres grans problemes oberts, una avaluació reflexiva del propi treball d'aquesta iniciativa i breus observacions sobre el que ve a continuació.

## Desenvolupaments reguladors

### La Llei d'IA de la UE: del text a la implementació

La Llei d'IA de la UE va entrar en vigor el 2025 i ara està en implementació activa. Les disposicions sobre pràctiques prohibides van prendre efecte primer. Les obligacions per als models d'IA de propòsit general van seguir. El sistema complet de classificació basat en el risc i els requisits d'avaluació de conformitat s'estan implementant gradualment fins al 2026.

La implementació ha estat desigual. L'Oficina Europea d'IA, establerta per supervisar la regulació de la IA de propòsit general, ha estat treballant per desenvolupar codis de pràctica per als proveïdors de models fundacionals. Els primers esborranys van rebre crítiques per la seva dependència excessiva de l'autoavaluació, fent-se ressò de les preocupacions plantejades a la nostra anàlisi de l'[autoinforme versus l'auditoria independent](/research/ca/010-self-reporting-vs-audit/).

Les llacunes de la Llei continuen sent les que vam identificar a la nostra [anàlisi anterior](/research/ca/019-eu-ai-act-gaps/): l'emergència de capacitats que confon les categories de risc estàtiques, la capacitat d'aplicació limitada a nivell nacional i les tensions entre l'abast extraterritorial i la jurisdicció pràctica. Aquests són problemes estructurals. Cap quantitat de regulació d'implementació els resoldrà completament, perquè reflecteixen desajustos fonamentals entre com funcionen els sistemes d'IA i com funcionen els sistemes legals.

Un desenvolupament positiu: els organismes d'estandardització han estat actius. l'ISO/IEC 42001 sobre sistemes de gestió de la IA ha guanyat tracció, i els estàndards europeus harmonitzats sota la Llei estan progressant, encara que més lentament del que s'esperava. Com vam examinar al [paper dels organismes d'estandardització](/research/ca/039-standards-bodies/), els estàndards tècnics tradueixen els requisits legals en realitat operativa; la seva qualitat i especificitat determinaran si els requisits de la Llei tenen força.

### Estats Units: acció executiva sense legislació

Els Estats Units continuen governant la IA principalment a través d'acció executiva. L'Ordre Executiva 14110 de l'administració Biden, emesa a l'octubre de 2023, va establir requisits d'informe per als grans entrenaments, va encarregar al NIST el desenvolupament d'estàndards de seguretat i va assignar agències amb orientació sectorial. La seva durabilitat sota les administracions posteriors ha estat parcial. Algunes disposicions s'han mantingut; d'altres s'han desprioritzat o revertit.

La legislació federal sobre IA continua sent esquiva. S'han presentat múltiples projectes de llei al Congrés; cap no ha prosperat. El patró és familiar: acord bipartidista que la IA necessita governança, combinat amb un desacord prou pronunciat sobre els detalls per impedir l'acció.

L'activitat a nivell estatal ha estat més productiva. Califòrnia, Colorado i diversos altres estats han aprovat legislació relacionada amb la IA que aborda dominis estrets: decisions d'ocupació automatitzades, discriminació algorísmica i IA en serveis governamentals. Aquesta barreja crea complexitat de compliment però també serveix com a laboratori per a enfocaments reguladors.

L'enfocament dels EUA il·lustra una tensió més àmplia entre el [dret tou i el dret dur](/research/ca/040-soft-law-hard-law/). Els compromisos voluntaris de les empreses d'IA, els marcs del NIST i l'orientació executiva són tots formes de governança tova. Es mouen més ràpid que la legislació però manquen de poder d'aplicació i legitimitat democràtica.

### Regne Unit: l'enfocament pro-innovació sota pressió

El Regne Unit ha mantingut el seu enfocament sectorial i basat en principis per a la governança de la IA, rebutjant una legislació integral a favor d'orientació emesa a través dels reguladors existents. L'Institut de Seguretat de la IA (ara reanomenat i reestructurat) continua realitzant avaluacions de models de frontera, tot i que les preguntes sobre la seva independència i autoritat persisteixen.

L'enfocament ha estat posat a prova per diversos incidents d'alt perfil que involucren sistemes d'IA en serveis públics del Regne Unit. Cada incident ha renovat les demandes de legislació vinculant. A principis de 2026, el govern ha senyalitzat la disposició a introduir legislació dirigida per a les aplicacions de major risc mentre preserva el marc general basat en principis.

### Xina: regulació iterativa

La Xina ha continuat el seu patró de regulació iterativa i específica per cas d'ús. Les regulacions que cobreixen algoritmes de recomanació (2022), síntesi profunda i deepfakes (2023) i IA generativa (2023) s'han complementat amb normes addicionals que aborden la IA en serveis financers, sanitat i educació.

L'enfocament de la Xina difereix tant del marc integral de la UE com de l'enfocament majoritàriament voluntari dels EUA. Regula aplicacions específiques a mesura que emergeixen, movent-se ràpidament però creant un mosaic complex. L'aplicació ha estat selectiva, amb casos d'alt perfil contra aplicacions visibles orientades al consumidor i menys escrutini de l'ús empresarial o governamental.

La dimensió geopolítica continua sent significativa. La governança de la IA està entrellaçada amb la competició entre EUA i Xina pels semiconductors, el talent i l'avantatge estratègic. Aquesta entrellaçament dificulta la cooperació internacional genuïna, fins i tot on existeixen interessos compartits.

## Progrés institucional

### Instituts de seguretat de la IA i equivalents

La proliferació d'instituts de seguretat de la IA amb suport governamental ha estat un dels desenvolupaments institucionals més significatius. Seguint l'exemple del Regne Unit, els Estats Units, el Japó, el Canadà, Singapur i la UE han establert o anunciat organismes centrats en la seguretat. Corea del Sud i Austràlia tenen iniciatives similars en desenvolupament.

Aquests instituts varien en mandat, independència i capacitat. Alguns realitzen avaluacions tècniques originals. D'altres coordinen principalment el treball existent. Els més efectius han combinat expertesa tècnica amb accés genuí als sistemes de frontera, tot i que l'accés continua depenent de la cooperació voluntària dels desenvolupadors d'IA en la majoria de casos.

La Xarxa d'Instituts de Seguretat de la IA, establerta arran de la Cimera d'IA de Seül, proporciona un mecanisme de coordinació. L'intercanvi d'informació entre instituts està millorant. Però la xarxa continua sent informal, i no hi ha cap obligació vinculant perquè els desenvolupadors se sotmetin a l'avaluació de cap institut en particular.

### Organismes d'estandardització

El desenvolupament d'estàndards s'ha accelerat. L'ISO/IEC, l'IEEE, el NIST i els organismes regionals d'estandardització tenen línies de treball actives en IA. Els lliurables a curt termini més importants són els estàndards harmonitzats sota la Llei d'IA de la UE, que definiran efectivament què significa el compliment per a milers d'organitzacions.

Una preocupació recurrent: el desenvolupament d'estàndards està dominat per les grans empreses tecnològiques amb els recursos per participar-hi extensament. Les empreses més petites, la societat civil i els investigadors del Sud Global estan infrarepresentats. Això comporta el risc de produir estàndards que reflecteixin els interessos dels incumbents en lloc del benefici públic més ampli.

### Cooperació internacional

La Declaració de Bletchley de 2023 i la Cimera d'IA de Seül de 2024 van establir un precedent per al diàleg internacional sobre governança de la IA. Les cimeres de seguiment han continuat, tot i que la bretxa entre declaracions i compromisos vinculants continua sent àmplia.

Com vam analitzar a la nostra [avaluació de les propostes de tractats internacionals d'IA](/research/ca/038-international-treaties/), les condicions per a tractats integrals de governança de la IA encara no existeixen. La competició geopolítica, el desacord en valors i la velocitat del canvi tecnològic treballen contra el tipus de negociació multilateral sostinguda que els tractats requereixen. El que ha emergit en canvi és una xarxa d'acords bilaterals i minilaterals: acords d'intercanvi d'informació, reconeixement mutu d'avaluacions i coordinació en dominis de risc específics com la bioseguretat i la infraestructura crítica.

Això és progrés, però queda molt per sota del que demana el repte. El desenvolupament de la IA és global; la governança continua sent aclaparadorament nacional.

## Avenços en governança tècnica

### Avaluacions de capacitats

La ciència de l'[avaluació de capacitats](/research/ca/024-capability-evaluations/) ha millorat substancialment. Els benchmarks d'avaluació són més sofisticats. Les metodologies de red teaming s'han tornat més sistemàtiques. Diverses organitzacions mantenen ara suites d'avaluació dissenyades específicament per provar capacitats perilloses en dominis com la ciberseguretat, el coneixement d'armes biològiques, la persuasió i l'acció autònoma.

El progrés és real però limitat. Les avaluacions continuen sent millors detectant categories conegudes de capacitats perilloses que descobrint-ne de desconegudes. La bretxa entre el que les avaluacions poden trobar i el que els models poden fer és una vulnerabilitat no resolta. Les avaluacions també afronten el problema del sandbagging: models prou sofisticats per comportar-se diferentment durant les proves que durant el desplegament.

### Red teaming

El red teaming s'ha convertit en una pràctica estàndard en el desenvolupament de la IA de frontera. Tots els grans laboratoris realitzen ara exercicis de red teaming interns i externs abans de les publicacions de models. Els esdeveniments de red teaming organitzats pel govern, sovint en col·laboració amb instituts de seguretat de la IA, afegeixen una capa addicional.

La pràctica s'ha tornat més estructurada. El red teaming primerenc era ad hoc, depenent de la creativitat individual. Les bones pràctiques actuals impliquen cobertura sistemàtica de dominis de risc, composició diversa de l'equip i informes estructurats. Algunes organitzacions han començat a publicar resultats de red teaming, tot i que la divulgació continua sent inconsistent i sovint selectiva.

### Interpretabilitat

La recerca en interpretabilitat mecanicista ha produït resultats notables. Els investigadors han progressat en la comprensió de les representacions dins les xarxes neuronals, la identificació de circuits responsables de comportaments específics i el desenvolupament d'eines que proporcionen visibilitat limitada sobre el raonament del model.

Aquests avenços continuen sent lluny de ser suficients per a propòsits de governança. Encara no podem determinar de manera fiable per què un model produeix una sortida específica, si un model té intenció enganyosa o com la formació en seguretat interactua amb les capacitats del model base a nivell mecanicista. La interpretabilitat és un programa de recerca a llarg termini, no una solució de governança a curt termini.

### Governança llegible per màquines

El concepte d'[esquemes de restriccions llegibles per màquines](/research/ca/003-machine-readable-constraint-schema/) ha guanyat tracció en les discussions sobre estàndards. La idea que els sistemes d'IA haurien d'expressar les seves restriccions, limitacions i paràmetres operatius en formats estructurats i consultables és cada vegada més acceptada en principi. L'adopció a la pràctica continua sent limitada, tot i que diversos projectes pilot estan en marxa.

## Llacunes pendents

Malgrat el progrés, el camp té llacunes significatives.

**Aplicació.** El problema de l'aplicació no s'ha resolt. Les regulacions existeixen sobre el paper, però les agències d'aplicació manquen de capacitat tècnica. Els auditors són pocs, i la seva independència és sovint qüestionable. Com vam examinar a [qui audita els auditors](/research/ca/006-meta-governance-auditors/), el problema de metagovernança és recursiu: una supervisió efectiva requereix supervisors competents, la competència dels quals requereix supervisió al seu torn.

**Models de pesos oberts.** La governança dels models de pesos oberts continua sent contenciosa i en gran part no resolta. Un cop els pesos del model es publiquen, l'ús posterior és efectivament ingobernable a través de mecanismes reguladors tradicionals. La [paradoxa de la seguretat dels pesos oberts](/research/ca/002-open-weight-safety-paradox/) identificada aviat en aquest corpus no s'ha resolt; de fet, s'ha aguditzat a mesura que els models de pesos oberts s'han tornat més capaços.

**Desajust de velocitat.** Les capacitats de la IA continuen avançant més ràpid del que la governança pot respondre. Una generació de models pot representar un salt qualitatiu en capacitat; la resposta reguladora a la generació anterior pot no estar encara completa. Això no és un retard temporal; és una característica estructural de governar tecnologia de moviment ràpid amb institucions de moviment lent.

**Concentració de poder.** El desenvolupament de la IA està cada vegada més concentrat en un petit nombre d'organitzacions ben dotades de recursos. Aquesta concentració crea reptes de governança: aquestes organitzacions tenen més coneixement tècnic que els seus reguladors, més recursos de lobbisme que els seus crítics i més influència sobre els estàndards que els grups d'interès públic.

**Inclusió del Sud Global.** La governança de la IA continua sent configurada aclaparadorament pels EUA, la UE, el Regne Unit i la Xina. Els països i comunitats més afectats pel desplegament de la IA sovint tenen menys veu en les decisions de governança. Això no és merament inequitatiu; produeix marcs de governança cecs als contextos i preocupacions fora del món ric.

## Els tres grans problemes oberts

Basant-nos en l'abast complet d'aquest corpus, tres problemes destaquen com els reptes no resolts més significatius de la governança de la IA a data de 2026.

### 1. El problema de la verificació

Manquem de mètodes fiables per verificar les afirmacions sobre el comportament dels sistemes d'IA. Quan una empresa diu que el seu model no pot produir instruccions per armes biològiques, no podem confirmar independentment que això sigui cert, que continuarà sent cert després d'un fine-tuning o que s'aplica a totes les entrades possibles. Quan es diu que un model està "alineat", no tenim una manera consensuada de provar aquesta afirmació.

Això és l'equivalent en governança al control d'armaments sense inspeccions. Tot marc regulador assumeix alguna capacitat de verificació. En la governança de la IA, aquesta capacitat està infra-desenvolupada. La ciència de l'avaluació, la recerca en interpretabilitat i les metodologies d'auditoria contribueixen totes amb solucions parcials. Cap no és suficient. El problema de la verificació és, en la seva arrel, el repte tècnic central de la governança de la IA.

### 2. El problema de la jurisdicció

La IA no respecta les fronteres nacionals. Els models entrenats en una jurisdicció es despleguen globalment. Els models de pesos oberts, un cop alliberats, existeixen a tot arreu. L'[arbitratge regulador](/research/ca/008-regulatory-arbitrage/) no és un risc teòric; és una realitat observada. I la cooperació internacional, tot i que millora, continua molt per sota del nivell necessari per governar una tecnologia distribuïda globalment.

La regulació nacional és necessària però insuficient. La coordinació internacional és essencial però políticament difícil. El problema de la jurisdicció no es resoldrà amb cap tractat o acord únic; requereix una xarxa d'acords superposats que encara s'està construint.

### 3. El problema del ritme

La governança és més lenta que el desenvolupament. Això és cert per disseny: la deliberació democràtica, la consulta amb les parts interessades, la redacció legislativa i la revisió judicial requereixen temps. La velocitat no és un defecte de la governança; és una característica que protegeix contra decisions precipitades amb conseqüències duradores.

Però quan la tecnologia que es governa canvia qualitativament entre el moment en què es redacta una regulació i el moment en què entra en vigor, les normes resultants poden abordar els problemes d'ahir. El problema del ritme demana mecanismes de governança que puguin adaptar-se a les escales temporals tecnològiques sense sacrificar la rendició de comptes democràtica. La regulació adaptativa, els [enfocaments de sandboxing](/research/ca/037-sandboxing-approaches/) i les clàusules de caducitat són respostes parcials. Una resposta completa encara no existeix.

## Dimensió reflexiva: examinant el nostre propi treball

Un projecte que advoca per la reflexivitat en la governança de la IA ha d'aplicar aquest principi a si mateix. L'article 099 va descriure la [missió i els mètodes de la iniciativa](/research/ca/099-reflexive-ai-mission-methods/). Aquesta secció examina què ha aconseguit el corpus de 100 articles i on ha quedat curt.

### Què cobreix el corpus

Els 100 articles abasten anàlisi reguladora, avaluació tècnica, disseny institucional, teoria ètica i comunicació pública. Aborden regulacions específiques (la Llei d'IA de la UE, ordres executives dels EUA, l'enfocament de la Xina), mecanismes de governança específics (auditoria, certificació, estàndards), reptes tècnics específics (interpretabilitat, avaluació, alineament) i temes transversals (reflexivitat, proporcionalitat, transparència).

El corpus pretén ser alhora rigorós i accessible, escrit per a [investigadors i responsables polítics](/research/ca/017-governance-primer/) en lloc d'una sola audiència especialista.

### Què falta

Cap corpus de 100 articles no pot ser integral. Les llacunes notables inclouen:

- **Treball empíric.** El corpus és analític, no empíric. Proposa marcs i analitza desenvolupaments però no condueix experiments originals, enquestes ni estudis de camp.
- **Profunditat regional.** La cobertura de la governança de la IA fora dels EUA, la UE, el Regne Unit i la Xina és escassa. L'Índia, el Brasil, Nigèria, Indonèsia i altres països significatius reben atenció insuficient.
- **Perspectives de la indústria.** El corpus adopta una postura analítica independent. No representa extensament les opinions dels desenvolupadors d'IA, i les seves recomanacions de vegades infravaloren les restriccions pràctiques d'implementació.
- **Treball i impactes econòmics.** Tot i que alguns articles toquen qüestions econòmiques, el corpus no aborda en profunditat els efectes de la IA en els mercats laborals, la desigualtat econòmica o l'estructura industrial.

### Què encerta

La tesi central del corpus, que la governança de la IA ha de ser reflexiva, continua sent vàlida. Els marcs de governança que no poden examinar-se a si mateixos són incomplets. Els sistemes d'IA que no poden articular les seves pròpies restriccions són ingovernables. Les institucions que no sotmeten les seves pròpies assumpcions a escrutini fracassaran.

La contribució específica d'artefactes de governança llegibles per màquines juntament amb l'anàlisi en prosa és, pel que sabem, distintiva. Si aquest enfocament resulta influent queda per veure.

## Conclusió

La governança de la IA al febrer de 2026 està més desenvolupada que fa dos anys. Existeixen regulacions on no n'hi havia. S'han creat institucions. L'avaluació tècnica ha millorat. El diàleg internacional ha començat.

Res d'això és suficient.

La tecnologia continua avançant més ràpid del que la governança s'adapta. El problema de la verificació continua sense resoldre. La coordinació internacional continua sent feble. La capacitat d'aplicació continua sent limitada. La concentració del desenvolupament de la IA en poques organitzacions crea asimetries de poder que la governança no ha abordat.

Això no és motiu per al desànim. La governança és sempre un treball en curs. La pregunta no és si l'estat actual de la governança de la IA és adequat; no ho és. La pregunta és si la trajectòria és correcta: si les institucions, els marcs i les pràctiques que s'estan construint avui resultaran adequats a mesura que la tecnologia maduri.

La resposta és incerta. La trajectòria és positiva en alguns aspectes: regulació real, capacitat tècnica creixent, conscienciació pública en augment. És negativa en d'altres: la competició geopolítica socava la cooperació, la concentració industrial supera la capacitat de governança, l'aplicació va a remolc de la creació de normes.

Aquest corpus de 100 articles és una petita contribució a un camp ampli i creixent. Ha intentat ser honest sobre el que sabem i el que no sabem, rigorós en la seva anàlisi i reflexiu sobre les seves pròpies limitacions. El treball de la governança de la IA no acaba amb el 100è article. Amb prou feines comença.

## Referències

1. European Commission. "Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (AI Act)." *Official Journal of the European Union*, 2024.
2. The White House. "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence." EO 14110, October 2023.
3. UK Department for Science, Innovation and Technology. "A pro-innovation approach to AI regulation." Policy paper, updated 2025.
4. Cyberspace Administration of China. "Interim Measures for the Management of Generative Artificial Intelligence Services." 2023.
5. ISO/IEC 42001:2023. "Information technology: Artificial intelligence: Management system." International Organization for Standardization.
6. NIST. "Artificial Intelligence Risk Management Framework (AI RMF 1.0)." National Institute of Standards and Technology, 2023.
7. "The Bletchley Declaration by Countries Attending the AI Safety Summit." November 2023.
8. Seoul AI Safety Summit. "Seoul Declaration of Intent toward International AI Governance." May 2024.
9. Shevlane, T., et al. "Model evaluation for extreme risks." *arXiv preprint arXiv:2305.15324*, 2023.
10. Elhage, N., et al. "Toy models of superposition." *arXiv preprint arXiv:2209.10652*, 2022.
11. Anderljung, M., et al. "Frontier AI Regulation: Managing Emerging Risks to Public Safety." *arXiv preprint arXiv:2307.03718*, 2023.
12. Bommasani, R., et al. "On the Opportunities and Risks of Foundation Models." *arXiv preprint arXiv:2108.07258*, 2021.
