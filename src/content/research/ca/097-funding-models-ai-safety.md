---
title: "Models de finançament per a la recerca en seguretat de la IA"
excerpt: "La recerca en seguretat de la IA està crònicament infrafinançada en relació amb el treball en capacitats. Aquest article examina les fonts de finançament actuals, compara models alternatius des de premis fins a impostos sobre la computació, i proposa mecanismes concrets per tancar la bretxa."
date: 2026-02-07
toc: true
categories:
  - Anàlisi de Governança
tags:
  - finançament
  - seguretat-ia
  - política-de-recerca
  - filantropia
  - finançament-públic
version: "1.0"
---

**Objecte de Recerca Reflexiva 097**
*Tipus: Recerca*

## Introducció

La recerca en seguretat de la IA està infrafinançada. No en termes absoluts: centenars de milions de dòlars flueixen cap al treball relacionat amb la seguretat anualment. Però en relació amb l'escala de la inversió en capacitats, el finançament de la seguretat continua sent un error d'arrodoniment. El 2024, la inversió global en IA va superar els 100.000 milions de dòlars. Les estimacions de la despesa dedicada a la recerca en seguretat oscil·laven entre 500 milions i 1.500 milions de dòlars, depenent de què es considera "seguretat" (Epoch AI, 2024). Això és aproximadament un cèntim de despesa en seguretat per cada dòlar d'inversió en capacitats.

Aquest desequilibri importa. Com vam examinar a [L'economia de la seguretat de la IA](/research/ca/078-economics-ai-safety/), qui paga per la seguretat determina quin treball de seguretat es fa. Les estructures de finançament determinen quins problemes reben atenció, quins investigadors poden permetre's treballar en seguretat i si la seguretat manté el ritme de les capacitats.

La pregunta no és si la recerca en seguretat de la IA hauria de rebre més finançament. Hauria de rebre'n. La pregunta és com. Diferents models de finançament creen diferents estructures d'incentius, atrauen investigadors diferents i produeixen tipus de treball diferents. Alguns models fomenten la independència. Altres fomenten la rellevància. Alguns generen programes sostinguts. Altres produeixen ràfegues d'esforç. Encertar el mecanisme importa tant com encertar la quantitat.

Aquest article cartografia el panorama de finançament actual, avalua models de finançament alternatius i proposa mecanismes específics per tancar la bretxa entre la inversió en seguretat i la inversió en capacitats.

## El panorama de finançament actual

### Finançament filantròpic

La filantropia privada ha estat la columna vertebral de la recerca en seguretat de la IA durant més d'una dècada. Open Philanthropy ha atorgat més de 300 milions de dòlars a la seguretat i la governança de la IA des de 2016, convertint-se en el finançador individual més gran del camp fora dels laboratoris corporatius (Open Philanthropy, 2025). El Survival and Flourishing Fund ha distribuït desenes de milions més. Donants individuals, molts d'ells emprenedors tecnològics, contribueixen a través de donacions directes i fons assessorats per donants.

El finançament filantròpic va permetre que el camp existís. Abans que els governs o les corporacions es prenguessin seriosament la seguretat de la IA, les fundacions van finançar els investigadors, les organitzacions i les institucions que van definir l'agenda de recerca. Organitzacions com MIRI, el Center for AI Safety, el Centre for the Governance of AI i Redwood Research deuen la seva existència al suport filantròpic.

Però el finançament filantròpic té limitacions estructurals. Les prioritats dels donants canvien. Un gir estratègic d'una sola fundació pot desestabilitzar tota una comunitat de recerca. El finançament filantròpic també reflecteix les visions del món dels donants: durant anys, les preocupacions sobre el risc existencial van dominar el finançament filantròpic de la seguretat de la IA, mentre el treball a curt termini sobre equitat, biaix i rendició de comptes va rebre menys atenció d'aquestes fonts. A més, el capital filantròpic és finit d'una manera que el finançament governamental o corporatiu no ho és.

### Finançament governamental

La inversió governamental en seguretat de la IA creix ràpidament però va començar des d'una base baixa. L'Institut Nacional d'Estàndards i Tecnologia dels EUA (NIST) va rebre aproximadament 10 milions de dòlars anuals per al treball en seguretat de la IA fins al 2024, abans de rebre finançament ampliat. L'Institut de Seguretat de la IA del Regne Unit (AISI), establert el 2023, opera amb un pressupost d'aproximadament 100 milions de dòlars repartits en diversos anys. La UE va assignar porcions del seu programa Horizon Europe a la recerca en IA de confiança. El Japó, el Canadà i Corea del Sud també han anunciat iniciatives centrades en seguretat.

El finançament governamental aporta escala, legitimitat i durabilitat. Les beques de recerca públiques creen programes a llarg termini que sobreviuen a les decisions de donants individuals. El finançament governamental també senyalitza prioritat política, cosa que influeix en el comportament corporatiu i la coordinació internacional.

No obstant això, el finançament governamental es mou lentament. Els cicles de beques triguen mesos. Els requisits burocràtics consumeixen temps dels investigadors. Les prioritats polítiques canvien amb les eleccions. El finançament sovint afavoreix el treball aplicat amb rellevància política clara per sobre de la recerca fonamental. I els pressupostos governamentals per a la seguretat de la IA continuen sent petits en comparació amb la despesa en IA de defensa i intel·ligència. El Departament de Defensa dels EUA va gastar més de 1.800 milions de dòlars en IA l'any fiscal 2024 (Congressional Research Service, 2024); la seva despesa explícitament centrada en seguretat en va ser una fracció.

### Finançament corporatiu

Les empreses d'IA més grans finançen equips interns de seguretat. Anthropic, la missió declarada de la qual se centra en la seguretat de la IA, dedica una part significativa del seu pressupost de recerca a l'alineament i la interpretabilitat. Google DeepMind manté equips de seguretat i alineament. La iniciativa de Superalineament d'OpenAI, anunciada el 2023 amb una promesa del 20% de la computació, va produir treball notable abans que la seva direcció marxés el 2024. Meta, Microsoft i altres finançen recerca en seguretat a escales variables.

El finançament corporatiu de seguretat és la categoria més gran en termes absoluts, però és difícil de mesurar. Les empreses no reporten de manera consistent la despesa en seguretat per separat de la recerca general. Què compta com a "seguretat" versus "capacitats" és sovint ambigu: la recerca en interpretabilitat, per exemple, millora tant la comprensió de la seguretat com el rendiment del model.

El finançament corporatiu també crea dependència. Com vam assenyalar a [L'economia de la seguretat de la IA](/research/ca/078-economics-ai-safety/), els investigadors finançats per empreses senten pressió per alinear el seu treball amb els interessos comercials. La recerca en seguretat que alenteix el desplegament o imposa costos rep menys suport que la recerca en seguretat que facilita el desplegament. Els equips interns de seguretat afronten dinàmiques polítiques: massa fricció amb els equips de producte redueix la seva influència.

### Finançament acadèmic

Les universitats finançen la recerca en seguretat de la IA a través dels canals acadèmics estàndard: càtedres, suport a estudiants de doctorat i beques de recerca. Existeixen centres dedicats a diverses institucions, incloent-hi el Center for Human-Compatible AI a UC Berkeley i el Centre for the Study of Existential Risk a Cambridge.

El finançament acadèmic és modest en termes absoluts però proporciona independència, horitzons temporals llargs i connexió amb la comunitat científica més àmplia. Les seves limitacions són les limitacions estàndard de la recerca acadèmica: incentius de publicació que premien la novetat per sobre de l'impacte pràctic, terminis de tenure que descoratgen el treball aplicat, i pressupostos que no poden suportar els experiments intensius en computació que requereix el treball de seguretat de frontera.

## Models de finançament alternatius

La barreja actual de filantropia, beques governamentals i pressupostos corporatius ha portat el camp al seu estat actual. Escalar la recerca en seguretat de la IA per igualar el creixement de les capacitats requereix mecanismes addicionals.

### Despesa obligatòria en seguretat

L'enfocament més directe: exigir a les empreses d'IA que gastin un percentatge fix dels ingressos o del pressupost d'I+D en recerca de seguretat. Això reflecteix els requisits d'altres indústries. Les empreses farmacèutiques han de finançar la farmacovigilància. Els operadors nuclears finançen programes de seguretat. Les institucions financeres gasten quantitats obligatòries en gestió del compliment i del risc.

Una norma de despesa obligatòria en seguretat té avantatges clars. S'escala automàticament amb el creixement de la indústria. Assegura que les empreses que creen risc assumeixin el cost de gestionar-lo. Crea un sòl per sota del qual la inversió en seguretat no pot caure.

Els reptes són la definició i l'aplicació. Què compta com a despesa en seguretat? Si la recerca en interpretabilitat millora tant la seguretat com les capacitats, compta? Les empreses tindrien incentius per reetiquetar treball existent com a recerca en seguretat. Qualsevol norma de despesa obligatòria requereix definicions clares i auditoria independent.

Un punt de partida raonable: exigir a les empreses per sobre d'un llindar d'ingressos que gastin almenys un 5% de l'I+D relacionada amb la IA en treball de seguretat verificat independentment per un auditor extern. Això és modest comparat amb la despesa en seguretat de la indústria farmacèutica, que típicament supera el 10% dels ingressos.

### Impostos sobre la computació

Gravar els recursos computacionals utilitzats per entrenar models grans i dirigir els ingressos a la recerca en seguretat. Aquest enfocament apunta al recurs més directament associat amb l'avanç de les capacitats. Com es va explorar a la nostra anàlisi de la [governança de la computació](/research/ca/023-compute-governance/), la computació és mesurable, concentrada en un petit nombre de proveïdors de núvol i ja està subjecta a controls d'exportació.

Un impost sobre la computació de 0,01 $ per hora de GPU en entrenaments per sobre d'un llindar (per exemple, 10^23 FLOP) generaria ingressos significatius afegint un cost marginal als grans entrenaments. A les escales d'entrenament de 2025, un sol entrenament de model de frontera consumeix milions d'hores de GPU. Fins i tot un petit impost per hora genera desenes de milions de dòlars per model.

Els ingressos dels impostos sobre la computació podrien finançar un institut de recerca en seguretat independent, de manera similar a com els ingressos de les subhastes d'espectre finançen la recerca en telecomunicacions. La decisió de disseny clau és la governança: qui decideix com es gasten els fons i com mantenen la independència tant de la pressió industrial com política?

### Models de premis i recompenses

Els premis recompensen assoliments específics en lloc de finançar programes en curs. Els Grand Challenges de DARPA, el Netflix Prize i les competicions de Kaggle demostren que els premis poden atraure talent i accelerar el progrés en problemes ben definits.

Per a la seguretat de la IA, els premis funcionen millor quan el problema està clarament definit i les solucions són verificables. Exemples inclouen recompenses de red-teaming (trobar i documentar modes de fallada específics), reptes d'interpretabilitat (explicar el raonament intern d'un model donat) i desenvolupament d'avaluacions (crear millors punts de referència per mesurar propietats de seguretat).

La Fundació X-Prize va anunciar un premi de seguretat de la IA de 10 milions de dòlars el 2025. Diverses empreses gestionen programes de recompenses per errors que inclouen troballes rellevants per a la seguretat. Aquests esforços són petits però demostren la viabilitat del model.

Els premis tenen limitacions. Afavoreixen problemes amb criteris d'èxit clars. Molta recerca en seguretat implica exploració oberta on l'èxit és difícil de definir per endavant. Els premis també creen dinàmiques on el guanyador s'ho emporta tot, cosa que descoratja la col·laboració i el treball incremental.

Un enfocament híbrid combina recompenses per a reptes específics i ben definits amb finançament per beques per a la recerca oberta. Recompenses de 100.000 $ a 1 milió de dòlars per contribucions específiques a la seguretat (descobrir una nova categoria de jailbreak, desenvolupar una eina d'interpretabilitat verificada, crear una avaluació de seguretat que un model de frontera no superi) podrien complementar un finançament de recerca més ampli.

### Finançament vinculat a assegurances

Com es va examinar a [Mercats d'assegurances i fixació de preus del risc de la IA](/research/ca/036-insurance-markets/), les assegurances creen incentius financers per a la seguretat. El finançament vinculat a assegurances estén aquesta lògica: exigir als desplegadors d'IA que contractin assegurança de responsabilitat civil i permetre que les asseguradores financin recerca en seguretat que redueixi la seva exposició.

Aquest model existeix en altres indústries. Les asseguradores de compensació laboral finançen la recerca en seguretat laboral. Les asseguradores de responsabilitat mèdica finançen iniciatives de seguretat del pacient. L'Insurance Institute for Highway Safety, finançat per asseguradores d'automòbils, ha impulsat millores significatives en la seguretat dels vehicles.

Un Institut d'Assegurances de Seguretat de la IA, finançat per les primes d'una assegurança de responsabilitat d'IA obligatòria, podria donar suport a la recerca en seguretat independent amb rellevància directa per als riscos de desplegament del món real. Això vincula el finançament al perfil de risc real dels sistemes d'IA en lloc de les prioritats polítiques o filantròpiques.

### Finançament internacional conjunt

La seguretat de la IA és un bé públic global. Cap país no captura tots els beneficis de la recerca en seguretat, i els riscos creuen fronteres. Això crea un argument per al finançament international conjunt, similar al CERN per a la física de partícules o el sistema CGIAR per a la recerca agrícola.

El Frontier AI Forum, establert després de la cimera de Bletchley Park de 2023, representa un primer pas. Una versió més ambiciosa crearia un Fons Internacional de Recerca en Seguretat de la IA amb contribucions de les principals nacions desenvolupadores d'IA, governat per un consell científic independent i distribuint beques a través de competició oberta.

Contribucions anuals del 0,1% dels ingressos de la indústria nacional d'IA dels EUA, la Xina, la UE, el Regne Unit, el Japó i Corea del Sud generarien més de 1.000 milions de dòlars anuals. Això supera el finançament filantròpic actual i donaria suport a una empresa de recerca independent significativa.

L'obstacle és geopolític. La IA està imbricada en la competició estratègica entre els EUA i la Xina. El finançament conjunt requereix confiança que els resultats de la recerca es compartiran obertament, cosa que entra en conflicte amb les preocupacions de seguretat nacional. Un enfocament a curt termini més realista limita el finançament conjunt a aliats i socis, acceptant una escala reduïda per una viabilitat augmentada.

## El problema de l'impost d'alineament

Qualsevol model de finançament ha d'afrontar l'impost d'alineament: la recerca en seguretat competeix amb la recerca en capacitats pels mateixos recursos escassos, particularment investigadors amb talent i computació.

Els millors investigadors en aprenentatge automàtic poden exigir salaris superiors a 1 milió de dòlars anuals als laboratoris de frontera. Les organitzacions centrades en seguretat, especialment les sense ànim de lucre i els grups acadèmics, no poden igualar aquestes ofertes. El resultat és un drenatge persistent de talent de la seguretat cap a les capacitats.

Aquesta dinàmica s'autoreforça. Els laboratoris de capacitats atrauen els millors investigadors. Els millors investigadors produeixen resultats impressionants. Els resultats impressionants atrauen finançament i més investigadors. Les organitzacions de seguretat lluiten per competir pel talent, produeixen menys resultats destacats i troben més difícil atraure la propera generació.

Com es va discutir a [L'equilibri entre velocitat i seguretat](/research/ca/077-speed-safety-tradeoff/), aquest desequilibri no és inevitable. Reflecteix decisions sobre com s'assignen els recursos. Diverses estratègies poden ajudar a tancar la bretxa.

**Competitivitat salarial.** Les organitzacions de seguretat han de pagar salaris competitius. Això requereix fons de finançament més grans. L'expectativa que els investigadors en seguretat haurien d'acceptar compensació per sota del mercat per raons de missió és insostenible. Alguns finançadors filantròpics han començat a donar suport a salaris de mercat a les organitzacions de seguretat.

**Accés a la computació.** La recerca en seguretat requereix cada vegada més accés a computació a escala de frontera. Clústers de computació finançats pel govern dedicats a la recerca en seguretat, com el Recurs Nacional de Recerca en IA dels EUA (NAIRR) proposat el 2023, poden proporcionar-ho sense requerir que cada organització de seguretat financi la seva pròpia infraestructura.

**Incentius de carrera.** Les institucions acadèmiques haurien de reconèixer la recerca en seguretat per a les decisions de contractació i tenure. Les agències governamentals de finançament haurien de crear trajectòries professionals dedicades a la recerca en seguretat. Les empreses haurien de crear itineraris per als investigadors en seguretat que no acabin en un carreró sense sortida per sota del nivell de direcció.

**Prestigi de la recerca.** La comunitat de recerca en seguretat ha progressat en l'establiment de llocs de publicació (la sèrie de workshops SafeAI, tracks dedicats a ICML i NeurIPS) i reconeixement de la recerca. El progrés continuat per fer que la recerca en seguretat sigui intel·lectualment prestigiosa atrau talent independentment de la compensació.

## Una proposta concreta: una arquitectura de finançament diversificada

Cap model de finançament únic és suficient. Cadascun té punts forts i febles. L'objectiu hauria de ser una arquitectura diversificada que combini múltiples models per crear redundància, independència i escala.

Proposem cinc components:

1. **Despesa corporativa obligatòria en seguretat.** Exigir a les empreses amb ingressos d'IA superiors a 1.000 milions de dòlars que destinin almenys un 5% de la despesa en I+D d'IA a recerca en seguretat, auditada independentment. Rendiment estimat: de 2.000 a 5.000 milions de dòlars anuals a l'escala actual de la indústria.

2. **Programes governamentals de recerca en seguretat.** Ampliar el NIST, l'AISI del Regne Unit i els organismes equivalents a un pressupost combinat de 1.000 milions de dòlars anuals entre nacions aliades. Finançar tant recerca interna com beques externes a través de competició oberta.

3. **Impost sobre la computació per a recerca independent.** Establir un impost sobre els grans entrenaments (per sobre de 10^23 FLOP) per finançar una Fundació de Recerca en Seguretat de la IA independent governada per un consell científic sense majoria industrial ni governamental. Rendiment estimat: de 200 a 500 milions de dòlars anuals.

4. **Filantropia ampliada amb coordinació.** Encoratjar els finançadors filantròpics a coordinar-se a través d'un marc estratègic compartit per reduir la duplicació i assegurar la cobertura d'àrees negligides. Objectiu: mantenir el finançament filantròpic actual (500 milions de dòlars+) mentre es millora l'assignació.

5. **Fons internacional de recerca en seguretat.** Establir un fons multilateral per a la recerca en seguretat de la IA, començant amb les nacions dels Five Eyes i expandint-se. Objectiu inicial: 500 milions de dòlars anuals.

Aquesta arquitectura portaria el finançament total de la seguretat de la IA a entre 4.000 i 7.000 milions de dòlars anuals: encara modest en relació amb la inversió en capacitats, però un augment significatiu respecte als nivells actuals i suficient per donar suport a una comunitat de recerca en seguretat àmplia, diversa i independent.

## Dimensió reflexiva

Aquesta anàlisi està ella mateixa configurada per les dinàmiques de finançament que descriu. La Iniciativa Reflexive AI opera fora del desenvolupament comercial d'IA, cosa que atorga independència però limita l'accés als sistemes de frontera i al coneixement intern. La nostra avaluació de la despesa corporativa en seguretat es basa en divulgacions públiques que són incompletes per disseny.

També assenyalem una tensió en la nostra proposta. Les normes de despesa obligatòria i els impostos sobre la computació augmenten el cost del desenvolupament de la IA. Com vam examinar a [L'equilibri entre velocitat i seguretat](/research/ca/077-speed-safety-tradeoff/), imposar costos al desenvolupament el ralenteix. Si les aplicacions d'IA proporcionen beneficis genuïns, ralentir el desenvolupament té costos. El nostre judici és que els beneficis d'una recerca en seguretat adequada superen aquests costos, però persones raonables discrepen.

Hi ha una preocupació reflexiva addicional: els investigadors finançats per mecanismes específics de seguretat tenen incentius per emfatitzar la importància de la recerca en seguretat. Els finançadors de la recerca en seguretat tenen incentius per creure que el seu finançament és necessari. Això no fa que els arguments siguin erronis, però exigeix transparència sobre aquestes dinàmiques.

Finalment, qualsevol arquitectura de finançament crea poder. Qui controla les decisions de finançament configura l'agenda de recerca. La història del finançament de la ciència mostra que el control centralitzat produeix pensament grupal i negligència d'idees heterodoxes. La nostra arquitectura proposada distribueix deliberadament l'autoritat de finançament entre múltiples organismes independents per mitigar aquest risc.

## Conclusió

El finançament de la recerca en seguretat de la IA és inadequat en escala, excessivament concentrat en les seves fonts i estructuralment desalineat amb els problemes que necessita abordar. La barreja actual de filantropia, beques governamentals i pressupostos corporatius va construir el camp, però no el sostindrà a l'escala que exigeix el desenvolupament de la IA de frontera.

Tancar la bretxa requereix nous mecanismes: despesa obligatòria en seguretat, impostos sobre la computació, models de premis, finançament vinculat a assegurances i cooperació internacional. Cap mecanisme únic és suficient. Una arquitectura de finançament diversificada, que distribueix l'autoritat i els recursos entre organismes independents, proporciona l'escala, la independència i la resiliència que la recerca en seguretat necessita.

L'impost d'alineament és real. La recerca en seguretat competeix amb la recerca en capacitats pel talent, la computació i l'atenció. Superar aquesta competició requereix no només més diners sinó sistemes de finançament millor dissenyats que facin que la recerca en seguretat sigui intel·lectualment gratificant, financerament competitiva i institucionalment recolzada.

Un finançament adequat de la seguretat costa milers de milions. Una recerca en seguretat inadequada, si els sistemes d'IA avançats causen danys greus, costa més del que ningú vol quantificar. L'argument d'inversió és directe.

## Referències

1. Epoch AI. (2024). "Trends in AI Investment and Safety Spending." Epoch AI Research.
2. Open Philanthropy. (2025). "Grants Database: AI Safety and Governance." openphilanthropy.org.
3. Congressional Research Service. (2024). "Department of Defense Artificial Intelligence Spending." CRS Report R47976.
4. UK Department for Science, Innovation and Technology. (2023). "AI Safety Institute: Mission and Budget." gov.uk.
5. European Commission. (2024). "Horizon Europe: Trustworthy AI Cluster Funding." ec.europa.eu.
6. NIST. (2024). "AI Safety Research Program: Annual Report." National Institute of Standards and Technology.
7. Grunewald, E. & Heim, L. (2024). "Compute Governance and AI Safety." Centre for the Governance of AI Working Paper.
8. Amodei, D. (2023). "The Case for More AI Safety Funding." Anthropic Blog.
9. X-Prize Foundation. (2025). "AI Safety X-Prize: Competition Details." xprize.org.
10. National Academies of Sciences. (2024). "Funding Models for Emerging Technology Safety Research." National Academies Press.
11. Frontier AI Forum. (2024). "Charter and Funding Commitments." frontieraiforum.org.
12. Berger, A. & Lazar, S. (2024). "The Political Economy of AI Safety Research." Oxford University Working Paper.
