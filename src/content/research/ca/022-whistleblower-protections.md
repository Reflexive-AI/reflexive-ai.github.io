---
title: "Proteccions per a denunciants en laboratoris d'IA"
excerpt: "Els empleats d'empreses d'IA sovint tenen una perspectiva unica sobre els riscos. Les proteccions actuals son inadequades. Aquesta analisi examina que requeririen marcs significatius de proteccio de denunciants per a la IA."
date: 2026-01-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - whistleblowing
  - transparency
  - safety
  - governance
  - reporting
---

## El dilema de l'informant intern

El 2024 i el 2025, diversos investigadors prominents d'IA van plantejar publicament preocupacions sobre les practiques de seguretat dels seus ocupadors. Alguns van dimitir. Alguns van ser acomiadats. Quasi tots van afrontar consequencies personals i professionals significatives.

Aquestes persones sabien quelcom important: informacio privilegiada sobre el desenvolupament d'IA a la qual el public i els reguladors no tenien acces. Van prendre decisions dificils per compartir-la, amb un cost personal considerable.

Les seves experiencies revelen una bretxa fonamental de governanca. Les persones millor posicionades per identificar problemes de seguretat de la IA --els empleats d'empreses d'IA-- afronten forts desincentius per plantejar preocupacions. Les proteccions legals actuals son inadequades, i la cultura de molts laboratoris d'IA descoratja la dissidencia interna.

Aquesta analisi examina per que les proteccions per a denunciants son importants per a la governanca de la IA, avalua les bretxes actuals i proposa com seria una proteccio significativa.

## Per que importen les fonts internes

El desenvolupament d'IA es inusualment opac. Els sistemes mes capacos son construits per un petit nombre d'empreses. Gran part del que fan es propietari. Els observadors externs --inclosos els reguladors-- tenen visibilitat limitada sobre les practiques de desenvolupament, les proves de seguretat i les preocupacions internes.

Els empleats, en canvi, sovint saben:

**Quines capacitats existeixen.** Les proves internes poden revelar capacitats no divulgades publicament. Vam explorar els perills de les capacitats no documentades a [el problema de l'excedent de capacitats](/research/009-capability-overhang/) --la bretxa entre el que els sistemes poden fer i el que es coneix publicament.

**Com es prenen les decisions de seguretat.** S'avaluen seriosament les preocupacions de seguretat, o s'anul·len rutinariament per pressio comercial? Son adequades les proves? S'aborden o s'ignoren les troballes del red teaming?

**Quins dreceres es prenen.** Sota la pressio de llancar productes competitius, quines mesures de seguretat s'ometen, es retarden o es debiliten?

**Quins problemes han passat.** Incidents interns, quasi-accidents i comportaments preocupants que no arriben al public.

Aquesta informacio es essencial per a una governanca efectiva. Sense ella, els reguladors operen en gran mesura amb el que les empreses trien divulgar. Com vam discutir a [autonotificacio versus auditoria externa](/research/010-self-reporting-vs-audit/), l'autonotificacio per si sola es insuficient per a dominis critics per a la seguretat.

## Les proteccions actuals son inadequades

Els marcs existents de proteccio de denunciants van ser dissenyats per a dominis diferents i es tradueixen malament a la IA.

### Cobertura legal limitada

Les proteccions per a denunciants a la majoria de les jurisdiccions cobreixen industries especifiques o tipus especifics d'infraccions: frau financer, violacions ambientals, violacions de lleis de valors. Les preocupacions de seguretat de la IA sovint no encaixen en aquestes categories.

Un empleat que creu que la seva empresa esta desenvolupant IA perillosament capac sense mesures de seguretat adequades pot no estar legalment protegit. Si no s'esta violant cap llei especifica --simplement s'estan ignorant normes o subestimant riscos-- els estatuts de proteccio de denunciants poden no aplicar-se.

### Definicions estretes de conducta indeguda

Fins i tot on les divulgacions relacionades amb la IA podrien qualificar per a proteccio, la divulgacio tipicament ha d'involucrar activitat il·legal. Pero els riscos de seguretat de la IA que mes preocupen els investigadors sovint no son il·legals: son irresponsables, imprudents o violatoris de normes, pero no criminals.

Construir un sistema d'IA capac sense proves de seguretat adequades no es actualment un delicte a la majoria de les jurisdiccions. Llancar un model amb capacitats perilloses conegudes pot no violar cap llei. La bretxa entre el que es legal i el que es segur es precisament on les proteccions per a denunciants son mes necessaries i menys disponibles.

### Remeis febles

On existeixen proteccions, els remeis per a les represalies sovint son inadequats. La reincorporacio a un lloc de treball on l'ocupador et guarda rancunia es una victoria buida. Els danys poden no compensar la destruccio de carrera en una industria petita. Els casos triguen anys a resoldre's.

En camps tecnologics amb ocupadors concentrats i fortes xarxes informals, les consequencies reputacionals de ser etiquetat com un empleat "dificil" poden acabar amb una carrera independentment dels resultats legals.

### Obligacions de confidencialitat

Els empleats d'IA tipicament signen acords de confidencialitat i cessions de propietat intel·lectual que excedeixen els contractes d'ocupacio estandard. Aquests poden aplicar-se fins i tot contra divulgacions que serveixen l'interes public.

Les empreses poden utilitzar aquests acords per dissuadir possibles divulgacions, amenant amb accions legals que serien costoses de defensar fins i tot si finalment no resulten exitoses.

### Dependencies de visa

El desenvolupament d'IA atrau talent internacional. Molts empleats tenen visats de treball vinculats al seu ocupador. Perdre un lloc de treball significa perdre l'autoritzacio de treball, i potencialment haver d'abandonar el pais en dies.

Aixo crea un poder coercitiu extraordinari. Un empleat que altrament podria plantejar preocupacions no pot arriscar la pertorbacio personal i familiar de la perdua del visat. Els empleats mes vulnerables son els menys capacos de servir com a denunciants.

## Que requeriria una proteccio significativa

Les proteccions efectives per a denunciants en la IA necessitarien diversos components.

### Cobertura ampliada

Les divulgacions haurien d'estar protegides si es refereixen a una creenca raonable de riscos per a la seguretat publica, fins i tot si no es viola cap llei especifica. Aixo requereix definir la divulgacio protegida de manera prou amplia per cobrir les preocupacions de seguretat de la IA.

L'estandard hauria de ser que l'empleat cregui raonablement que la informacio es refereix a un risc genuí per a la seguretat publica derivat del desenvolupament o la implementacio d'IA, independentment de si es tracta de violacions legals especifiques.

### Forta proteccio contra represalies

Els ocupadors haurien d'afrontar sancions significatives per represalies contra divulgacions protegides. Aquestes sancions haurien de ser prou grans per dissuadir, no merament un cost de fer negocis.

Els remeis haurien d'incloure compensacio per dany a la carrera, que en camps especialitzats pot excedir amb escreix els salaris endarrerits. Els tribunals haurien de tenir autoritat per ordenar remeis que deixin els denunciants integres, no merament tecnicament compensats.

### Anul·lacio de la confidencialitat

Els acords de confidencialitat haurien de ser inaplicables contra divulgacions protegides. Un acord de no divulgacio no hauria d'impedir que un empleat informi un regulador sobre riscos de seguretat, fins i tot si la informacio es tecnicament propietaria.

Aixo requereix disposicions legals que anul·lin explicitament les obligacions de confidencialitat per a les divulgacions de seguretat de la IA, en linia amb el que existeix en algunes jurisdiccions per al frau financer.

### Canals protegits

Hi hauria d'haver canals clars per a la notificacio interna (dins de l'empresa), la notificacio regulatoria (a agencies governamentals) i --com a ultim recurs-- la divulgacio publica. La proteccio hauria d'aplicar-se a tots els canals, amb requisits adequats d'escalament.

Aixo connecta amb el nostre treball sobre [protocols per a la comunicacio d'IA a regulador](/research/014-ai-regulator-protocol/). Els denunciants humans i els sistemes de monitoratge basats en IA necessiten vies clares per comunicar preocupacions als organismes de supervisio.

### Proteccions d'immigracio

Els titulars de visat que realitzin divulgacions protegides haurien de rebre autoritzacio de treball temporal suficient per permetre'ls romandre al pais mentre busquen nova feina o persegueixen reclamacions legals. L'amenca del visat s'hauria de neutralitzar.

### Notificacio anonima

Alguns empleats nomes estaran disposats a notificar si poden romandre anonims. Els sistemes per rebre i actuar sobre informes anonims --tot i ser mes dificils de verificar-- haurien de formar part de la infraestructura.

## Dinamiques de la industria

Mes enlla de les proteccions legals, la cultura de la industria importa.

Actualment, la IA es una industria concentrada on un petit nombre d'empreses i inversors tenen influencia significativa sobre les perspectives de carrera. El dany reputacional de la denuncia pot seguir algu indefinidament.

Canviar aixo requereix:

**Validacio.** Quan els denunciants plantegen preocupacions legitimes, la investigacio posterior i el reconeixement validen la seva accio i assenyalen als altres que plantejar preocupacions es valorat.

**Normalitzacio.** Els liders de la industria que es comprometen publicament a protegir els dissidents interns, i demostren aquest compromis, canvien les normes sobre el que es acceptable.

**Ocupacio alternativa.** Com mes robust sigui l'ecosistema d'organitzacions de seguretat de la IA, posicions academiques i trajectories professionals alternatives, menys pot qualsevol ocupador individual amenacar amb la destruccio de carrera.

**Pressio dels inversors.** Els inversors que pregunten sobre la cultura de seguretat i la proteccio de la dissidencia interna creen incentius perque les empreses desenvolupin millors practiques.

## Connexio amb la governanca

Les proteccions per a denunciants no estan separades d'altres mecanismes de governanca de la IA, sino que son part integral d'ells.

Les regulacions nomes funcionen si es detecten les violacions. Les auditories externes nomes arriben al que les empreses trien revelar. [Qui vigila els vigilants](/research/006-meta-governance-auditors/) --la nostra analisi de la governanca de les auditories-- va concloure que la supervisio externa requereix informacio de multiples fonts. Les fonts internes estan entre les mes valuoses.

De la mateixa manera, els sistemes de notificacio d'incidents que vam analitzar a [llicons de l'aviacio](/research/021-aviation-lessons/) depenen que la informacio flueixi d'aquells que presencien els incidents. Si els empleats tenen por de notificar, la informacio necessaria per a l'aprenentatge no arriba a qui la necessita.

I la governanca reflexiva --sistemes d'IA que participen en la seva propia supervisio-- es complementaria a la denuncia humana, no un substitut. Els sistemes d'IA poden monitoritzar algunes coses; els humans en noten d'altres. Ambdos canals necessiten estar protegits.

## Conclusio

Les persones que mes saben sobre els riscos de la IA son sovint les menys protegides quan intenten compartir aquest coneixement. Aixo es una fallada de governanca que soscava tots els altres mecanismes de seguretat de la IA.

Les proteccions efectives per a denunciants en la IA requereixen cobertura legal ampliada, fortes mesures contra represalies, anul·lacio de la confidencialitat, canals protegits, proteccions d'immigracio i un canvi cultural en com la industria tracta la dissidencia interna.

Aquestes proteccions serveixen no nomes als denunciants individuals sino a l'interes public en sistemes d'IA que siguin segurs, beneficiosos i responsables. Sense informacio d'aquells que estan dins del desenvolupament d'IA, la governanca externa opera a les fosques.

## Related Research

- [The Capability Overhang Problem](/research/009-capability-overhang/)
- [Self-Reporting vs. External Audit: Trade-offs](/research/010-self-reporting-vs-audit/)
- [Who Watches the Watchers? Auditing AI Auditors](/research/006-meta-governance-auditors/)
- [A Protocol for AI-to-Regulator Communication](/research/014-ai-regulator-protocol/)
