---
title: "La Llei d'IA de la UE: el que no contempla"
excerpt: "La Llei d'IA de la UE representa la legislació sobre IA més completa del món. Però fins i tot una regulació ben dissenyada té punts cecs. Una crítica constructiva del que la Llei deixa sense abordar."
date: 2026-01-01
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - eu-ai-act
  - regulation
  - policy
  - governance
  - enforcement
  - jurisdiction
---

## Una fita històrica

La Llei d'IA de la UE, que va entrar en vigor el 2025, representa el primer marc legal integral de la humanitat per governar la intel·ligència artificial. El seu enfocament basat en el risc, els seus requisits escalonats i la seva atenció als drets fonamentals estableixen un precedent global.

Aquesta anàlisi reconeix la importància de la Llei alhora que examina les llacunes que persisteixen fins i tot en una legislació ben dissenyada. L'objectiu no és la crítica per si mateixa, sinó la identificació d'àrees que requereixen mecanismes de governança complementaris.

## El que la Llei fa bé

Abans d'examinar les llacunes, val la pena reconèixer les fortaleses de la Llei.

La **classificació per nivells de risc** evita la trampa de tractar tota la IA de la mateixa manera. En distingir entre aplicacions inacceptables, d'alt risc, de risc limitat i de risc mínim, la Llei assigna l'atenció reguladora de manera proporcional. Aquest principi — que la governança ha d'escalar amb la capacitat i el context — s'alinea amb la nostra anàlisi a [proporcionalitat en la divulgació de models](/research/001-proportionality-disclosure/).

Els **requisits de transparència** per a certes aplicacions d'IA estableixen una línia base de rendició de comptes. Els usuaris tenen dret a saber quan estan interactuant amb sistemes d'IA, i els operadors han de mantenir documentació sobre com funcionen els sistemes.

L'**enfocament en els drets fonamentals** situa el benestar humà al centre, més enllà de la seguretat purament tècnica. Les restriccions a la vigilància biomètrica, les prohibicions de la puntuació social i les proteccions en contextos laborals reflecteixen una governança basada en valors.

Les **disposicions sobre IA de propòsit general** reconeixen que els models fundacionals requereixen un tractament diferent al de les aplicacions específiques, introduint requisits per a sistemes de frontera amb risc sistèmic.

Aquests són assoliments genuïns. Les llacunes que es discuteixen a continuació existeixen dins d'un marc general que representa un progrés significatiu.

## Llacuna 1: Emergència de capacitats

La Llei categoritza els sistemes d'IA segons els seus casos d'ús previstos. Una eina de diagnòstic mèdic és d'alt risc; un filtre de spam és de risc mínim. Això té sentit per a sistemes específics dissenyats per a propòsits concrets.

Però els sistemes d'IA de propòsit general desafien aquest enfocament. Un gran model de llenguatge podria estar "destinat" a la generació de text, situant-lo en una categoria, mentre que en realitat és capaç d'assistir amb tasques en tot l'espectre de risc. El mateix model que redacta textos publicitaris pot, amb les indicacions adequades, ajudar a sintetitzar materials perillosos.

La Llei intenta abordar això a través de les seves disposicions sobre IA de propòsit general, però aquestes se centren principalment en models per sobre de certs llindars de capacitat. El problema de les capacitats emergents — on els models desenvolupen habilitats inesperades no anticipades pels seus creadors — continua insuficientment abordat.

Un model podria llançar-se complint tots els requisits, i després descobrir-se que té capacitats que haurien activat un tractament diferent. La categorització estàtica de la Llei té dificultats amb aquesta realitat dinàmica. Vam explorar això a [el problema de l'excedent de capacitat](/research/009-capability-overhang/) — el repte de governar capacitats que existeixen però que encara no han estat documentades o descobertes.

## Llacuna 2: Capacitat d'aplicació

La legislació només és tan eficaç com la seva aplicació. La Llei crea autoritats nacionals competents responsables de la vigilància del mercat i el compliment, però proporciona orientació limitada sobre com aquestes autoritats haurien de desenvolupar la capacitat tècnica per avaluar realment els sistemes d'IA.

L'avaluació actual de la seguretat de la IA requereix experiència especialitzada concentrada en un grapat d'organitzacions a nivell mundial. La majoria dels organismes reguladors nacionals manquen del personal, les eines i el coneixement per avaluar de manera independent si un sistema d'IA complex compleix amb els requisits de la Llei.

Això genera dependència ja sigui de l'autoavaluació per part dels desenvolupadors o d'auditors externs. Ambdós enfocaments tenen vulnerabilitats. L'autoavaluació pateix de conflictes d'interès obvis. L'auditoria externa requereix auditors competents, independents i amb recursos adequats — una oferta que continua sent limitada. Vam examinar aquest repte a [qui vigila els vigilants?](/research/006-meta-governance-auditors/).

La Llei podria reforçar-se creant una infraestructura tècnica d'avaluació compartida a nivell europeu, formació obligatòria per a les autoritats nacionals i mecanismes per garantir la independència dels auditors.

## Llacuna 3: Abast extraterritorial

La Llei s'aplica als sistemes d'IA comercialitzats al mercat de la UE, independentment d'on estiguin establerts els proveïdors. Aquest abast extraterritorial s'inspira en el RGPD i reflecteix el considerable poder de mercat de la UE.

No obstant això, l'aplicació contra entitats no pertanyents a la UE és pràcticament difícil. Una empresa sense presència a la UE que desplegui serveis d'IA accessibles des d'Europa té una exposició limitada a les sancions de la UE. Si bé la Llei prohibeix tals serveis no conformes, la prohibició és més fàcil d'enunciar que d'aplicar.

A més, la jurisdicció de la Llei es defineix per on els sistemes són "comercialitzats" o on s'utilitzen els resultats a la UE. Els sistemes d'IA entrenats en un altre lloc, amb dades d'un altre lloc, però els resultats dels quals eventualment afecten residents de la UE a través de canals indirectes, poden quedar fora d'una jurisdicció clara.

Aquesta llacuna importa perquè el desenvolupament de la IA és globalment distribuït, i l'arbitratge regulador — reubicar activitats per evitar la supervisió — és un risc real. Vam analitzar aquesta dinàmica a [arbitratge regulador en el desplegament d'IA](/research/008-regulatory-arbitrage/). Sense coordinació internacional, fins i tot una legislació nacional completa pot ser eludida.

## Llacuna 4: Ritme del canvi tècnic

La Llei va ser redactada basant-se en les capacitats de la IA circa 2022-2023 i s'implementarà durant 2024-2027. El panorama de la IA el 2027 pot ser molt diferent del que els legisladors van preveure.

Si bé la Llei inclou disposicions per actualitzar actes d'implementació i estàndards tècnics, les categories de risc fonamentals i els requisits estan integrats en la legislació primària. Canviar-los requereix el procés legislatiu complet, que opera en una escala temporal d'anys.

Aquest desfasament temporal és particularment agut per a la IA de propòsit general, on les capacitats avancen ràpidament. Els llindars que defineixen el "risc sistèmic" — actualment vinculats al còmput d'entrenament — poden quedar obsolets a mesura que noves arquitectures aconsegueixin més capacitat amb menys còmput, o a mesura que l'ajust fi i l'scaffolding amplifiquin les capacitats del model base de maneres que els llindars originals no capturen.

Els mecanismes de governança adaptativa — com empoderar les agències per ajustar llindars tècnics sense nova legislació, o incorporar cicles obligatoris de revisió — podrien ajudar a abordar aquesta llacuna.

## Llacuna 5: El problema de la cadena de subministrament

Els sistemes d'IA es construeixen cada vegada més sobre capes d'altres sistemes d'IA. Una empresa podria ajustar un model fundacional, integrar-lo amb sistemes de recuperació, embolicar-lo en una capa d'aplicació i desplegar-lo a través d'una plataforma de tercers. La responsabilitat del compliment es fragmenta al llarg d'aquesta cadena de subministrament.

La Llei distingeix entre proveïdors (qui desenvolupa sistemes) i operadors (qui els utilitza), assignant diferents obligacions a cadascun. Però la distinció nítida entre proveïdor i operador es difumina quan la mateixa organització modifica un sistema abans del desplegament, quan els sistemes es componen de múltiples components, o quan l'ajust fi canvia substancialment el comportament d'un model.

Els models de pesos oberts creen complexitat addicional. Si una empresa allibera els pesos d'un model sota una llicència oberta, i un usuari aigües avall modifica aquests pesos per crear una aplicació danyina, qui assumeix la responsabilitat? Les disposicions de la Llei sobre responsabilitat derivada continuen sent ambigües en aquests casos.

Això connecta amb la tensió que vam explorar a [la paradoxa de seguretat dels pesos oberts](/research/002-open-weight-safety-paradox/) — la dificultat de mantenir controls de seguretat sobre sistemes dissenyats per ser modificats i redistribuïts.

## Llacuna 6: Transparència significativa

La Llei exigeix transparència, però la transparència no és un bé indiferenciat. Cent pàgines de documentació tècnica poden satisfer els requisits de divulgació mentre proporcionen poca informació útil a aquells a qui la transparència pretén servir.

Les fitxes de model i la documentació tècnica actuals sovint prioritzen el compliment legal sobre la comprensió genuïna. Divulguen fets que satisfan les caselles reguladores mentre ometen informació que permetria una avaluació significativa dels riscos i limitacions.

Una transparència eficaç requereix atenció no només a què es divulga, sinó a qui, en quin format i amb quin suport per a la interpretació. Les disposicions de transparència de la Llei es beneficiarien d'orientació complementària sobre la qualitat de la divulgació — assegurant que la documentació realment permeti la supervisió que se suposa que ha de donar suport.

## Llacuna 7: Governança reflexiva

Potser la llacuna més fonamental és la suposició de la Llei que la governança és una activitat purament externa — quelcom que els humans fan als sistemes d'IA. La possibilitat que els sistemes d'IA participin en la seva pròpia governança no es contempla.

A mesura que els sistemes d'IA es tornen més capaços, podrien ser capaços de monitorar el seu propi comportament en busca d'anomalies, assenyalar possibles usos indeguts, explicar les seves restriccions als usuaris i fins i tot participar en l'avaluació de si els seus contextos de desplegament són apropiats. Aquesta capacitat reflexiva no és un substitut de la governança externa, sinó un complement.

La Llei podria ampliar-se per reconèixer i crear incentius per als mecanismes de governança reflexiva — sistemes d'IA que contribueixin a la seva pròpia supervisió. El nostre treball sobre [esquemes de restricció llegibles per màquina](/research/003-machine-readable-constraint-schema/), [detecció reflexiva d'ús indegut](/research/011-reflexive-misuse-detection/) i [protocols de comunicació entre IA i reguladors](/research/014-ai-regulator-protocol/) explora com podria ser això.

## Cap a una governança complementària

Les llacunes identificades aquí no són fracassos de la Llei d'IA de la UE, sinó limitacions del que qualsevol instrument legislatiu individual pot aconseguir. Assenyalen àrees on calen mecanismes de governança complementaris:

- **Coordinació internacional** per limitar l'arbitratge regulador
- **Inversió en capacitat tècnica** per a l'avaluació i l'aplicació
- **Mecanismes adaptatius** que puguin respondre al canvi ràpid de capacitats
- **Governança de la cadena de subministrament** que abordi la complexitat composicional
- **Estàndards de qualitat de transparència** més enllà dels mers requisits de divulgació
- **Mecanismes reflexius** on els sistemes d'IA contribueixin a la seva pròpia governança

La Llei d'IA de la UE és un començament, no un final. El seu assoliment és crear una base; el repte continu és construir una estructura de governança completa sobre aquesta base.

## Recerca relacionada

- [Proporcionalitat en la divulgació de models](/research/001-proportionality-disclosure/)
- [El problema de l'excedent de capacitat](/research/009-capability-overhang/)
- [Arbitratge regulador en el desplegament d'IA](/research/008-regulatory-arbitrage/)
- [Qui vigila els vigilants? Auditar els auditors d'IA](/research/006-meta-governance-auditors/)
- [La paradoxa de seguretat dels pesos oberts](/research/002-open-weight-safety-paradox/)
