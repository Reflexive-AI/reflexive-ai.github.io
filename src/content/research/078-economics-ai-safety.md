---
title: "The Economics of AI Safety: Who Pays and Why It Matters"
excerpt: "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
date: 2026-02-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - economics
  - funding
  - incentives
  - safety
  - investment
---

## Safety is Not Free

AI safety requires resources: researcher salaries, compute for experiments, time for testing, infrastructure for auditing. These resources are not free.

Who pays for safety work shapes what work gets done. If safety funding comes from commercial labs, it reflects commercial priorities. If it comes from governments, it reflects political priorities. If it comes from philanthropy, it reflects donor priorities.

Understanding the economics of AI safety is essential for understanding why certain work happens and other work does not.

## Current Funding Landscape

AI safety funding comes from several sources.

### Commercial Labs

The largest AI companies, OpenAI, Anthropic, Google DeepMind, fund internal safety research. This funding is substantial in absolute terms but small relative to capability investment.

Commercial funding creates alignment between safety research and company interests. Work that helps companies deploy safely gets funded. Work that might slow deployment or impose costs is less attractive.

Commercial safety teams also face political pressures. Safety researchers who too aggressively constrain products may find their influence reduced.

### Philanthropy

Private foundations and wealthy individuals fund significant AI safety research, particularly at academic institutions and nonprofits. Open Philanthropy has been particularly prominent.

Philanthropic funding enables research independent of commercial interests. But it reflects donor worldviews. Philanthropists focused on existential risk fund existential risk work. Work on near-term harms receives less philanthropic attention.

Philanthropic funding is also uncertain. Donor priorities shift. Foundations sunset. Research agendas dependent on continued philanthropy face sustainability risks.

### Government

Government funding for AI safety is growing but remains limited relative to capability funding. Defense agencies, research councils, and regulatory bodies provide some support.

Government funding reflects political priorities. Research aligned with national security receives more support. Work that might constrain domestic industry is less attractive.

Government funding also involves bureaucracy, reporting requirements, and restrictions that some researchers find burdensome.

### Academia

Universities fund some AI safety research through general academic budgets. This funding is modest and subject to standard academic incentives: publication pressure, tenure requirements, grant seeking.

Academic funding enables independent research but is not well-suited to applied safety work requiring expensive infrastructure or rapid iteration.

## Incentive Misalignments

Current funding structures create systematic misalignments.

### Safety as Cost Center

Within commercial organizations, safety is typically a cost center: it consumes resources without directly generating revenue. Cost centers face pressure during budget constraints.

This contrasts with capability research, which is seen as investment in competitive advantage. When resources are scarce, cost centers are cut before investments.

### Short-Term vs. Long-Term

Quarterly earnings pressures encourage short-term thinking. Safety investments with long-term payoffs are underweighted relative to short-term revenue generation.

This is particularly problematic because safety failures often have long-term consequences: regulatory backlash, reputation damage, or catastrophic events. But these consequences are discounted relative to immediate competitive pressures.

### Externalities

Safety failures often impose costs on third parties: users harmed by systems, society affected by AI incidents, future generations facing existential risk. These costs are externalities not borne by developers.

When developers do not bear the full costs of safety failures, they underinvest in safety relative to what is socially optimal.

### Free Rider Problems

If one company invests in safety research whose benefits are shared across the industry, competitors free-ride. This discourages investment in public-good safety research.

Foundational safety research that benefits everyone may be underfunded because no individual actor captures sufficient returns.

## What Would Adequate Funding Look Like?

How would we know if AI safety is adequately funded?

### Proportionality

One benchmark: safety spending proportional to capability spending. If 10% of resources went to safety, that might represent adequate attention.

Current proportions are far lower. Estimates vary, but safety spending is likely 1-5% of AI R&D at major labs.

### Risk-Adjusted Investment

Investment should be proportional to risk. Higher-risk systems warrant higher safety investment. Current investment does not obviously follow this pattern.

This would require systems for assessing risk, which are themselves underdeveloped.

### Comparison to Other Sectors

Other high-risk sectors (pharmaceutical, aerospace, nuclear) have established safety spending norms. AI safety investment could be benchmarked against these sectors.

Such comparison suggests AI safety is dramatically underinvested relative to the risks involved.

## Policy Mechanisms

Several policy mechanisms could alter the economics of AI safety.

### Liability

If companies bear financial liability for AI harms, they internalize externality costs. This creates incentives for safety investment proportional to risk.

[Liability frameworks](/research/020-liability-frameworks/) require that harms be attributable and recoverable. Current legal structures make this difficult.

### Taxation and Subsidy

Governments could tax AI systems proportional to risk and use proceeds to fund safety research. This would internalize externalities and fund public goods simultaneously.

Alternatively, subsidies for safety research could increase investment without requiring liability reform.

### Regulatory Requirements

Mandatory safety investments, similar to pharmaceutical testing requirements, could guarantee minimum spending levels.

This approach requires regulators capable of specifying and verifying adequate safety investment, which is currently lacking.

### Insurance

Mandatory insurance for AI systems would create market incentives for safety. Insurers would price risk and require safety measures.

[Insurance markets](/research/036-insurance-markets/) could thus become governance mechanisms, with pricing incentivizing safety investment.

### Collective Agreements

Industry-wide agreements on safety spending would prevent individual companies from gaining advantage by skimping on safety.

Such agreements face coordination problems and antitrust concerns but have precedents in other industries.

## The Public Goods Problem

Some safety research is a public good: its benefits are non-excludable and non-rivalrous. Interpretability research, governance frameworks, and safety standards benefit everyone.

Public goods are classically underfunded by markets. Government or philanthropic funding is the standard solution.

This suggests that even if commercial incentives align for proprietary safety work, public good safety research may remain underfunded without deliberate intervention.

Possible approaches:

- **Government research institutes** dedicated to AI safety, analogous to national labs in other domains
- **Mandatory contributions** to shared safety research, similar to industry consortia in other sectors
- **Tax treatment** favoring open safety research over proprietary work
- **International cooperation** to fund global public goods in AI safety

## Who Decides Priorities?

Beyond total funding, who decides what safety work to prioritize?

Current structures concentrate priority-setting among a small number of actors: lab leadership, major philanthropists, and government officials. This creates risks of blind spots, bias, and capture.

Alternative structures could broaden priority-setting:

- **Diverse funding sources** prevent any single actor from dominating
- **Community input** on research agendas incorporates broader perspectives
- **Independent assessment** of safety gaps identifies underinvested areas
- **Rotation of decision-makers** prevents incumbency advantages

## Conclusion

AI safety is shaped by economics. Current structures create systematic underinvestment, particularly in public goods and long-term risks.

Addressing this requires changing incentives: internalizing externalities through liability, funding public goods through taxation or philanthropy, and coordinating to prevent free-riding.

The question is not just "is there enough safety" but "who pays, who decides, and what work results." Economic analysis is essential for answering these questions.

## Related Research

- [Liability Frameworks for AI Harm](/research/020-liability-frameworks/)
- [Insurance Markets and AI Risk Pricing](/research/036-insurance-markets/)
- [The Speed-Safety Tradeoff: Making the Implicit Explicit](/research/077-speed-safety-tradeoff/)
- [The Game Theory of AI Disclosure](/research/067-game-theory-disclosure/)
