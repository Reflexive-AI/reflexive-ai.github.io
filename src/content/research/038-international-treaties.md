---
title: "International AI Treaty Proposals: A Comparative Analysis"
excerpt: "From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects."
date: 2026-01-25
categories:
  - Governance Analysis
  - Public
tags:
  - regulation
  - governance
  - policy
  - jurisdiction
---

## The Case for International AI Governance

AI development is global; effective governance arguably must be too. Several features of AI suggest that purely national governance is insufficient:

**Cross-border effects.** AI systems developed in one country affect people worldwide. An AI system trained in California might make decisions about users in Germany, generate content consumed in Brazil, and run on servers in Singapore.

**Competitive dynamics.** National regulation creates competitive pressures. Countries fear that strict domestic rules will drive AI development elsewhere. This can create races to the bottom unless international coordination establishes common standards.

**Shared risks.** Some AI risks—catastrophic misalignment, biological weapons assistance, massive disinformation—transcend national boundaries. No country alone can manage risks that could affect humanity collectively.

**Concentration.** Frontier AI development is concentrated in a small number of countries, primarily the United States and China, with significant activity in the UK, France, and elsewhere. Agreements among few actors are more feasible than among many.

This analysis examines what international AI agreements have been proposed or achieved, what different proposals would accomplish, and what obstacles they face.

## Current International Arrangements

Several international AI governance arrangements already exist, though none constitutes comprehensive treaty-based governance.

### Bletchley Declaration (2023)

The AI Safety Summit at Bletchley Park produced a declaration signed by 28 countries, including the US and China. Key elements:

- Recognition that frontier AI presents safety risks
- Acknowledgment that risks are international in scope
- Commitment to cooperation on AI safety
- Establishment of ongoing international dialogue

**Significance:** The declaration established that AI safety is a legitimate topic for international engagement and achieved surprisingly broad participation. However, it is non-binding and contains no specific commitments or enforcement mechanisms.

### G7 Hiroshima AI Process

The G7 developed AI governance principles and a code of conduct for AI developers. Key features:

- Voluntary commitments for advanced AI developers
- Focus on foundation models and generative AI
- Principles covering safety, transparency, and responsible development

**Limitations:** Applies only to G7 countries and is voluntary. Doesn't address AI development outside the G7, notably China.

### OECD AI Principles

The OECD adopted AI principles in 2019, endorsed by over 40 countries:

- Human-centered values and fairness
- Transparency and explainability
- Robustness, security, and safety
- Accountability

**Significance:** Broad international endorsement of principles. However, principles are high-level and implementation is national, creating significant variation in practice.

### Council of Europe AI Convention

The Council of Europe developed a binding convention on AI and human rights:

- Applies to AI systems in the public sector
- Requires human rights impact assessments
- Establishes accountability requirements
- Opens for signature to non-Council of Europe members

**Significance:** The first binding international treaty on AI. However, scope is limited to human rights applications, and enforcement relies on domestic implementation.

### Bilateral Arrangements

Various bilateral discussions and agreements address AI:

- US-EU Trade and Technology Council discussions on AI standards
- US-UK AI safety cooperation
- Limited US-China dialogue on AI risks

These arrangements are important but fragmented and often informal.

## Proposed Treaty Models

Several models for more comprehensive international AI agreements have been proposed.

### Nuclear Analogy: IAEA for AI

Drawing on nuclear governance, some propose an International AI Agency modeled on the IAEA:

- Verification of AI development activities
- Safety standards and inspections
- Technical assistance for safe AI development
- Information sharing on risks and capabilities

**Advantages:** Proven model for governing dangerous technology. Addresses verification challenges.

**Challenges:** AI differs from nuclear technology—it's distributed, dual-use by default, and developed primarily by private actors. Inspection regimes that work for nuclear facilities translate poorly to AI labs. We explored related challenges in [compute governance](/research/023-compute-governance/).

### Arms Control Model

Some propose AI agreements modeled on arms control treaties:

- Limits on development of specific AI capabilities
- Transparency and verification measures
- Confidence-building mechanisms
- Crisis communication channels

**Advantages:** Addresses competitive dynamics between major powers. Proven template for managing dangerous technologies.

**Challenges:** AI "capabilities" are harder to define and verify than weapon systems. Dual-use nature makes drawing lines between civilian and military AI difficult. Unlike nuclear weapons, AI development doesn't require observable physical infrastructure.

### Biosecurity Model

The Biological Weapons Convention (BWC) offers another analogy:

- Prohibition of specific applications
- Confidence-building measures
- Expert review processes
- Norm development

**Advantages:** Addresses dual-use technology with civilian and military applications.

**Challenges:** The BWC's weakness—lack of verification—would likely apply to AI agreements. As we discussed in [dual-use AI in biology](/research/035-dual-use-biology/), AI makes biosecurity harder, not easier.

### Climate Model

Climate governance offers different insights:

- Framework convention establishing principles
- Conference of parties for ongoing negotiation
- Nationally determined contributions
- Transparency and review mechanisms
- Financial mechanisms for developing countries

**Advantages:** Accommodates different national circumstances. Creates ongoing engagement. Combines hard and soft law.

**Challenges:** Climate governance has achieved limited emissions reductions. AI changes faster than climate negotiation timelines accommodate.

### Internet Governance Model

Multi-stakeholder internet governance offers another template:

- Involvement of governments, companies, civil society, and technical community
- Technical standards developed through expert bodies
- Norms developed through multiple forums
- Limited formal treaty structure

**Advantages:** Flexibility and adaptability. Includes non-state actors central to AI development.

**Challenges:** Internet governance has significant gaps and conflicts. Multi-stakeholder approaches can obscure accountability.

## What Agreements Might Cover

Different aspects of AI governance might benefit from international coordination.

### Safety Standards

International standards for AI safety testing, evaluation, and deployment could reduce fragmentation and races to the bottom. This connects to our analysis of [capability evaluations](/research/024-capability-evaluations/).

### Incident Reporting

International mechanisms for reporting and sharing information about AI incidents could improve collective learning. Aviation's ICAO offers a model, as we discussed in [aviation incident reporting lessons](/research/021-aviation-lessons/).

### Prohibited Applications

Agreement on AI applications that should be universally prohibited—perhaps CBRN weapon development, mass surveillance, or autonomous weapons—could establish global norms. Our [red lines taxonomy](/research/004-red-lines-taxonomy/) identifies candidates.

### Transparency

Agreements on what AI developers should disclose could address [regulatory arbitrage](/research/008-regulatory-arbitrage/) and support consistent global oversight.

### Compute Governance

Given compute's role in frontier AI development, international coordination on compute access and tracking could support governance objectives. See our [compute governance analysis](/research/023-compute-governance/).

### Research Cooperation

Agreements could facilitate international cooperation on AI safety research, sharing knowledge that benefits global rather than national safety.

## Obstacles to Agreement

Significant obstacles impede international AI governance.

### Great Power Competition

The US-China relationship currently prioritizes competition over cooperation. Neither country is likely to accept agreements that constrain its AI development while competitors benefit.

### Verification Challenges

Unlike nuclear weapons or chemical agents, AI capabilities are difficult to observe and verify. Software can be copied and concealed. Training runs occur on distributed infrastructure. Inspections would struggle to confirm compliance.

### Definitional Difficulties

What is "AI" for treaty purposes? What capabilities warrant international concern? Drawing lines that are specific enough to be meaningful but flexible enough to remain relevant is extremely difficult.

### Private Sector Role

Unlike nuclear or military technology, AI development is driven primarily by private companies. International agreements between states don't directly bind corporate actors.

### Speed of Change

AI capabilities change faster than international negotiation processes. A treaty negotiated over years might be obsolete before ratification.

### Incentive Misalignment

Countries leading in AI development benefit from the status quo and may resist constraints. Countries lagging may see constraints as locking in disadvantage.

## Realistic Near-Term Prospects

Given these obstacles, what international AI governance might actually emerge?

### Continued Soft Law Development

Principles, guidelines, and voluntary commitments will likely continue expanding. These are easier to achieve than binding treaties but have limited enforcement.

### Bilateral and Minilateral Agreements

Smaller-scale agreements among like-minded countries are more feasible than global treaties. The US, UK, and EU might achieve meaningful coordination even if China doesn't participate.

### Technical Standards Coordination

International standards bodies (ISO, IEEE) can develop AI standards that achieve de facto international application through market adoption rather than treaty obligation.

### Issue-Specific Agreements

Narrow agreements on specific issues—perhaps autonomous weapons, biological weapons applications, or compute tracking—might be achievable even if comprehensive governance isn't.

### Crisis-Driven Progress

Major AI-related incidents might create political windows for governance progress that isn't currently possible. This is unfortunate but historically common in technology governance.

## Conclusion

International AI governance is both necessary and extraordinarily difficult. Current arrangements are fragmented and largely non-binding. Comprehensive treaty-based governance faces significant obstacles.

The realistic path forward likely involves:

- Continued development of principles and norms
- Bilateral and minilateral agreements among willing parties
- Technical standards with de facto international reach
- Issue-specific agreements where alignment exists
- Building institutional capacity for future, stronger governance

The goal should be establishing foundations for more robust international governance as political conditions permit—recognizing that those conditions may require either steady diplomatic work or crisis-driven urgency to emerge.

## Further Reading

- [Why "Just Regulate AI" Is Harder Than It Sounds](/research/018-regulation-is-hard/)
- [Regulatory Arbitrage in AI Deployment](/research/008-regulatory-arbitrage/)
- [Compute Governance: Promises and Limits](/research/023-compute-governance/)
- [Incident Reporting Systems: Lessons from Aviation](/research/021-aviation-lessons/)
