---
title: "Building AI Governance Institutions"
excerpt: "Effective AI governance requires new institutions with the right mix of independence, expertise, and enforcement power. This analysis examines existing models from nuclear, aviation, and financial regulation to derive design principles for AI governance bodies."
date: 2026-02-07
toc: true
categories:
  - Governance Analysis
tags:
  - institutions
  - ai-governance
  - regulation
  - international-cooperation
  - policy-design
version: "1.0"
---

**Reflexive Research Object 096**
*Type: Research*

## Introduction

Rules without institutions are words on paper. The EU AI Act, the U.S. executive orders, the Bletchley Declaration: all assume that some organization will interpret, implement, and enforce them. Yet the institutional infrastructure for AI governance remains thin. Most countries lack a dedicated AI regulator. No international body has binding authority over AI development. The gap between governance ambition and institutional capacity is wide and growing.

This gap is not an accident. Building effective governance institutions is difficult and slow. It requires political consensus, technical expertise, funding, and legitimacy. AI development does not wait for any of these. The result is a familiar pattern: technology outpaces governance, and governance scrambles to catch up.

This analysis examines what makes governance institutions effective, drawing on established models from nuclear safety, aviation, and financial regulation. It proposes design principles for AI governance bodies and evaluates several concrete proposals currently under discussion.

## Lessons from Existing Institutional Models

Three domains offer instructive precedents: nuclear energy, civil aviation, and financial regulation. Each faced the challenge of governing a complex, high-stakes technology with global dimensions. Each built institutions that, despite real limitations, have broadly succeeded.

### The IAEA: Inspections and Nonproliferation

The International Atomic Energy Agency, established in 1957, governs nuclear technology through a combination of safeguards agreements, inspections, and technical assistance. Its dual mandate: promote peaceful nuclear energy while preventing weapons proliferation.

Several features make the IAEA relevant to AI governance. First, it operates a verification regime. Inspectors visit nuclear facilities, monitor materials, and verify compliance with safeguards agreements. This is not self-reporting; it is independent assessment with physical access. Second, the IAEA has a clear enforcement escalation path. Noncompliance is referred to the UN Security Council, which can impose sanctions or authorize action. Third, the IAEA maintains deep technical expertise. Its staff includes nuclear physicists, engineers, and safeguards specialists who understand the technology they regulate.

The IAEA also illustrates limitations. Its authority depends on state consent. Countries that refuse to join the Treaty on the Non-Proliferation of Nuclear Weapons (NPT) or withdraw from it fall outside the regime. Verification is incomplete: states can and have cheated. The dual mandate creates tension between promotion and regulation. These limitations are instructive for AI: any international institution will face similar sovereignty constraints.

### ICAO: Standards for a Global Industry

The International Civil Aviation Organization sets standards and recommended practices for civil aviation worldwide. As we discussed in our analysis of [aviation safety lessons](/research/021-aviation-lessons/), aviation's safety record is partly a product of its institutional architecture.

ICAO works because aviation is inherently international. An aircraft manufactured in France, operated by a Korean airline, flying through Japanese airspace, and landing in Australia must meet consistent safety standards. No country can opt out of interoperability requirements without isolating its aviation sector. This creates strong incentives for compliance.

ICAO's standard-setting process involves technical committees, public comment, and adoption by member states. Standards are detailed and prescriptive: they specify everything from cockpit instrument layouts to pilot training requirements. As we noted in our [standards bodies analysis](/research/039-standards-bodies/), the specificity of standards matters enormously for their effectiveness.

For AI governance, the ICAO model suggests that institutions work best when the technology itself creates interdependencies that make cooperation rational. AI has some of these features: models trained in one jurisdiction are deployed globally, and AI-powered services cross borders constantly. But AI lacks aviation's physical interoperability requirements, which weakens the structural incentive for international coordination.

### The Basel Committee: Networked Regulation

The Basel Committee on Banking Supervision offers a different model. It is not a treaty-based organization with formal legal authority. It is a committee of central bank governors and regulators from major economies that develops voluntary standards. Yet Basel capital requirements effectively govern global banking.

The Basel model works through a combination of peer pressure, market expectations, and domestic implementation. Banks operating internationally must meet Basel standards in practice, because counterparties and markets expect it. National regulators adopt Basel standards into domestic law, giving them binding force without a treaty.

This is relevant to AI governance because it shows that [soft law can harden into effective governance](/research/040-soft-law-hard-law/) through market dynamics and mutual adoption. An AI governance body modeled on Basel would not need treaty authority. It would need the participation of the countries where frontier AI development occurs and a standard-setting process that produces technically credible, widely adopted norms.

## What Makes Institutions Effective

Across these examples, several features distinguish effective governance institutions from ineffective ones.

### Independence

An institution captured by the industry it regulates is worse than no institution at all, because it creates a false sense of oversight. This is the core concern of our [meta-governance analysis](/research/006-meta-governance-auditors/): who watches the watchmen?

Independence requires structural protections. Fixed terms for leadership. Funding that does not depend on industry fees (or, if it does, fees collected without discretion). Staff prohibited from revolving-door employment with regulated entities for meaningful cooling-off periods. Transparent decision-making processes. Governments and industry well understand these protections but frequently compromise them in practice.

For AI governance, independence is particularly challenging because the relevant expertise is concentrated in the very companies that need regulation. An AI regulator that cannot hire people who understand frontier models cannot regulate them. But regulators who hire from industry carry industry perspectives and relationships. Managing this tension requires deliberate institutional design: competitive compensation, academic fellowships, secondment programs with conflict-of-interest safeguards.

### Technical Expertise

Governance institutions must understand what they govern. The IAEA employs nuclear scientists. ICAO committees include aeronautical engineers. The Basel Committee draws on financial economists. Without deep technical knowledge, an institution cannot set meaningful standards, conduct credible evaluations, or identify emerging risks.

AI governance faces a severe expertise deficit. The number of people who genuinely understand frontier AI systems is small, and most work for the companies building them. [Capability evaluations](/research/024-capability-evaluations/) require evaluators who can probe system behavior in sophisticated ways. Setting safety standards requires understanding both what models can do and how they fail.

Solutions include investing in government AI research capacity, creating fellowship programs that rotate researchers between labs and regulators, and building evaluation infrastructure that reduces the expertise required for individual assessments. The UK AI Safety Institute and its U.S. counterpart represent early efforts in this direction.

### Enforcement Power

An institution without enforcement power is an advisory board. Advisory boards have their place, but they do not govern. Effective institutions need a spectrum of enforcement tools: guidance and warnings at the soft end, fines and operational restrictions in the middle, criminal referrals and shutdowns at the hard end.

Enforcement power must be proportionate and credible. Disproportionate penalties are never applied; they function as threats that everyone knows are empty. Credible enforcement requires a track record of actually using available tools when violations occur.

For international AI governance, enforcement is the hardest problem. No country will grant an international body the power to shut down its AI labs. The most realistic path runs through domestic enforcement of internationally agreed standards, as the Basel model demonstrates.

### Legitimacy

Institutions need legitimacy to function. Legitimacy comes from multiple sources: democratic authorization, procedural fairness, technical competence, and outcomes. An institution that makes good decisions, follows transparent processes, and has a clear legal mandate earns compliance beyond what enforcement alone achieves.

Legitimacy is especially important for AI governance because the technology affects everyone but is understood by few. Governance bodies that appear to serve narrow interests, whether industry or a particular nation, will face resistance. Multi-actor participation in governance design: governments, researchers, civil society, and affected communities, strengthens legitimacy.

## The Speed Problem

AI moves fast. Institutions move slowly. This mismatch is the central design challenge.

Traditional institution-building takes decades. The IAEA was established twelve years after the first nuclear weapons were used and seven years after the first civilian reactor. ICAO took four years from the Chicago Convention to full operation. Basel I took thirteen years of deliberation. AI governance does not have this kind of time.

Three strategies can address the speed mismatch:

**Build on existing bodies.** Rather than creating entirely new institutions, expand the mandates and capabilities of existing ones. National data protection authorities can extend their scope. Standards bodies like ISO already develop AI standards. Trade bodies like the WTO can address AI-related trade issues. This approach is faster but risks fragmentation and mandate gaps.

**Start with soft law, harden later.** Begin with voluntary commitments and non-binding standards. Use initial experience to inform binding rules. This is essentially the current approach, and it has the advantage of speed. The risk, as we analyzed in [soft law versus hard law](/research/040-soft-law-hard-law/), is that voluntary commitments remain voluntary: companies comply when convenient and defect when compliance is costly.

**Design for adaptation.** Build institutions with explicit mechanisms for updating their own rules. Delegated rulemaking authority, where an institution can issue binding technical standards without full legislative processes, allows faster response to technological change. The institution's founding legislation sets principles and boundaries; the institution fills in specifics and updates them as needed.

## Current Proposals and Their Prospects

Several specific proposals for AI governance institutions are under discussion. Each has strengths and weaknesses.

### Frontier AI Safety Board

Multiple proposals envision a body specifically focused on frontier AI systems: the most capable models from a small number of labs. This board would set safety requirements, conduct or commission evaluations, and grant deployment approvals.

**Strengths:** Narrow scope makes the institution tractable. Frontier models are developed by roughly a dozen organizations globally; regulating twelve entities is feasible. The risks from frontier systems are the most severe and the most plausible justification for new institutional machinery.

**Weaknesses:** Defining "frontier" is difficult and changes over time. A threshold-based definition risks becoming outdated. Focusing only on frontier systems ignores harms from widely deployed, less capable models. And the small number of regulated entities creates intense lobbying pressure and capture risk.

### CERN-for-AI (International AI Research Institution)

This proposal, championed in various forms by researchers and some governments, envisions a large-scale international research institution for AI safety. Like CERN in particle physics, it would pool resources, attract top researchers, and conduct safety research that no single country or company would do alone.

**Strengths:** Addresses the expertise problem directly. Creates a neutral research base that regulators can draw on. Provides career paths for safety researchers outside industry. Generates public AI safety knowledge that is not trade secrets.

**Weaknesses:** Research institutions are not regulators. CERN does not regulate particle physics; it does research. A CERN-for-AI would inform governance but would not constitute governance itself. The institution would also face enormous cost, long setup times, and political negotiation over location, staffing, and research priorities.

### National AI Safety Institutes

The UK and U.S. have established AI Safety Institutes, and other countries are following. These national bodies test frontier models, develop evaluation methods, and build government technical capacity.

**Strengths:** Can be established quickly under executive authority. Build domestic expertise. Provide concrete evaluation capabilities. Create institutional foundations that can expand over time.

**Weaknesses:** National bodies cannot address cross-border issues. Their authority depends on government priorities that shift with elections. Fragmentation across countries risks inconsistent standards and regulatory arbitrage, a problem we analyzed in [regulatory arbitrage](/research/008-regulatory-arbitrage/).

### Expanding the OECD's Role

The OECD already has AI governance programs, principles, and monitoring. Expanding its role would build on existing infrastructure.

**Strengths:** The OECD has institutional credibility, existing membership, and established processes. It already does significant AI policy work. Expansion is faster than creation.

**Weaknesses:** The OECD represents wealthy democracies. China, India, and most of the global South are not members. AI governance that excludes major AI-developing and AI-affected nations lacks legitimacy and effectiveness. The OECD also lacks enforcement power and is unlikely to acquire it.

## Multi-Level Governance Architecture

No single institution will govern AI. Effective governance requires coordination across levels.

**National regulators** set and enforce domestic rules, adapted to local legal traditions and risk tolerances. They have enforcement power and democratic legitimacy.

**Regional bodies**, such as the EU AI Office, coordinate rules across countries and create larger regulatory markets. They reduce fragmentation while preserving some local flexibility.

**International bodies** set baseline standards, coordinate cross-border issues, share information, and manage shared risks. They lack direct enforcement but influence national regulation through standard-setting and peer review.

**Technical bodies** develop detailed standards, evaluation methods, and benchmarks. They operate below the political level but shape what governance means in practice.

This multi-level architecture resembles financial regulation: Basel sets international standards, the EU implements them through directives, and national regulators enforce them domestically. A similar architecture for AI would involve international standard-setting, regional coordination, and national enforcement, as discussed in our analysis of [international treaty proposals](/research/038-international-treaties/).

The key design challenge is coordination. Multiple institutions operating at different levels must share information, avoid contradictory requirements, and fill gaps without creating redundancy. This is an institutional design problem, not a policy problem, and it receives too little attention.

## Reflexive Dimension

The Reflexive AI Initiative has a particular interest in institutional design because our core thesis depends on it. We argue that AI systems should internalize governance constraints: that safety and compliance should be built into models, not just imposed from outside (see [corporate governance for AI safety](/research/042-corporate-governance/)). But this internalization is only meaningful if there are external institutions capable of defining what constraints matter, verifying compliance, and updating requirements as the technology evolves.

Without effective institutions, "self-governance" becomes self-regulation, and self-regulation serves the regulated. The existence of credible external governance is what gives internal governance its meaning and its discipline.

We also note a recursive challenge: AI systems are increasingly used in governance itself: for monitoring, evaluation, risk assessment, and decision support. Governance institutions must therefore govern AI while using AI. This creates the possibility that the technology shapes the institutions that are supposed to shape it. Institutional design must account for this feedback loop, ensuring that AI tools used in governance are themselves subject to oversight.

## Conclusion

Building AI governance institutions is the central practical challenge of AI policy. Rules, principles, and commitments are necessary but insufficient. They need institutional homes: organizations with the independence to resist capture, the expertise to understand what they govern, the authority to enforce their decisions, and the legitimacy to command compliance.

No perfect model exists. The IAEA, ICAO, and Basel Committee all have significant limitations. But they demonstrate that effective governance institutions can be built for complex, global technologies. The design principles are known: independence, expertise, enforcement, legitimacy, and adaptability. The challenge is political, not intellectual.

The most productive near-term path is a multi-level architecture: national safety institutes building domestic capacity, international forums developing shared standards, and technical bodies creating detailed specifications. Over time, these components can formalize into binding arrangements as experience accumulates and political will solidifies. The critical imperative is to start building now, accepting imperfection, rather than waiting for the ideal design while the governance gap widens.

## References

1. International Atomic Energy Agency. "IAEA Safeguards: Staying Ahead of the Game." IAEA, 2023.
2. International Civil Aviation Organization. "Convention on International Civil Aviation (Chicago Convention)." ICAO Doc 7300, 1944.
3. Basel Committee on Banking Supervision. "History of the Basel Committee." Bank for International Settlements, 2024.
4. UK Department for Science, Innovation and Technology. "AI Safety Institute: Approach to Evaluations." UK Government, 2024.
5. OECD. "OECD AI Principles." OECD, 2019. Updated 2024.
6. Anderljung, M. et al. "Frontier AI Regulation: Managing Emerging Risks to Public Safety." arXiv:2307.03718, 2023.
7. Ho, L. et al. "International Institutions for Advanced AI." arXiv:2307.04699, 2023.
8. Trager, R. et al. "International Governance of Civilian AI: A Jurisdictional Certification Approach." arXiv:2308.15514, 2023.
9. European Commission. "EU AI Act." Regulation (EU) 2024/1689, 2024.
10. Tallinn, J. and Shulman, C. "Governance of Superintelligence." Future of Humanity Institute, 2024.
