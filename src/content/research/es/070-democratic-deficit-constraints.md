---
title: "¿Quién decide qué debería rechazar la IA? El déficit democrático en el diseño de restricciones"
excerpt: "Los rechazos de la IA codifican juicios de valor. Actualmente, pequeños equipos en los laboratorios de IA toman estas decisiones. ¿Es esto legítimo? Exploración del déficit democrático en el diseño de restricciones de la IA y posibles alternativas."
date: 2026-02-04
toc: true
categories:
  - Governance Analysis
  - Public
tags:
  - democracy
  - legitimacy
  - refusals
  - constraints
  - participation
---

## El poder oculto

Cuando un sistema de IA rechaza una solicitud, emite un juicio de valor. Ha determinado que la solicitud queda fuera de lo que debería hacer. Esta determinación refleja decisiones tomadas por alguien, en algún lugar, sobre lo que los sistemas de IA deberían y no deberían permitir.

¿Quién toma estas decisiones?

Actualmente: pequeños equipos de investigadores, personal de políticas y ejecutivos en empresas de IA. Estos equipos deciden si rechazar solicitudes sobre armas, drogas, política, sexualidad, religión e innumerables otros dominios. Sus decisiones moldean lo que miles de millones de usuarios pueden y no pueden hacer con la IA.

Esta es una concentración de poder notable. Y plantea una pregunta de legitimidad: ¿con qué autoridad estos equipos toman decisiones cargadas de valores para el mundo?

## La magnitud del problema

Consideremos la gama de decisiones incorporadas en las políticas de rechazo de la IA.

**Cuestiones políticas en disputa.** ¿Deberían los sistemas de IA discutir sobre el aborto, los derechos de armas o la inmigración? ¿Deberían negarse a ayudar con actividades legales que algunos consideran inmorales? ¿Deberían negarse a criticar gobiernos, religiones o ideologías?

**Variación cultural.** Lo que se considera ofensivo varía entre culturas. Lo que es educación sexual aceptable en un país es contenido prohibido en otro. ¿Las normas de quién deberían aplicar los sistemas globales?

**Capacidades de doble uso.** Muchas capacidades tienen usos tanto beneficiosos como dañinos. El conocimiento de química permite la medicina y las armas. La asistencia en programación permite la seguridad y los ataques. ¿Dónde deberían trazarse las líneas?

**Casos límite.** ¿Debería la IA ayudar a un adolescente a escribir un ensayo persuasivo argumentando a favor de reducir la edad para beber? ¿Debería simular ser un terapeuta? ¿Debería generar contenido que es legal pero estigmatizado?

Estas no son preguntas técnicas. Son preguntas de valores. Diferentes personas razonables, informadas por diferentes culturas, experiencias y creencias, las responderían de forma diferente.

## La situación actual

En la práctica, las empresas de IA toman estas decisiones a través de:

**Equipos internos de políticas.** Las empresas emplean profesionales de confianza y seguridad que redactan políticas de contenido y directrices de rechazo. Estos equipos tienen experiencia relevante pero representan una demografía y un conjunto de valores estrechos.

**Entrenamiento RLHF.** Evaluadores humanos califican las salidas de los modelos. Sus preferencias moldean el comportamiento del modelo. ¿Quiénes son estos evaluadores? Mayormente contratistas, a menudo en países de salarios bajos, seleccionados y gestionados por las empresas.

**Retroalimentación de red teams.** Las pruebas adversariales identifican modos de fallo. Los red teams influyen en qué comportamientos se consideran lo suficientemente problemáticos como para abordar.

**Juicio ejecutivo.** En última instancia, los ejecutivos de las empresas aprueban las políticas. Sus incentivos incluyen la posición competitiva, el riesgo regulatorio y las preocupaciones reputacionales junto con la seguridad.

**Presión regulatoria.** Los requisitos gubernamentales en diferentes jurisdicciones moldean las políticas. Esto introduce algo de participación externa pero refleja los valores de gobiernos poderosos, no necesariamente de las poblaciones afectadas.

Este proceso no es [transparente](/research/001-proportionality-disclosure/). Los usuarios no saben qué rechazarán los sistemas de IA hasta que se encuentran con los rechazos. El razonamiento detrás de las políticas rara vez se publica. No existe un mecanismo sistemático para que las partes afectadas proporcionen su opinión.

## Por qué esto importa

El déficit democrático en el diseño de restricciones importa por varias razones.

**Legitimidad.** Las decisiones que afectan a miles de millones deberían reflejar alguna forma de consentimiento o representación. La discreción corporativa pura no satisface este estándar, independientemente de cuán reflexivos sean los tomadores de decisiones.

**Precisión.** Los equipos estrechos omiten perspectivas que una participación más amplia capturaría. Las políticas de restricción desarrolladas por trabajadores tecnológicos de Silicon Valley no reflejan las necesidades y valores de usuarios en todo el mundo.

**Confianza.** Los usuarios que no entienden por qué los sistemas rechazan pueden desconfiar de ellos. La falta de participación fomenta la sospecha sobre agendas ocultas.

**Rendición de cuentas.** ¿Quién es responsable cuando las políticas de rechazo causan daño? Si un sistema se niega a discutir la prevención del suicidio porque el tema parece arriesgado, ¿quién asume la responsabilidad por la persona que necesitaba esa información?

**Sesgo.** Los equipos homogéneos reproducen sus sesgos en el diseño de restricciones. Lo que parece neutral para los profesionales de la industria tecnológica incorpora suposiciones políticas o culturales particulares.

## Posibles alternativas

Existen varias alternativas a la discreción corporativa pura, cada una con compensaciones.

### Mandatos regulatorios

Los gobiernos podrían especificar qué deben rechazar los sistemas de IA. Esto introduce participación democrática a través de representantes electos.

**Ventajas:** Proporciona legitimidad en sociedades democráticas. Crea requisitos legales claros. Permite la rendición de cuentas a través de procesos políticos.

**Desventajas:** Diferentes jurisdicciones tienen diferentes valores. Los mandatos gubernamentales pueden reflejar preferencias de la mayoría que perjudican a las minorías. La regulación se mueve lentamente mientras la tecnología evoluciona rápidamente. Los gobiernos autoritarios podrían exigir restricciones dañinas.

### Consulta pública

Las empresas podrían solicitar opiniones del público sobre las políticas de restricción antes de la implementación, similar a los procesos de comentario público en la elaboración de regulaciones.

**Ventajas:** Incorpora perspectivas diversas. Crea transparencia sobre el desarrollo de políticas. Señala respeto por las partes afectadas.

**Desventajas:** Favorece a los grupos organizados sobre los públicos difusos. No puede resolver desacuerdos profundos de valores. Podría volverse performativa en lugar de sustantiva.

### Personalización por el usuario

Dentro de ciertos límites, los usuarios podrían personalizar los niveles de restricción. Un investigador puede deshabilitar rechazos para consultas académicas. Un padre puede reforzar las restricciones para la cuenta de un hijo.

**Ventajas:** Respeta la autonomía del usuario. Reconoce la variación legítima en las necesidades. Reduce los conflictos de talla única.

**Desventajas:** Algunas restricciones no deberían ser personalizables (como las [líneas rojas](/research/004-red-lines-taxonomy/)). Los usuarios sofisticados pueden explotar la personalización para acceder a capacidades dañinas. Crea complejidad y potencial de confusión.

### Gobernanza participativa

Las políticas de restricción podrían desarrollarse a través de procesos que incluyan empresas, sociedad civil, gobiernos y representantes de usuarios. Esto se asemeja a los modelos multiactor de la [gobernanza de internet](/research/066-internet-governance-lessons/).

**Ventajas:** Incorpora perspectivas diversas institucionalmente. Crea rendición de cuentas continua. Permite la negociación entre intereses.

**Desventajas:** Lento y potencialmente capturado por actores poderosos. No existe un mecanismo claro para resolver desacuerdos. La legitimidad de los "representantes" puede ser cuestionada.

### Supervisión electa

Los órganos democráticos podrían supervisar las políticas de restricción de la IA, de manera similar a como los funcionarios electos supervisan otras instituciones poderosas.

**Ventajas:** Máxima legitimidad democrática. Rendición de cuentas clara.

**Desventajas:** Los políticos carecen de experiencia técnica. Los ciclos electorales crean inestabilidad. La tecnología global gobernada por la política nacional crea fragmentación.

## Un enfoque reflexivo

La Reflexive AI Initiative propone que los propios sistemas de IA pueden contribuir a este problema, aunque no resolverlo.

**Restricciones transparentes.** Los [esquemas de restricciones legibles por máquinas](/research/003-machine-readable-constraint-schema/) hacen las políticas explícitas y comparables. Los usuarios pueden ver qué restricciones existen en lugar de descubrirlas por ensayo y error.

**Explicación de los rechazos.** Los sistemas que [explican sus restricciones](/research/026-explaining-constraints/) ayudan a los usuarios a comprender el razonamiento detrás de los rechazos, incluso si no están de acuerdo.

**Mecanismos de retroalimentación.** Los sistemas pueden recopilar retroalimentación estructurada sobre los rechazos, identificando patrones donde las restricciones pueden ser inapropiadas.

**Auditoría de restricciones.** Los auditores independientes pueden evaluar si las restricciones declaradas coinciden con el comportamiento real, haciendo que las empresas rindan cuentas de sus políticas publicadas.

Estos mecanismos no resuelven la pregunta de legitimidad. No responden quién debería decidir. Pero crean infraestructura para la rendición de cuentas independientemente de quién decida.

## No hay solución perfecta

No existe una solución perfecta al déficit democrático. Cada alternativa tiene debilidades.

Los mandatos regulatorios requieren gobiernos que representen a sus poblaciones, estén de acuerdo entre sí y puedan mantener el ritmo de la tecnología. Estas condiciones a menudo no se cumplen.

La consulta pública puede convertirse en teatro. La personalización por el usuario puede crear asimetrías dañinas. La gobernanza multiactor puede ser capturada. La supervisión electa puede ser politizada.

La pregunta no es qué solución perfecta adoptar sino qué combinación imperfecta perseguir. Algunos principios pueden guiar esta elección:

**Transparencia.** Cualquiera que sea el proceso, las restricciones resultantes deberían ser públicas y comprensibles.

**Rendición de cuentas.** Alguien debería responder por las decisiones de restricción y sus consecuencias.

**Partes afectadas.** Quienes se ven afectados por las restricciones deberían tener alguna voz en su diseño.

**No negociables protegidos.** Las [líneas rojas](/research/004-red-lines-taxonomy/) que previenen daños catastróficos no deberían estar sujetas a la anulación del usuario ni a la negociación política.

**Adaptabilidad.** Los procesos de restricción deberían permitir la actualización a medida que mejora la comprensión y cambian los contextos.

## Conclusión

El déficit democrático en el diseño de restricciones de la IA no es un problema técnico que requiera una solución técnica. Es un problema político que requiere soluciones políticas.

Actualmente, pequeños equipos toman decisiones cargadas de valores que afectan a miles de millones. Esta concentración de poder es inevitable en el despliegue temprano de la IA, pero no es obviamente legítima.

Avanzar hacia un diseño de restricciones más legítimo requiere transparencia sobre los procesos actuales, mecanismos para una participación más amplia y rendición de cuentas por los resultados. También requiere aceptar que los desacuerdos de valores son genuinos y no se resolverán con mejores algoritmos o más datos.

El marco de gobernanza reflexiva contribuye con infraestructura para la rendición de cuentas sin resolver la pregunta subyacente de legitimidad. Esa pregunta requiere innovación política e institucional, no solo desarrollo técnico.

## Investigación relacionada

- [Un esquema de restricciones legible por máquinas](/research/003-machine-readable-constraint-schema/)
- [Líneas rojas: una taxonomía de límites no negociables de la IA](/research/004-red-lines-taxonomy/)
- [Sistemas de IA que explican sus restricciones](/research/026-explaining-constraints/)
- [Participación pública en la política de IA](/research/045-public-participation/)
- [El papel de la sociedad civil en la gobernanza de la IA](/research/044-civil-society-role/)
