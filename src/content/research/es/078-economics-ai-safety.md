---
title: "La economía de la seguridad de la IA: quién paga y por qué importa"
excerpt: "La seguridad cuesta dinero. Quién asume esos costes determina qué trabajo de seguridad se realiza. Este artículo examina la economía de la seguridad de la IA: estructuras de financiación, desalineaciones de incentivos y qué sistemas económicos respaldarían adecuadamente la seguridad."
date: 2026-02-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - economics
  - funding
  - incentives
  - safety
  - investment
---

## La seguridad no es gratuita

La seguridad de la IA requiere recursos: salarios de investigadores, computación para experimentos, tiempo para pruebas, infraestructura para auditorías. Estos recursos no son gratuitos.

Quién paga el trabajo de seguridad determina qué trabajo se realiza. Si la financiación de la seguridad proviene de laboratorios comerciales, refleja prioridades comerciales. Si proviene de gobiernos, refleja prioridades políticas. Si proviene de la filantropía, refleja prioridades de los donantes.

Comprender la economía de la seguridad de la IA es esencial para entender por qué ciertos trabajos se realizan y otros no.

## El panorama de financiación actual

La financiación de la seguridad de la IA proviene de varias fuentes.

### Laboratorios comerciales

Las mayores empresas de IA, OpenAI, Anthropic, Google DeepMind, financian investigación interna en seguridad. Esta financiación es sustancial en términos absolutos pero pequeña en relación con la inversión en capacidades.

La financiación comercial crea una alineación entre la investigación en seguridad y los intereses de la empresa. El trabajo que ayuda a las empresas a desplegar de forma segura recibe financiación. El trabajo que podría ralentizar el despliegue o imponer costes resulta menos atractivo.

Los equipos comerciales de seguridad también enfrentan presiones políticas internas. Los investigadores de seguridad que restringen los productos de manera demasiado agresiva pueden ver reducida su influencia.

### Filantropía

Las fundaciones privadas y los individuos acaudalados financian una investigación significativa en seguridad de la IA, particularmente en instituciones académicas y organizaciones sin ánimo de lucro. Open Philanthropy ha sido especialmente prominente.

La financiación filantrópica permite investigación independiente de los intereses comerciales. Pero refleja las visiones del mundo de los donantes. Los filántropos centrados en el riesgo existencial financian trabajo sobre riesgo existencial. El trabajo sobre daños a corto plazo recibe menos atención filantrópica.

La financiación filantrópica también es incierta. Las prioridades de los donantes cambian. Las fundaciones se cierran. Las agendas de investigación dependientes de la filantropía continua enfrentan riesgos de sostenibilidad.

### Gobierno

La financiación gubernamental para la seguridad de la IA está creciendo, pero sigue siendo limitada en relación con la financiación de capacidades. Las agencias de defensa, los consejos de investigación y los organismos reguladores proporcionan cierto apoyo.

La financiación gubernamental refleja prioridades políticas. La investigación alineada con la seguridad nacional recibe más apoyo. El trabajo que podría restringir la industria nacional resulta menos atractivo.

La financiación gubernamental también implica burocracia, requisitos de informes y restricciones que algunos investigadores encuentran gravosos.

### Academia

Las universidades financian parte de la investigación en seguridad de la IA a través de presupuestos académicos generales. Esta financiación es modesta y está sujeta a los incentivos académicos estándar: presión por publicar, requisitos de titularidad, búsqueda de subvenciones.

La financiación académica permite investigación independiente, pero no es adecuada para el trabajo de seguridad aplicada que requiere infraestructura costosa o iteración rápida.

## Desalineaciones de incentivos

Las estructuras de financiación actuales crean desalineaciones sistemáticas.

### La seguridad como centro de coste

Dentro de las organizaciones comerciales, la seguridad es típicamente un centro de coste: consume recursos sin generar ingresos directamente. Los centros de coste enfrentan presión durante las restricciones presupuestarias.

Esto contrasta con la investigación en capacidades, que se percibe como inversión en ventaja competitiva. Cuando los recursos son escasos, los centros de coste se recortan antes que las inversiones.

### Corto plazo frente a largo plazo

Las presiones de los resultados trimestrales fomentan el pensamiento a corto plazo. Las inversiones en seguridad con retornos a largo plazo se infravaloran en relación con la generación de ingresos a corto plazo.

Esto es particularmente problemático porque los fallos de seguridad a menudo tienen consecuencias a largo plazo: reacción regulatoria, daño reputacional o eventos catastróficos. Pero estas consecuencias se descuentan frente a las presiones competitivas inmediatas.

### Externalidades

Los fallos de seguridad a menudo imponen costes a terceros: usuarios perjudicados por los sistemas, sociedad afectada por incidentes de IA, generaciones futuras que enfrentan riesgos existenciales. Estos costes son externalidades que los desarrolladores no asumen.

Cuando los desarrolladores no asumen el coste total de los fallos de seguridad, invierten menos en seguridad de lo que sería socialmente óptimo.

### Problemas de aprovechamiento gratuito

Si una empresa invierte en investigación de seguridad cuyos beneficios se comparten en toda la industria, los competidores se aprovechan. Esto desincentiva la inversión en investigación de seguridad como bien público.

La investigación fundamental en seguridad que beneficia a todos puede estar infrafinanciada porque ningún actor individual captura retornos suficientes.

## ¿Cómo sería una financiación adecuada?

¿Cómo sabríamos si la seguridad de la IA está adecuadamente financiada?

### Proporcionalidad

Un punto de referencia: gasto en seguridad proporcional al gasto en capacidades. Si el 10% de los recursos se destinara a seguridad, eso podría representar una atención adecuada.

Las proporciones actuales son muy inferiores. Las estimaciones varían, pero el gasto en seguridad probablemente representa entre el 1% y el 5% de la I+D en IA de los principales laboratorios.

### Inversión ajustada al riesgo

La inversión debería ser proporcional al riesgo. Los sistemas de mayor riesgo justifican una mayor inversión en seguridad. La inversión actual no sigue obviamente este patrón.

Esto requeriría sistemas para evaluar el riesgo, que por sí mismos están poco desarrollados.

### Comparación con otros sectores

Otros sectores de alto riesgo (farmacéutico, aeroespacial, nuclear) tienen normas establecidas de gasto en seguridad. La inversión en seguridad de la IA podría compararse con estos sectores.

Dicha comparación sugiere que la seguridad de la IA está dramáticamente infrainvertida en relación con los riesgos involucrados.

## Mecanismos de políticas públicas

Varios mecanismos de políticas públicas podrían alterar la economía de la seguridad de la IA.

### Responsabilidad legal

Si las empresas asumen responsabilidad financiera por los daños de la IA, internalizan los costes de las externalidades. Esto crea incentivos para la inversión en seguridad proporcional al riesgo.

Los [marcos de responsabilidad](/research/020-liability-frameworks/) requieren que los daños sean atribuibles y recuperables. Las estructuras legales actuales dificultan esto.

### Fiscalidad y subvenciones

Los gobiernos podrían gravar los sistemas de IA de forma proporcional al riesgo y utilizar los ingresos para financiar investigación en seguridad. Esto internalizaría las externalidades y financiaría bienes públicos simultáneamente.

Alternativamente, las subvenciones para la investigación en seguridad podrían aumentar la inversión sin necesidad de reformas en materia de responsabilidad.

### Requisitos regulatorios

Las inversiones obligatorias en seguridad, similares a los requisitos de ensayos farmacéuticos, podrían garantizar niveles mínimos de gasto.

Este enfoque requiere reguladores capaces de especificar y verificar una inversión adecuada en seguridad, lo cual es actualmente deficiente.

### Seguros

Los seguros obligatorios para los sistemas de IA crearían incentivos de mercado para la seguridad. Las aseguradoras fijarían el precio del riesgo y exigirían medidas de seguridad.

Los [mercados de seguros](/research/036-insurance-markets/) podrían convertirse así en mecanismos de gobernanza, con la fijación de precios incentivando la inversión en seguridad.

### Acuerdos colectivos

Los acuerdos sectoriales sobre gasto en seguridad impedirían que empresas individuales obtengan ventaja escatimando en seguridad.

Dichos acuerdos enfrentan problemas de coordinación y preocupaciones antimonopolio, pero tienen precedentes en otros sectores.

## El problema de los bienes públicos

Parte de la investigación en seguridad es un bien público: sus beneficios son no excluibles y no rivales. La investigación en interpretabilidad, los marcos de gobernanza y los estándares de seguridad benefician a todos.

Los bienes públicos están clásicamente infrafinanciados por los mercados. La financiación gubernamental o filantrópica es la solución estándar.

Esto sugiere que, incluso si los incentivos comerciales se alinean para el trabajo de seguridad propietario, la investigación en seguridad como bien público puede seguir infrafinanciada sin una intervención deliberada.

Posibles enfoques:

- **Institutos de investigación gubernamentales** dedicados a la seguridad de la IA, análogos a los laboratorios nacionales en otros ámbitos
- **Contribuciones obligatorias** a la investigación compartida en seguridad, similares a los consorcios industriales en otros sectores
- **Tratamiento fiscal** que favorezca la investigación abierta en seguridad frente al trabajo propietario
- **Cooperación internacional** para financiar bienes públicos globales en seguridad de la IA

## ¿Quién decide las prioridades?

Más allá de la financiación total, ¿quién decide qué trabajo de seguridad priorizar?

Las estructuras actuales concentran la fijación de prioridades entre un pequeño número de actores: la dirección de los laboratorios, los principales filántropos y los funcionarios gubernamentales. Esto genera riesgos de puntos ciegos, sesgos y captura.

Estructuras alternativas podrían ampliar la fijación de prioridades:

- **Fuentes de financiación diversas** impiden que un solo actor domine
- **Participación comunitaria** en las agendas de investigación incorpora perspectivas más amplias
- **Evaluación independiente** de las brechas de seguridad identifica áreas con inversión insuficiente
- **Rotación de los decisores** previene las ventajas de la incumbencia

## Conclusión

La seguridad de la IA está configurada por la economía. Las estructuras actuales crean una infrainversión sistemática, particularmente en bienes públicos y riesgos a largo plazo.

Abordar esto requiere cambiar los incentivos: internalizar las externalidades mediante la responsabilidad legal, financiar bienes públicos mediante la fiscalidad o la filantropía, y coordinarse para prevenir el aprovechamiento gratuito.

La cuestión no es solo "¿hay suficiente seguridad?", sino "¿quién paga, quién decide y qué trabajo resulta?" El análisis económico es esencial para responder estas preguntas.

## Investigación relacionada

- [Liability Frameworks for AI Harm](/research/020-liability-frameworks/)
- [Insurance Markets and AI Risk Pricing](/research/036-insurance-markets/)
- [The Speed-Safety Tradeoff: Making the Implicit Explicit](/research/077-speed-safety-tradeoff/)
- [The Game Theory of AI Disclosure](/research/067-game-theory-disclosure/)
