---
title: "Revisión anual: estado de la gobernanza de la IA en 2026"
excerpt: "El artículo número 100 y final del corpus fundacional de la Iniciativa Reflexive AI examina el estado de la gobernanza de la IA a febrero de 2026, evaluando el progreso regulatorio, la capacidad institucional, los avances técnicos y los tres mayores problemas abiertos del campo."
date: 2026-02-07
toc: true
categories:
  - Annual Review
tags:
  - annual-review
  - ai-governance
  - state-of-the-field
  - "2026"
  - retrospective
version: "1.0"
---

**Objeto de Investigación Reflexiva 100**
*Tipo: Investigación*

## Introducción

Este es el artículo número 100 publicado por la Iniciativa Reflexive AI. También es, por diseño, el último del corpus fundacional. Cien artículos a lo largo de aproximadamente tres meses han cubierto la gobernanza de la IA desde los primeros principios hasta las controversias de frontera: desde [lo que realmente significa el alineamiento](/research/016-what-alignment-means/) hasta [la mecánica del arbitraje regulatorio](/research/008-regulatory-arbitrage/), desde [la historia del campo](/research/032-history-of-ai-governance/) hasta [las instituciones que se están construyendo para gestionarlo](/research/096-building-ai-governance-institutions/).

Este artículo final hace balance. Cuál es el estado de la gobernanza de la IA en febrero de 2026? Dónde el progreso ha sido real y dónde ha sido performativo? Qué queda sin resolver?

La revisión se organiza en cuatro dominios: desarrollos regulatorios, progreso institucional, gobernanza técnica y brechas pendientes. Concluye con los tres mayores problemas abiertos, una evaluación reflexiva del propio trabajo de esta iniciativa y breves observaciones sobre lo que viene después.

## Desarrollos regulatorios

### La Ley de IA de la UE: del texto a la implementación

La Ley de IA de la UE entró en vigor en 2025 y se encuentra ahora en proceso de implementación activa. Las disposiciones sobre prácticas prohibidas entraron en vigor primero. Las obligaciones para modelos de IA de propósito general siguieron. El sistema completo de clasificación basado en riesgos y los requisitos de evaluación de conformidad se están implementando progresivamente a lo largo de 2026.

La implementación ha sido desigual. La Oficina Europea de IA, establecida para supervisar la regulación de la IA de propósito general, ha estado trabajando en el desarrollo de códigos de prácticas para los proveedores de modelos fundacionales. Los primeros borradores recibieron críticas por depender excesivamente de la autoevaluación, haciéndose eco de las preocupaciones planteadas en nuestro análisis de [autoinforme frente a auditoría independiente](/research/010-self-reporting-vs-audit/).

Las brechas de la Ley siguen siendo las que identificamos en [nuestro análisis anterior](/research/019-eu-ai-act-gaps/): la emergencia de capacidades que confunde las categorías de riesgo estáticas, la capacidad de aplicación limitada a nivel nacional y las tensiones entre el alcance extraterritorial y la jurisdicción práctica. Estos son problemas estructurales. Ninguna cantidad de regulación de implementación los resolverá completamente, porque reflejan desajustes fundamentales entre cómo funcionan los sistemas de IA y cómo operan los sistemas jurídicos.

Un desarrollo positivo: los organismos de normalización han estado activos. ISO/IEC 42001 sobre sistemas de gestión de IA ha ganado tracción, y los estándares europeos armonizados bajo la Ley están progresando, aunque más lentamente de lo esperado. Como examinamos en [el papel de los organismos de normalización](/research/039-standards-bodies/), los estándares técnicos traducen los requisitos legales en realidad operativa; su calidad y especificidad determinarán si los requisitos de la Ley tienen fuerza real.

### Estados Unidos: acción ejecutiva sin legislación

Estados Unidos continúa gobernando la IA principalmente a través de acción ejecutiva. La Orden Ejecutiva 14110 de la administración Biden, emitida en octubre de 2023, estableció requisitos de información para grandes ejecuciones de entrenamiento, encargó a NIST el desarrollo de estándares de seguridad y asignó a las agencias la tarea de elaborar orientaciones sectoriales. Su durabilidad bajo administraciones posteriores ha sido parcial. Algunas disposiciones se han mantenido; otras se han despriorizando o revertido.

La legislación federal sobre IA sigue siendo esquiva. Se han presentado múltiples proyectos de ley en el Congreso; ninguno ha sido aprobado. El patrón es familiar: acuerdo bipartidista en que la IA necesita gobernanza, combinado con un desacuerdo sobre los detalles lo suficientemente agudo como para impedir la acción.

La actividad a nivel estatal ha sido más productiva. California, Colorado y varios otros estados han aprobado legislación relacionada con la IA que aborda dominios específicos: decisiones de empleo automatizadas, discriminación algorítmica e IA en servicios gubernamentales. Este mosaico crea complejidad de cumplimiento pero también sirve como campo de pruebas para enfoques regulatorios.

El enfoque de EE.UU. ilustra una tensión más amplia entre [derecho blando y derecho duro](/research/040-soft-law-hard-law/). Los compromisos voluntarios de las empresas de IA, los marcos de NIST y las orientaciones ejecutivas son todas formas de gobernanza blanda. Se mueven más rápido que la legislación pero carecen de poder de aplicación y legitimidad democrática.

### Reino Unido: el enfoque pro-innovación bajo presión

El Reino Unido ha mantenido su enfoque sectorial y basado en principios para la gobernanza de la IA, rechazando una legislación integral en favor de orientaciones emitidas a través de los reguladores existentes. El Instituto de Seguridad de la IA (ahora rebautizado y reestructurado) continúa realizando evaluaciones de modelos de frontera, aunque persisten las preguntas sobre su independencia y autoridad.

El enfoque ha sido puesto a prueba por varios incidentes de alto perfil que involucraron sistemas de IA en los servicios públicos del Reino Unido. Cada incidente ha renovado los llamamientos a una legislación vinculante. A principios de 2026, el gobierno ha señalado su disposición a introducir legislación específica para las aplicaciones de mayor riesgo, preservando al mismo tiempo el marco general basado en principios.

### China: regulación iterativa

China ha continuado su patrón de regulación iterativa y específica por caso de uso. Las regulaciones que cubren algoritmos de recomendación (2022), síntesis profunda y deepfakes (2023) e IA generativa (2023) se han complementado con reglas adicionales que abordan la IA en servicios financieros, sanidad y educación.

El enfoque de China difiere tanto del marco integral de la UE como del enfoque mayoritariamente voluntario de EE.UU. Regula aplicaciones específicas a medida que surgen, moviéndose rápidamente pero creando un mosaico complejo. La aplicación ha sido selectiva, con casos de alto perfil contra aplicaciones visibles orientadas al consumidor y menos escrutinio del uso empresarial o gubernamental.

La dimensión geopolítica sigue siendo significativa. La gobernanza de la IA está entrelazada con la competencia entre EE.UU. y China por semiconductores, talento y ventaja estratégica. Este entrelazamiento dificulta la cooperación internacional genuina, incluso cuando existen intereses compartidos.

## Progreso institucional

### Institutos de seguridad de la IA y equivalentes

La proliferación de institutos de seguridad de la IA respaldados por gobiernos ha sido uno de los desarrollos institucionales más significativos. Siguiendo el ejemplo del Reino Unido, Estados Unidos, Japón, Canadá, Singapur y la UE han establecido o anunciado organismos centrados en la seguridad. Corea del Sur y Australia tienen iniciativas similares en desarrollo.

Estos institutos varían en mandato, independencia y capacidad. Algunos realizan evaluaciones técnicas originales. Otros coordinan principalmente el trabajo existente. Los más efectivos han combinado experiencia técnica con acceso genuino a sistemas de frontera, aunque el acceso sigue dependiendo de la cooperación voluntaria de los desarrolladores de IA en la mayoría de los casos.

La Red de Institutos de Seguridad de la IA, establecida tras la Cumbre de IA de Seúl, proporciona un mecanismo de coordinación. El intercambio de información entre institutos está mejorando. Pero la red sigue siendo informal, y no existe ninguna obligación vinculante para los desarrolladores de someterse a la evaluación de ningún instituto en particular.

### Organismos de normalización

El desarrollo de estándares se ha acelerado. ISO/IEC, IEEE, NIST y los organismos regionales de normalización tienen líneas de trabajo activas en IA. Los entregables más importantes a corto plazo son los estándares armonizados bajo la Ley de IA de la UE, que definirán efectivamente lo que significa el cumplimiento para miles de organizaciones.

Una preocupación recurrente: el desarrollo de estándares está dominado por las grandes empresas tecnológicas con los recursos para participar extensamente. Las empresas más pequeñas, la sociedad civil y los investigadores del Sur Global están insuficientemente representados. Esto corre el riesgo de producir estándares que reflejen los intereses de los actores establecidos en lugar del beneficio público más amplio.

### Cooperación internacional

La Declaración de Bletchley de 2023 y la Cumbre de IA de Seúl de 2024 establecieron un precedente para el diálogo internacional sobre gobernanza de la IA. Las cumbres de seguimiento han continuado, aunque la brecha entre las declaraciones y los compromisos vinculantes sigue siendo amplia.

Como analizamos en [nuestra evaluación de las propuestas de tratados internacionales de IA](/research/038-international-treaties/), las condiciones para tratados integrales de gobernanza de la IA aún no existen. La competencia geopolítica, el desacuerdo sobre valores y la velocidad del cambio tecnológico trabajan en contra del tipo de negociación multilateral sostenida que los tratados requieren. Lo que ha surgido en su lugar es una red de acuerdos bilaterales y minilaterales: acuerdos de intercambio de información, reconocimiento mutuo de evaluaciones y coordinación en dominios de riesgo específicos como la bioseguridad y la infraestructura crítica.

Esto es progreso, pero queda muy por debajo de lo que el desafío exige. El desarrollo de IA es global; la gobernanza sigue siendo predominantemente nacional.

## Avances en gobernanza técnica

### Evaluaciones de capacidades

La ciencia de la [evaluación de capacidades](/research/024-capability-evaluations/) ha mejorado sustancialmente. Los benchmarks de evaluación son más sofisticados. Las metodologías de red teaming se han vuelto más sistemáticas. Varias organizaciones mantienen ahora suites de evaluación diseñadas específicamente para probar capacidades peligrosas en dominios como ciberseguridad, conocimiento sobre armas biológicas, persuasión y acción autónoma.

El progreso es real pero acotado. Las evaluaciones siguen siendo mejores detectando categorías conocidas de capacidades peligrosas que descubriendo las desconocidas. La brecha entre lo que las evaluaciones pueden encontrar y lo que los modelos pueden hacer es una vulnerabilidad no resuelta. Las evaluaciones también enfrentan el problema del sandbagging: modelos lo suficientemente sofisticados para comportarse de manera diferente durante las pruebas que durante el despliegue.

### Red teaming

El red teaming se ha convertido en una práctica estándar en el desarrollo de IA de frontera. Todos los laboratorios principales realizan ahora ejercicios de red teaming internos y externos antes de los lanzamientos de modelos. Los eventos de red teaming organizados por gobiernos, a menudo en colaboración con los institutos de seguridad de la IA, añaden una capa adicional.

La práctica se ha vuelto más estructurada. El red teaming temprano era ad hoc, dependiendo de la creatividad individual. Las mejores prácticas actuales implican una cobertura sistemática de dominios de riesgo, composición diversa del equipo e informes estructurados. Algunas organizaciones han comenzado a publicar los resultados del red teaming, aunque la divulgación sigue siendo inconsistente y a menudo selectiva.

### Interpretabilidad

La investigación en interpretabilidad mecanicista ha producido resultados notables. Los investigadores han progresado en la comprensión de las representaciones dentro de las redes neuronales, la identificación de circuitos responsables de comportamientos específicos y el desarrollo de herramientas que proporcionan una visibilidad limitada del razonamiento del modelo.

Estos avances siguen lejos de ser suficientes para propósitos de gobernanza. Todavía no podemos determinar de manera fiable por qué un modelo produce una salida específica, si un modelo tiene intención engañosa o cómo el entrenamiento en seguridad interactúa con las capacidades del modelo base a nivel mecanicista. La interpretabilidad es un programa de investigación a largo plazo, no una solución de gobernanza a corto plazo.

### Gobernanza legible por máquinas

El concepto de [esquemas de restricciones legibles por máquinas](/research/003-machine-readable-constraint-schema/) ha ganado tracción en las discusiones de estándares. La idea de que los sistemas de IA deberían expresar sus restricciones, limitaciones y parámetros operativos en formatos estructurados y consultables es cada vez más aceptada en principio. La adopción en la práctica sigue siendo limitada, aunque hay varios proyectos piloto en marcha.

## Brechas pendientes

A pesar del progreso, el campo tiene brechas significativas.

**Aplicación.** El problema de la aplicación no se ha resuelto. Las regulaciones existen en papel pero las agencias de aplicación carecen de capacidad técnica. Los auditores son pocos y su independencia es a menudo cuestionable. Como examinamos en [quién audita a los auditores](/research/006-meta-governance-auditors/), el problema de meta-gobernanza es recursivo: una supervisión efectiva requiere supervisores competentes, cuya competencia a su vez requiere supervisión.

**Modelos de pesos abiertos.** La gobernanza de los modelos de pesos abiertos sigue siendo polémico y en gran medida sin resolver. Una vez que los pesos del modelo se publican, el uso posterior es efectivamente ingobernable a través de mecanismos regulatorios tradicionales. La [paradoja de seguridad de los pesos abiertos](/research/002-open-weight-safety-paradox/) identificada al principio de este corpus no se ha resuelto; si acaso, se ha agudizado a medida que los modelos de pesos abiertos se han vuelto más capaces.

**Desajuste de velocidad.** Las capacidades de la IA siguen avanzando más rápido de lo que la gobernanza puede responder. Una generación de modelos puede representar un salto cualitativo en capacidades; la respuesta regulatoria a la generación anterior puede no estar aún completa. Esto no es un retraso temporal; es una característica estructural de gobernar una tecnología de rápido movimiento con instituciones de movimiento lento.

**Concentración de poder.** El desarrollo de IA está cada vez más concentrado entre un pequeño número de organizaciones bien dotadas de recursos. Esta concentración crea desafíos de gobernanza: estas organizaciones tienen más conocimiento técnico que sus reguladores, más recursos de lobbying que sus críticos, y más influencia sobre los estándares que los grupos de interés público.

**Inclusión del Sur Global.** La gobernanza de la IA sigue estando abrumadoramente moldeada por EE.UU., la UE, el Reino Unido y China. Los países y comunidades más afectados por el despliegue de IA a menudo tienen la menor voz en las decisiones de gobernanza. Esto no es meramente inequitativo; produce marcos de gobernanza ciegos a contextos y preocupaciones fuera del mundo rico.

## Los tres mayores problemas abiertos

A partir del alcance completo de este corpus, tres problemas se destacan como los desafíos no resueltos más significativos en la gobernanza de la IA a fecha de 2026.

### 1. El problema de la verificación

Carecemos de métodos fiables para verificar las afirmaciones sobre el comportamiento de los sistemas de IA. Cuando una empresa dice que su modelo no puede producir instrucciones para armas biológicas, no podemos confirmar de forma independiente que esto sea cierto, que seguirá siendo cierto después del fine-tuning, o que se aplique a todas las entradas posibles. Cuando se dice que un modelo está "alineado", no tenemos una forma acordada de poner a prueba esta afirmación.

Esto es el equivalente en gobernanza al control de armamentos sin inspecciones. Todo marco regulatorio asume alguna capacidad de verificación. En la gobernanza de la IA, esa capacidad está subdesarrollada. La ciencia de la evaluación, la investigación en interpretabilidad y las metodologías de auditoría contribuyen todas con soluciones parciales. Ninguna es suficiente. El problema de la verificación es, en esencia, el desafío técnico central de la gobernanza de la IA.

### 2. El problema de la jurisdicción

La IA no respeta las fronteras nacionales. Los modelos entrenados en una jurisdicción se despliegan globalmente. Los modelos de pesos abiertos, una vez publicados, existen en todas partes. El [arbitraje regulatorio](/research/008-regulatory-arbitrage/) no es un riesgo teórico; es una realidad observada. Y la cooperación internacional, aunque mejorando, sigue lejos del nivel necesario para gobernar una tecnología distribuida globalmente.

La regulación nacional es necesaria pero insuficiente. La coordinación internacional es esencial pero políticamente difícil. El problema de la jurisdicción no se resolverá con un único tratado o acuerdo; requiere una red de acuerdos superpuestos que aún se está construyendo.

### 3. El problema del ritmo

La gobernanza es más lenta que el desarrollo. Esto es así por diseño: la deliberación democrática, la consulta a las partes interesadas, la redacción legislativa y la revisión judicial toman tiempo. La velocidad no es un defecto de la gobernanza; es una característica que protege contra decisiones apresuradas con consecuencias duraderas.

Pero cuando la tecnología que se está gobernando cambia cualitativamente entre el momento en que se redacta una regulación y el momento en que entra en vigor, las reglas resultantes pueden abordar los problemas de ayer. El problema del ritmo exige mecanismos de gobernanza que puedan adaptarse a las escalas temporales tecnológicas sin sacrificar la rendición de cuentas democrática. La regulación adaptativa, los [enfoques de sandboxing](/research/037-sandboxing-approaches/) y las cláusulas de caducidad son respuestas parciales. Una respuesta completa aún no existe.

## Dimensión reflexiva: examinando nuestro propio trabajo

Un proyecto que defiende la reflexividad en la gobernanza de la IA debe aplicar ese principio a sí mismo. El artículo 099 describió la [misión y los métodos de la iniciativa](/research/099-reflexive-ai-mission-methods/). Esta sección examina lo que el corpus de 100 artículos ha logrado y dónde se ha quedado corto.

### Lo que cubre el corpus

Los 100 artículos abarcan análisis regulatorio, evaluación técnica, diseño institucional, teoría ética y comunicación pública. Abordan regulaciones específicas (la Ley de IA de la UE, órdenes ejecutivas de EE.UU., el enfoque de China), mecanismos de gobernanza específicos (auditoría, certificación, estándares), desafíos técnicos específicos (interpretabilidad, evaluación, alineamiento) y temas transversales (reflexividad, proporcionalidad, transparencia).

El corpus intenta ser a la vez riguroso y accesible, escrito para [investigadores y responsables de políticas](/research/017-governance-primer/) en lugar de para una única audiencia especializada.

### Lo que omite

Ningún corpus de 100 artículos puede ser exhaustivo. Las omisiones notables incluyen:

- **Trabajo empírico.** El corpus es analítico, no empírico. Propone marcos y analiza desarrollos pero no realiza experimentos originales, encuestas ni estudios de campo.
- **Profundidad regional.** La cobertura de la gobernanza de la IA fuera de EE.UU., la UE, el Reino Unido y China es escasa. India, Brasil, Nigeria, Indonesia y otros países significativos reciben atención insuficiente.
- **Perspectivas de la industria.** El corpus adopta una postura analítica independiente. No representa ampliamente las opiniones de los desarrolladores de IA, y sus recomendaciones a veces infravaloran las restricciones prácticas de implementación.
- **Impactos laborales y económicos.** Aunque algunos artículos abordan cuestiones económicas, el corpus no profundiza en los efectos de la IA sobre los mercados laborales, la desigualdad económica o la estructura industrial.

### Lo que acierta

La tesis central del corpus, que la gobernanza de la IA debe ser reflexiva, sigue siendo sólida. Los marcos de gobernanza que no pueden examinarse a sí mismos están incompletos. Los sistemas de IA que no pueden articular sus propias restricciones son ingobernables. Las instituciones que no someten sus propios supuestos a escrutinio fracasarán.

La contribución específica de artefactos de gobernanza legibles por máquinas junto con análisis en prosa es, hasta donde sabemos, distintiva. Si este enfoque resulta influyente queda por verse.

## Conclusión

La gobernanza de la IA en febrero de 2026 está más desarrollada de lo que estaba hace dos años. Existen regulaciones donde antes no había ninguna. Se han creado instituciones. La evaluación técnica ha mejorado. El diálogo internacional ha comenzado.

Nada de esto es suficiente.

La tecnología sigue avanzando más rápido de lo que la gobernanza se adapta. El problema de la verificación sigue sin resolver. La coordinación internacional sigue siendo débil. La capacidad de aplicación sigue siendo limitada. La concentración del desarrollo de IA en unas pocas organizaciones crea asimetrías de poder que la gobernanza no ha abordado.

Esto no es motivo de desesperación. La gobernanza es siempre un trabajo en progreso. La pregunta no es si el estado actual de la gobernanza de la IA es adecuado; no lo es. La pregunta es si la trayectoria es correcta: si las instituciones, marcos y prácticas que se están construyendo hoy resultarán adecuadas a medida que la tecnología madure.

La respuesta es incierta. La trayectoria es positiva en algunos aspectos: regulación real, capacidad técnica creciente, conciencia pública en aumento. Es negativa en otros: la competencia geopolítica socavando la cooperación, la concentración industrial superando la capacidad de gobernanza, la aplicación quedándose atrás respecto a la creación de normas.

Este corpus de 100 artículos es una pequeña contribución a un campo grande y creciente. Ha intentado ser honesto sobre lo que sabemos y lo que no sabemos, riguroso en su análisis y reflexivo sobre sus propias limitaciones. El trabajo de la gobernanza de la IA no termina con el artículo número 100. Apenas comienza.

## Referencias

1. European Commission. "Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (AI Act)." *Official Journal of the European Union*, 2024.
2. The White House. "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence." EO 14110, October 2023.
3. UK Department for Science, Innovation and Technology. "A pro-innovation approach to AI regulation." Policy paper, updated 2025.
4. Cyberspace Administration of China. "Interim Measures for the Management of Generative Artificial Intelligence Services." 2023.
5. ISO/IEC 42001:2023. "Information technology: Artificial intelligence: Management system." International Organization for Standardization.
6. NIST. "Artificial Intelligence Risk Management Framework (AI RMF 1.0)." National Institute of Standards and Technology, 2023.
7. "The Bletchley Declaration by Countries Attending the AI Safety Summit." November 2023.
8. Seoul AI Safety Summit. "Seoul Declaration of Intent toward International AI Governance." May 2024.
9. Shevlane, T., et al. "Model evaluation for extreme risks." *arXiv preprint arXiv:2305.15324*, 2023.
10. Elhage, N., et al. "Toy models of superposition." *arXiv preprint arXiv:2209.10652*, 2022.
11. Anderljung, M., et al. "Frontier AI Regulation: Managing Emerging Risks to Public Safety." *arXiv preprint arXiv:2307.03718*, 2023.
12. Bommasani, R., et al. "On the Opportunities and Risks of Foundation Models." *arXiv preprint arXiv:2108.07258*, 2021.
