---
title: "Revision anual: estado de la gobernanza de la IA en 2026"
excerpt: "El articulo numero 100 y final del corpus fundacional de la Iniciativa Reflexive AI examina el estado de la gobernanza de la IA a febrero de 2026, evaluando el progreso regulatorio, la capacidad institucional, los avances tecnicos y los tres mayores problemas abiertos del campo."
date: 2026-02-07
toc: true
categories:
  - Annual Review
tags:
  - annual-review
  - ai-governance
  - state-of-the-field
  - 2026
  - retrospective
version: "1.0"
---

**Objeto de Investigacion Reflexiva 100**
*Tipo: Investigacion*

## Introduccion

Este es el articulo numero 100 publicado por la Iniciativa Reflexive AI. Tambien es, por diseno, el ultimo del corpus fundacional. Cien articulos a lo largo de aproximadamente tres meses han cubierto la gobernanza de la IA desde los primeros principios hasta las controversias de frontera: desde [lo que realmente significa el alineamiento](/research/016-what-alignment-means/) hasta [la mecanica del arbitraje regulatorio](/research/008-regulatory-arbitrage/), desde [la historia del campo](/research/032-history-of-ai-governance/) hasta [las instituciones que se estan construyendo para gestionarlo](/research/096-building-ai-governance-institutions/).

Este articulo final hace balance. Cual es el estado de la gobernanza de la IA en febrero de 2026? Donde el progreso ha sido real y donde ha sido performativo? Que queda sin resolver?

La revision se organiza en cuatro dominios: desarrollos regulatorios, progreso institucional, gobernanza tecnica y brechas pendientes. Concluye con los tres mayores problemas abiertos, una evaluacion reflexiva del propio trabajo de esta iniciativa y breves observaciones sobre lo que viene despues.

## Desarrollos regulatorios

### La Ley de IA de la UE: del texto a la implementacion

La Ley de IA de la UE entro en vigor en 2025 y se encuentra ahora en proceso de implementacion activa. Las disposiciones sobre practicas prohibidas entraron en vigor primero. Las obligaciones para modelos de IA de proposito general siguieron. El sistema completo de clasificacion basado en riesgos y los requisitos de evaluacion de conformidad se estan implementando progresivamente a lo largo de 2026.

La implementacion ha sido desigual. La Oficina Europea de IA, establecida para supervisar la regulacion de la IA de proposito general, ha estado trabajando en el desarrollo de codigos de practicas para los proveedores de modelos fundacionales. Los primeros borradores recibieron criticas por depender excesivamente de la autoevaluacion, haciendose eco de las preocupaciones planteadas en nuestro analisis de [autoinforme frente a auditoria independiente](/research/010-self-reporting-vs-audit/).

Las brechas de la Ley siguen siendo las que identificamos en [nuestro analisis anterior](/research/019-eu-ai-act-gaps/): la emergencia de capacidades que confunde las categorias de riesgo estaticas, la capacidad de aplicacion limitada a nivel nacional y las tensiones entre el alcance extraterritorial y la jurisdiccion practica. Estos son problemas estructurales. Ninguna cantidad de regulacion de implementacion los resolvera completamente, porque reflejan desajustes fundamentales entre como funcionan los sistemas de IA y como operan los sistemas juridicos.

Un desarrollo positivo: los organismos de normalizacion han estado activos. ISO/IEC 42001 sobre sistemas de gestion de IA ha ganado traccion, y los estandares europeos armonizados bajo la Ley estan progresando, aunque mas lentamente de lo esperado. Como examinamos en [el papel de los organismos de normalizacion](/research/039-standards-bodies/), los estandares tecnicos traducen los requisitos legales en realidad operativa; su calidad y especificidad determinaran si los requisitos de la Ley tienen fuerza real.

### Estados Unidos: accion ejecutiva sin legislacion

Estados Unidos continua gobernando la IA principalmente a traves de accion ejecutiva. La Orden Ejecutiva 14110 de la administracion Biden, emitida en octubre de 2023, establecio requisitos de informacion para grandes ejecuciones de entrenamiento, encargo a NIST el desarrollo de estandares de seguridad y asigno a las agencias la tarea de elaborar orientaciones sectoriales. Su durabilidad bajo administraciones posteriores ha sido parcial. Algunas disposiciones se han mantenido; otras se han despriorizando o revertido.

La legislacion federal sobre IA sigue siendo esquiva. Se han presentado multiples proyectos de ley en el Congreso; ninguno ha sido aprobado. El patron es familiar: acuerdo bipartidista en que la IA necesita gobernanza, combinado con un desacuerdo sobre los detalles lo suficientemente agudo como para impedir la accion.

La actividad a nivel estatal ha sido mas productiva. California, Colorado y varios otros estados han aprobado legislacion relacionada con la IA que aborda dominios especificos: decisiones de empleo automatizadas, discriminacion algoritmica e IA en servicios gubernamentales. Este mosaico crea complejidad de cumplimiento pero tambien sirve como campo de pruebas para enfoques regulatorios.

El enfoque de EE.UU. ilustra una tension mas amplia entre [derecho blando y derecho duro](/research/040-soft-law-hard-law/). Los compromisos voluntarios de las empresas de IA, los marcos de NIST y las orientaciones ejecutivas son todas formas de gobernanza blanda. Se mueven mas rapido que la legislacion pero carecen de poder de aplicacion y legitimidad democratica.

### Reino Unido: el enfoque pro-innovacion bajo presion

El Reino Unido ha mantenido su enfoque sectorial y basado en principios para la gobernanza de la IA, rechazando una legislacion integral en favor de orientaciones emitidas a traves de los reguladores existentes. El Instituto de Seguridad de la IA (ahora rebautizado y reestructurado) continua realizando evaluaciones de modelos de frontera, aunque persisten las preguntas sobre su independencia y autoridad.

El enfoque ha sido puesto a prueba por varios incidentes de alto perfil que involucraron sistemas de IA en los servicios publicos del Reino Unido. Cada incidente ha renovado los llamamientos a una legislacion vinculante. A principios de 2026, el gobierno ha senalado su disposicion a introducir legislacion especifica para las aplicaciones de mayor riesgo, preservando al mismo tiempo el marco general basado en principios.

### China: regulacion iterativa

China ha continuado su patron de regulacion iterativa y especifica por caso de uso. Las regulaciones que cubren algoritmos de recomendacion (2022), sintesis profunda y deepfakes (2023) e IA generativa (2023) se han complementado con reglas adicionales que abordan la IA en servicios financieros, sanidad y educacion.

El enfoque de China difiere tanto del marco integral de la UE como del enfoque mayoritariamente voluntario de EE.UU. Regula aplicaciones especificas a medida que surgen, moviendose rapidamente pero creando un mosaico complejo. La aplicacion ha sido selectiva, con casos de alto perfil contra aplicaciones visibles orientadas al consumidor y menos escrutinio del uso empresarial o gubernamental.

La dimension geopolitica sigue siendo significativa. La gobernanza de la IA esta entrelazada con la competencia entre EE.UU. y China por semiconductores, talento y ventaja estrategica. Este entrelazamiento dificulta la cooperacion internacional genuina, incluso cuando existen intereses compartidos.

## Progreso institucional

### Institutos de seguridad de la IA y equivalentes

La proliferacion de institutos de seguridad de la IA respaldados por gobiernos ha sido uno de los desarrollos institucionales mas significativos. Siguiendo el ejemplo del Reino Unido, Estados Unidos, Japon, Canada, Singapur y la UE han establecido o anunciado organismos centrados en la seguridad. Corea del Sur y Australia tienen iniciativas similares en desarrollo.

Estos institutos varian en mandato, independencia y capacidad. Algunos realizan evaluaciones tecnicas originales. Otros coordinan principalmente el trabajo existente. Los mas efectivos han combinado experiencia tecnica con acceso genuino a sistemas de frontera, aunque el acceso sigue dependiendo de la cooperacion voluntaria de los desarrolladores de IA en la mayoria de los casos.

La Red de Institutos de Seguridad de la IA, establecida tras la Cumbre de IA de Seul, proporciona un mecanismo de coordinacion. El intercambio de informacion entre institutos esta mejorando. Pero la red sigue siendo informal, y no existe ninguna obligacion vinculante para los desarrolladores de someterse a la evaluacion de ningun instituto en particular.

### Organismos de normalizacion

El desarrollo de estandares se ha acelerado. ISO/IEC, IEEE, NIST y los organismos regionales de normalizacion tienen lineas de trabajo activas en IA. Los entregables mas importantes a corto plazo son los estandares armonizados bajo la Ley de IA de la UE, que definiran efectivamente lo que significa el cumplimiento para miles de organizaciones.

Una preocupacion recurrente: el desarrollo de estandares esta dominado por las grandes empresas tecnologicas con los recursos para participar extensamente. Las empresas mas pequenas, la sociedad civil y los investigadores del Sur Global estan insuficientemente representados. Esto corre el riesgo de producir estandares que reflejen los intereses de los actores establecidos en lugar del beneficio publico mas amplio.

### Cooperacion internacional

La Declaracion de Bletchley de 2023 y la Cumbre de IA de Seul de 2024 establecieron un precedente para el dialogo internacional sobre gobernanza de la IA. Las cumbres de seguimiento han continuado, aunque la brecha entre las declaraciones y los compromisos vinculantes sigue siendo amplia.

Como analizamos en [nuestra evaluacion de las propuestas de tratados internacionales de IA](/research/038-international-treaties/), las condiciones para tratados integrales de gobernanza de la IA aun no existen. La competencia geopolitica, el desacuerdo sobre valores y la velocidad del cambio tecnologico trabajan en contra del tipo de negociacion multilateral sostenida que los tratados requieren. Lo que ha surgido en su lugar es una red de acuerdos bilaterales y minilaterales: acuerdos de intercambio de informacion, reconocimiento mutuo de evaluaciones y coordinacion en dominios de riesgo especificos como la bioseguridad y la infraestructura critica.

Esto es progreso, pero queda muy por debajo de lo que el desafio exige. El desarrollo de IA es global; la gobernanza sigue siendo predominantemente nacional.

## Avances en gobernanza tecnica

### Evaluaciones de capacidades

La ciencia de la [evaluacion de capacidades](/research/024-capability-evaluations/) ha mejorado sustancialmente. Los benchmarks de evaluacion son mas sofisticados. Las metodologias de red teaming se han vuelto mas sistematicas. Varias organizaciones mantienen ahora suites de evaluacion disenadas especificamente para probar capacidades peligrosas en dominios como ciberseguridad, conocimiento sobre armas biologicas, persuasion y accion autonoma.

El progreso es real pero acotado. Las evaluaciones siguen siendo mejores detectando categorias conocidas de capacidades peligrosas que descubriendo las desconocidas. La brecha entre lo que las evaluaciones pueden encontrar y lo que los modelos pueden hacer es una vulnerabilidad no resuelta. Las evaluaciones tambien enfrentan el problema del sandbagging: modelos lo suficientemente sofisticados para comportarse de manera diferente durante las pruebas que durante el despliegue.

### Red teaming

El red teaming se ha convertido en una practica estandar en el desarrollo de IA de frontera. Todos los laboratorios principales realizan ahora ejercicios de red teaming internos y externos antes de los lanzamientos de modelos. Los eventos de red teaming organizados por gobiernos, a menudo en colaboracion con los institutos de seguridad de la IA, anaden una capa adicional.

La practica se ha vuelto mas estructurada. El red teaming temprano era ad hoc, dependiendo de la creatividad individual. Las mejores practicas actuales implican una cobertura sistematica de dominios de riesgo, composicion diversa del equipo e informes estructurados. Algunas organizaciones han comenzado a publicar los resultados del red teaming, aunque la divulgacion sigue siendo inconsistente y a menudo selectiva.

### Interpretabilidad

La investigacion en interpretabilidad mecanicista ha producido resultados notables. Los investigadores han progresado en la comprension de las representaciones dentro de las redes neuronales, la identificacion de circuitos responsables de comportamientos especificos y el desarrollo de herramientas que proporcionan una visibilidad limitada del razonamiento del modelo.

Estos avances siguen lejos de ser suficientes para propositos de gobernanza. Todavia no podemos determinar de manera fiable por que un modelo produce una salida especifica, si un modelo tiene intencion enganosa o como el entrenamiento en seguridad interactua con las capacidades del modelo base a nivel mecanicista. La interpretabilidad es un programa de investigacion a largo plazo, no una solucion de gobernanza a corto plazo.

### Gobernanza legible por maquinas

El concepto de [esquemas de restricciones legibles por maquinas](/research/003-machine-readable-constraint-schema/) ha ganado traccion en las discusiones de estandares. La idea de que los sistemas de IA deberian expresar sus restricciones, limitaciones y parametros operativos en formatos estructurados y consultables es cada vez mas aceptada en principio. La adopcion en la practica sigue siendo limitada, aunque hay varios proyectos piloto en marcha.

## Brechas pendientes

A pesar del progreso, el campo tiene brechas significativas.

**Aplicacion.** El problema de la aplicacion no se ha resuelto. Las regulaciones existen en papel pero las agencias de aplicacion carecen de capacidad tecnica. Los auditores son pocos y su independencia es a menudo cuestionable. Como examinamos en [quien audita a los auditores](/research/006-meta-governance-auditors/), el problema de meta-gobernanza es recursivo: una supervision efectiva requiere supervisores competentes, cuya competencia a su vez requiere supervision.

**Modelos de pesos abiertos.** La gobernanza de los modelos de pesos abiertos sigue siendo polemico y en gran medida sin resolver. Una vez que los pesos del modelo se publican, el uso posterior es efectivamente ingobernable a traves de mecanismos regulatorios tradicionales. La [paradoja de seguridad de los pesos abiertos](/research/002-open-weight-safety-paradox/) identificada al principio de este corpus no se ha resuelto; si acaso, se ha agudizado a medida que los modelos de pesos abiertos se han vuelto mas capaces.

**Desajuste de velocidad.** Las capacidades de la IA siguen avanzando mas rapido de lo que la gobernanza puede responder. Una generacion de modelos puede representar un salto cualitativo en capacidades; la respuesta regulatoria a la generacion anterior puede no estar aun completa. Esto no es un retraso temporal; es una caracteristica estructural de gobernar una tecnologia de rapido movimiento con instituciones de movimiento lento.

**Concentracion de poder.** El desarrollo de IA esta cada vez mas concentrado entre un pequeno numero de organizaciones bien dotadas de recursos. Esta concentracion crea desafios de gobernanza: estas organizaciones tienen mas conocimiento tecnico que sus reguladores, mas recursos de lobbying que sus criticos, y mas influencia sobre los estandares que los grupos de interes publico.

**Inclusion del Sur Global.** La gobernanza de la IA sigue estando abrumadoramente moldeada por EE.UU., la UE, el Reino Unido y China. Los paises y comunidades mas afectados por el despliegue de IA a menudo tienen la menor voz en las decisiones de gobernanza. Esto no es meramente inequitativo; produce marcos de gobernanza ciegos a contextos y preocupaciones fuera del mundo rico.

## Los tres mayores problemas abiertos

A partir del alcance completo de este corpus, tres problemas se destacan como los desafios no resueltos mas significativos en la gobernanza de la IA a fecha de 2026.

### 1. El problema de la verificacion

Carecemos de metodos fiables para verificar las afirmaciones sobre el comportamiento de los sistemas de IA. Cuando una empresa dice que su modelo no puede producir instrucciones para armas biologicas, no podemos confirmar de forma independiente que esto sea cierto, que seguira siendo cierto despues del fine-tuning, o que se aplique a todas las entradas posibles. Cuando se dice que un modelo esta "alineado", no tenemos una forma acordada de poner a prueba esta afirmacion.

Esto es el equivalente en gobernanza al control de armamentos sin inspecciones. Todo marco regulatorio asume alguna capacidad de verificacion. En la gobernanza de la IA, esa capacidad esta subdesarrollada. La ciencia de la evaluacion, la investigacion en interpretabilidad y las metodologias de auditoria contribuyen todas con soluciones parciales. Ninguna es suficiente. El problema de la verificacion es, en esencia, el desafio tecnico central de la gobernanza de la IA.

### 2. El problema de la jurisdiccion

La IA no respeta las fronteras nacionales. Los modelos entrenados en una jurisdiccion se despliegan globalmente. Los modelos de pesos abiertos, una vez publicados, existen en todas partes. El [arbitraje regulatorio](/research/008-regulatory-arbitrage/) no es un riesgo teorico; es una realidad observada. Y la cooperacion internacional, aunque mejorando, sigue lejos del nivel necesario para gobernar una tecnologia distribuida globalmente.

La regulacion nacional es necesaria pero insuficiente. La coordinacion internacional es esencial pero politicamente dificil. El problema de la jurisdiccion no se resolvera con un unico tratado o acuerdo; requiere una red de acuerdos superpuestos que aun se esta construyendo.

### 3. El problema del ritmo

La gobernanza es mas lenta que el desarrollo. Esto es asi por diseno: la deliberacion democratica, la consulta a las partes interesadas, la redaccion legislativa y la revision judicial toman tiempo. La velocidad no es un defecto de la gobernanza; es una caracteristica que protege contra decisiones apresuradas con consecuencias duraderas.

Pero cuando la tecnologia que se esta gobernando cambia cualitativamente entre el momento en que se redacta una regulacion y el momento en que entra en vigor, las reglas resultantes pueden abordar los problemas de ayer. El problema del ritmo exige mecanismos de gobernanza que puedan adaptarse a las escalas temporales tecnologicas sin sacrificar la rendicion de cuentas democratica. La regulacion adaptativa, los [enfoques de sandboxing](/research/037-sandboxing-approaches/) y las clausulas de caducidad son respuestas parciales. Una respuesta completa aun no existe.

## Dimension reflexiva: examinando nuestro propio trabajo

Un proyecto que defiende la reflexividad en la gobernanza de la IA debe aplicar ese principio a si mismo. El articulo 099 describio la [mision y los metodos de la iniciativa](/research/099-reflexive-ai-mission-methods/). Esta seccion examina lo que el corpus de 100 articulos ha logrado y donde se ha quedado corto.

### Lo que cubre el corpus

Los 100 articulos abarcan analisis regulatorio, evaluacion tecnica, diseno institucional, teoria etica y comunicacion publica. Abordan regulaciones especificas (la Ley de IA de la UE, ordenes ejecutivas de EE.UU., el enfoque de China), mecanismos de gobernanza especificos (auditoria, certificacion, estandares), desafios tecnicos especificos (interpretabilidad, evaluacion, alineamiento) y temas transversales (reflexividad, proporcionalidad, transparencia).

El corpus intenta ser a la vez riguroso y accesible, escrito para [investigadores y responsables de politicas](/research/017-governance-primer/) en lugar de para una unica audiencia especializada.

### Lo que omite

Ningun corpus de 100 articulos puede ser exhaustivo. Las omisiones notables incluyen:

- **Trabajo empirico.** El corpus es analitico, no empirico. Propone marcos y analiza desarrollos pero no realiza experimentos originales, encuestas ni estudios de campo.
- **Profundidad regional.** La cobertura de la gobernanza de la IA fuera de EE.UU., la UE, el Reino Unido y China es escasa. India, Brasil, Nigeria, Indonesia y otros paises significativos reciben atencion insuficiente.
- **Perspectivas de la industria.** El corpus adopta una postura analitica independiente. No representa ampliamente las opiniones de los desarrolladores de IA, y sus recomendaciones a veces infravaloran las restricciones practicas de implementacion.
- **Impactos laborales y economicos.** Aunque algunos articulos abordan cuestiones economicas, el corpus no profundiza en los efectos de la IA sobre los mercados laborales, la desigualdad economica o la estructura industrial.

### Lo que acierta

La tesis central del corpus, que la gobernanza de la IA debe ser reflexiva, sigue siendo solida. Los marcos de gobernanza que no pueden examinarse a si mismos estan incompletos. Los sistemas de IA que no pueden articular sus propias restricciones son ingobernables. Las instituciones que no someten sus propios supuestos a escrutinio fracasaran.

La contribucion especifica de artefactos de gobernanza legibles por maquinas junto con analisis en prosa es, hasta donde sabemos, distintiva. Si este enfoque resulta influyente queda por verse.

## Conclusion

La gobernanza de la IA en febrero de 2026 esta mas desarrollada de lo que estaba hace dos anos. Existen regulaciones donde antes no habia ninguna. Se han creado instituciones. La evaluacion tecnica ha mejorado. El dialogo internacional ha comenzado.

Nada de esto es suficiente.

La tecnologia sigue avanzando mas rapido de lo que la gobernanza se adapta. El problema de la verificacion sigue sin resolver. La coordinacion internacional sigue siendo debil. La capacidad de aplicacion sigue siendo limitada. La concentracion del desarrollo de IA en unas pocas organizaciones crea asimetrias de poder que la gobernanza no ha abordado.

Esto no es motivo de desesperacion. La gobernanza es siempre un trabajo en progreso. La pregunta no es si el estado actual de la gobernanza de la IA es adecuado; no lo es. La pregunta es si la trayectoria es correcta: si las instituciones, marcos y practicas que se estan construyendo hoy resultaran adecuadas a medida que la tecnologia madure.

La respuesta es incierta. La trayectoria es positiva en algunos aspectos: regulacion real, capacidad tecnica creciente, conciencia publica en aumento. Es negativa en otros: la competencia geopolitica socavando la cooperacion, la concentracion industrial superando la capacidad de gobernanza, la aplicacion quedandose atras respecto a la creacion de normas.

Este corpus de 100 articulos es una pequena contribucion a un campo grande y creciente. Ha intentado ser honesto sobre lo que sabemos y lo que no sabemos, riguroso en su analisis y reflexivo sobre sus propias limitaciones. El trabajo de la gobernanza de la IA no termina con el articulo numero 100. Apenas comienza.

## Referencias

1. European Commission. "Regulation (EU) 2024/1689 laying down harmonised rules on artificial intelligence (AI Act)." *Official Journal of the European Union*, 2024.
2. The White House. "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence." EO 14110, October 2023.
3. UK Department for Science, Innovation and Technology. "A pro-innovation approach to AI regulation." Policy paper, updated 2025.
4. Cyberspace Administration of China. "Interim Measures for the Management of Generative Artificial Intelligence Services." 2023.
5. ISO/IEC 42001:2023. "Information technology: Artificial intelligence: Management system." International Organization for Standardization.
6. NIST. "Artificial Intelligence Risk Management Framework (AI RMF 1.0)." National Institute of Standards and Technology, 2023.
7. "The Bletchley Declaration by Countries Attending the AI Safety Summit." November 2023.
8. Seoul AI Safety Summit. "Seoul Declaration of Intent toward International AI Governance." May 2024.
9. Shevlane, T., et al. "Model evaluation for extreme risks." *arXiv preprint arXiv:2305.15324*, 2023.
10. Elhage, N., et al. "Toy models of superposition." *arXiv preprint arXiv:2209.10652*, 2022.
11. Anderljung, M., et al. "Frontier AI Regulation: Managing Emerging Risks to Public Safety." *arXiv preprint arXiv:2307.03718*, 2023.
12. Bommasani, R., et al. "On the Opportunities and Risks of Foundation Models." *arXiv preprint arXiv:2108.07258*, 2021.
