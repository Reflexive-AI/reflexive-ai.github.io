---
title: "Regímenes de certificación para sistemas de IA"
excerpt: "¿Podrían los sistemas de IA certificarse en materia de seguridad como las aeronaves o los dispositivos médicos? Un análisis de cómo podría ser la certificación de IA, sus beneficios y sus desafíos significativos."
date: 2026-01-23
categories:
  - Governance Analysis
tags:
  - regulation
  - standards
  - safety
  - deployment
---

## La idea de la certificación

Muchas tecnologías de alto riesgo requieren certificación antes de su despliegue. Las aeronaves deben certificarse como aptas para el vuelo. Los dispositivos médicos deben recibir aprobación regulatoria. Los productos eléctricos deben cumplir normas de seguridad. Estos regímenes de certificación garantizan un nivel mínimo de seguridad antes de que los productos lleguen a los usuarios.

¿Podrían funcionar regímenes similares para la IA? Sus defensores argumentan que la certificación podría proporcionar garantía de calidad, claridad en materia de responsabilidad civil y confianza pública. Los escépticos sostienen que las características distintivas de la IA hacen impracticables los enfoques tradicionales de certificación.

Este análisis examina cómo podría ser la certificación de IA, extrayendo lecciones de los regímenes existentes y reconociendo al mismo tiempo los desafíos específicos de la IA.

## Cómo funciona la certificación en otros ámbitos

Comprender los regímenes de certificación existentes ilumina lo que podría requerir la certificación de IA.

### Aviación

La certificación aeronáutica es de las más rigurosas:

- **Certificación de tipo:** Los nuevos diseños de aeronaves se someten a una evaluación exhaustiva conforme a las normas de aeronavegabilidad
- **Certificación de fabricación:** Las instalaciones de producción deben estar certificadas en calidad
- **Certificación individual:** Cada aeronave se inspecciona antes de su operación
- **Cumplimiento continuo:** La aeronavegabilidad continuada requiere inspección y mantenimiento regulares

Este régimen es costoso y lento, pero ha logrado registros de seguridad notables. Exploramos las lecciones de la aviación para la IA en [sistemas de notificación de incidentes](/research/021-aviation-lessons/).

### Dispositivos médicos

La certificación de dispositivos médicos varía según el nivel de riesgo:

- **Dispositivos de bajo riesgo:** Autocertificación con registro
- **Dispositivos de riesgo moderado:** Evaluación de conformidad por terceros
- **Dispositivos de alto riesgo:** Aprobación regulatoria con datos clínicos

El proceso lleva de meses a años y requiere pruebas sustanciales de seguridad y eficacia.

### Seguridad de productos

Los productos de consumo normalmente requieren:

- Conformidad con normas publicadas
- Ensayos por laboratorios acreditados
- Marcado (CE, UL, etc.) que indica conformidad

Esto es menos riguroso que la aviación o los dispositivos médicos, pero proporciona una garantía básica de seguridad.

### Características comunes

En todos los ámbitos, la certificación suele incluir:

- **Normas claras:** Requisitos definidos que los productos deben cumplir
- **Ensayos:** Evidencia de que se cumplen los requisitos
- **Verificación por terceros:** Evaluación independiente, no solo autocertificación
- **Documentación:** Registros que permiten la trazabilidad
- **Cumplimiento continuo:** Los requisitos no terminan con la certificación inicial
- **Aplicación:** Consecuencias por incumplimiento o fallo

## Cómo podría ser la certificación de IA

A partir de estos modelos, la certificación de IA podría incluir varios elementos.

### Evaluación previa al despliegue

Antes del despliegue, los sistemas de IA podrían someterse a una evaluación que cubriese:

- **Evaluación de capacidades:** ¿Qué puede hacer el sistema? (en conexión con nuestro análisis de las [evaluaciones de capacidades peligrosas](/research/024-capability-evaluations/))
- **Evaluación de riesgos:** ¿Qué daños podrían producirse?
- **Ensayos:** ¿Cómo se comporta el sistema en condiciones controladas?
- **Documentación:** ¿Existe documentación adecuada sobre el entrenamiento, la arquitectura y las limitaciones?

### Aprobación del despliegue

Sobre la base de la evaluación, las autoridades de certificación podrían:

- **Aprobar:** El sistema puede desplegarse según lo propuesto
- **Aprobar con condiciones:** Despliegue permitido con restricciones
- **Requerir modificaciones:** Se necesitan cambios antes del despliegue
- **Rechazar:** No se permite el despliegue

### Supervisión continua

La certificación no terminaría con el despliegue. Los requisitos podrían incluir:

- **Supervisión del rendimiento:** Seguimiento del comportamiento del sistema en operación real
- **Notificación de incidentes:** Divulgación de los problemas que surjan (como examinamos en [lecciones de la aviación](/research/021-aviation-lessons/))
- **Recertificación:** Reevaluación periódica, especialmente tras cambios significativos
- **Aprobación de actualizaciones:** Los cambios en los sistemas desplegados requieren revisión

### Enfoques escalonados

No todas las aplicaciones de IA justifican el mismo rigor de certificación. Los enfoques escalonados podrían ajustar el nivel de escrutinio al riesgo:

- **Aplicaciones de bajo riesgo:** Autocertificación con registro
- **Aplicaciones de riesgo moderado:** Evaluación por terceros
- **Aplicaciones de alto riesgo:** Aprobación regulatoria con evidencia exhaustiva

La Ley de IA de la UE adopta este enfoque, con requisitos diferentes según los distintos niveles de riesgo.

## Beneficios potenciales

La certificación de IA podría aportar beneficios significativos.

### Garantía de calidad

La certificación establecería estándares mínimos. Los sistemas que la superasen habrían demostrado ciertas capacidades y propiedades de seguridad. Los usuarios tendrían cierta confianza en los sistemas certificados.

### Claridad en la responsabilidad civil

La certificación podría clarificar la responsabilidad civil. Los sistemas que fallan a pesar de una certificación adecuada podrían atribuir la responsabilidad de forma diferente a los sistemas que no fueron certificados o que se desplegaron pese a fallos en la certificación.

### Confianza pública

Los marcados de certificación visibles —como el marcado CE o la aprobación de la FDA— podrían generar confianza pública en los sistemas de IA. Los usuarios sabrían que los sistemas certificados han superado una revisión independiente.

### Igualdad de condiciones

Los requisitos de certificación se aplicarían por igual a todos los sistemas cubiertos. Los desarrolladores responsables no sufrirían una desventaja competitiva por cumplir estándares más altos.

### Desarrollo del mercado

Unos requisitos de certificación claros podrían en realidad acelerar la adopción de la IA al proporcionar la garantía de la que carecen los usuarios actualmente reticentes. Los sistemas sanitarios que no adoptarían IA sin certificar podrían adoptar sistemas certificados.

## Desafíos significativos

A pesar de los beneficios potenciales, la certificación de IA enfrenta desafíos significativos.

### ¿Qué se certificaría?

Definir qué se certifica es difícil. ¿Un modelo fundacional? ¿Una versión ajustada? ¿Un despliegue específico? Los sistemas de IA suelen ser altamente configurables: el mismo modelo base puede adaptarse a innumerables aplicaciones.

Esto difiere fundamentalmente de los productos físicos, donde la certificación se aplica a bienes definidos y fabricados.

### ¿Según qué normas?

La certificación requiere normas claras. Para la IA, ¿qué normas se aplicarían?

- Las normas de seguridad para IA son incipientes
- Los umbrales de capacidad son difíciles de definir
- El rendimiento "suficientemente bueno" depende del contexto
- Las compensaciones entre distintas propiedades (precisión frente a explicabilidad frente a privacidad) requieren juicio

Examinamos los desafíos normativos en [el papel de los organismos de normalización](/research/039-standards-bodies/).

### ¿Cómo funcionarían los ensayos?

Los ensayos de certificación tradicionales implican pruebas definidas según especificaciones. Para la IA:

- Los sistemas pueden comportarse de forma diferente en el despliegue que en los ensayos
- Las entradas adversarias pueden causar fallos que los ensayos estándar no detectan
- Las situaciones raras pero importantes son difíciles de someter a ensayo
- Los ensayos pueden no revelar las capacidades que identificamos en el [excedente de capacidad](/research/009-capability-overhang/)

### Deriva posterior al despliegue

Los productos certificados normalmente no cambian. Los sistemas de IA pueden derivar con el tiempo al actualizarse, ajustarse o exponerse a datos cambiantes. Lo que se certificó en el momento del despliegue puede no coincidir con lo que opera después.

### Requisitos de recursos

La certificación rigurosa es costosa. La certificación aeronáutica cuesta millones; la aprobación de dispositivos médicos lleva años. ¿Podría la certificación de IA alcanzar el rigor necesario sin imponer costes prohibitivos, especialmente a los desarrolladores más pequeños?

### Capacidad regulatoria

La certificación requiere certificadores competentes. Los reguladores necesitarían experiencia técnica en IA de la que muchos carecen actualmente. Los certificadores externos necesitarían una capacidad similar. Exploramos desafíos relacionados en [por qué la regulación es difícil](/research/018-regulation-is-hard/).

### Riesgos de elusión

La certificación podría eludirse:

- Desplegando fuera de las jurisdicciones certificadas
- Evitando el umbral para los requisitos de certificación
- Modificando los sistemas después de la certificación
- Declarando falsamente las capacidades del sistema

La aplicación enfrentaría desafíos significativos.

## Enfoques híbridos

Dados estos desafíos, la certificación pura puede ser impracticable. Los enfoques híbridos podrían ser más viables.

### Certificación de procesos

En lugar de certificar sistemas específicos, certificar procesos:

- ¿Sigue el desarrollador prácticas adecuadas de gestión del riesgo?
- ¿Existen procedimientos de ensayo apropiados?
- ¿Hay supervisión continua?

Esto se asemeja más a la certificación de sistemas de gestión ISO que a la certificación de productos.

### Evaluación de conformidad

En lugar de una aprobación binaria, la evaluación de conformidad podría proporcionar información graduada:

- ¿A qué ensayos se sometió el sistema?
- ¿Qué resultados se obtuvieron?
- ¿Qué limitaciones se identificaron?

Los usuarios podrían tomar decisiones informadas basándose en la evaluación en lugar de depender de una certificación binaria.

### Certificación continua

En lugar de una certificación puntual, la supervisión continua podría mantener el cumplimiento permanente:

- Verificación automatizada de los sistemas desplegados
- Supervisión continua del rendimiento
- Reevaluación activada cuando aparecen anomalías

Esto aborda las preocupaciones sobre la deriva posterior al despliegue.

### Autocertificación con rendición de cuentas

Para las aplicaciones de menor riesgo, exigir autocertificación con:

- Documentación publicada
- Divulgación obligatoria de limitaciones
- Responsabilidad estricta por declaraciones falsas

Esto proporciona cierta garantía sin una infraestructura regulatoria pesada.

## La dimensión reflexiva

Nuestro trabajo sobre gobernanza reflexiva sugiere posibilidades adicionales.

Los sistemas de IA podrían participar en su propia certificación:

- **Autonotificación:** Los sistemas informan de sus capacidades y limitaciones (como exploramos en la [comunicación de IA a regulador](/research/014-ai-regulator-protocol/))
- **Explicación de restricciones:** Los sistemas explican lo que están diseñados para rechazar (véase [explicar las restricciones](/research/026-explaining-constraints/))
- **Asistencia en la supervisión:** Los sistemas ayudan a detectar sus propios problemas

Esto no sustituiría la certificación externa, pero podría complementarla. Sin embargo, como examinamos en [los límites de la autoconstrucción](/research/013-limits-of-self-constraint/), la autonotificación tiene limitaciones inherentes.

## Recomendaciones

Sobre la base de este análisis, la certificación de IA debería:

**Comenzar por las aplicaciones de alto riesgo.** Centrar los requisitos iniciales de certificación en las aplicaciones donde los daños son más graves y los beneficios de la certificación están más justificados.

**Enfatizar los procesos sobre los productos.** Dada la adaptabilidad de la IA, certificar los procesos de desarrollo y despliegue responsable en lugar de versiones específicas de sistemas.

**Desarrollarse de forma incremental.** Desarrollar la capacidad de certificación a lo largo del tiempo en lugar de intentar regímenes integrales de inmediato.

**Invertir en capacidad regulatoria.** La certificación es tan buena como los certificadores. Se requiere una inversión significativa en experiencia regulatoria.

**Combinar mecanismos.** La certificación por sí sola no resolverá la gobernanza de la IA. Integrar con responsabilidad civil, supervisión y otros mecanismos de gobernanza.

**Mantener la adaptabilidad.** Cualquiera que sea el enfoque de certificación que surja, debe poder evolucionar a medida que cambian las capacidades y la comprensión de la IA.

## Conclusión

La certificación de IA ofrece beneficios potenciales, pero enfrenta desafíos significativos. Los modelos de certificación puros de otros ámbitos no se traducen directamente. Los enfoques híbridos —que enfaticen los procesos, la supervisión continua y la evaluación graduada— pueden ser más prácticos.

La certificación debería ser parte de la gobernanza de la IA, pero no puede ser su totalidad. Construir una certificación eficaz requerirá inversión sostenida, experimentación y adaptación.

## Lecturas complementarias

- [Evaluaciones de capacidades peligrosas](/research/024-capability-evaluations/)
- [Sistemas de notificación de incidentes: lecciones de la aviación](/research/021-aviation-lessons/)
- [El papel de los organismos de normalización en la gobernanza de la IA](/research/039-standards-bodies/)
- [Autonotificación frente a auditoría externa: compensaciones](/research/010-self-reporting-vs-audit/)
