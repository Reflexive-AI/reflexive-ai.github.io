---
title: "Cuando los expertos se equivocaron: humildad epistémica en las predicciones sobre IA"
excerpt: "Los expertos en IA tienen un historial deficiente de predicciones. Plazos, capacidades e impactos sociales se han juzgado mal de forma sistemática. ¿Qué debería enseñarnos esto sobre la confianza en las afirmaciones actuales?"
date: 2026-02-04
categories:
  - Governance Analysis
  - Public
tags:
  - predictions
  - epistemic-humility
  - history
  - uncertainty
  - expert-judgment
---

## Una historia de errores seguros

En 1965, Herbert Simon predijo que en veinte años las máquinas serían capaces de realizar cualquier trabajo que un ser humano pueda hacer. En 1970, Marvin Minsky dijo que en tres a ocho años tendríamos una máquina con la inteligencia general de un ser humano promedio.

No eran figuras marginales haciendo conjeturas descabelladas. Eran fundadores del campo, hablando desde una profunda experiencia.

Se equivocaron.

La historia de la IA está llena de predicciones seguras que fracasaron estrepitosamente. Comprender esta historia es esencial para evaluar las afirmaciones actuales sobre plazos, capacidades e impactos de la IA. Aconseja humildad epistémica: no escepticismo, sino cautela apropiada ante afirmaciones seguras.

## Categorías de predicciones fallidas

Los errores de los expertos se agrupan en patrones reconocibles.

### Errores en los plazos de capacidades

El error más común: predecir que las capacidades llegarían antes de lo que lo hicieron.

- **1958:** Simon y Newell: "En diez años una computadora digital será la campeona mundial de ajedrez."
  - *Realidad:* Deep Blue venció a Kasparov en 1997, casi 40 años después.

- **Años 70:** La comprensión del lenguaje natural estaba "casi resuelta."
  - *Realidad:* El rendimiento aceptable no llegó hasta la década de 2020.

- **Años 80:** Los sistemas expertos revolucionarían la medicina y el derecho en pocos años.
  - *Realidad:* La mayoría de los sistemas expertos fueron abandonados a principios de los años 90.

Estos fracasos comparten un patrón: el progreso temprano creó un optimismo que se extrapoló incorrectamente. El primer 80% del rendimiento se logró con relativa facilidad; el 20% restante llevó décadas.

### Errores en la predicción de impactos

Los expertos también juzgaron mal los impactos sociales.

- **Años 60-70:** La automatización eliminaría en gran medida el trabajo, requiriendo una reestructuración económica importante.
  - *Realidad:* El empleo siguió creciendo durante décadas. La automatización desplazó trabajos específicos mientras creaba otros.

- **Años 90:** La IA seguiría siendo una herramienta limitada sin una relevancia más amplia.
  - *Realidad:* La IA se volvió central para la economía, la política y la vida cotidiana.

- **Años 2000:** Internet había llegado a una meseta en gran medida; mejoras incrementales por delante.
  - *Realidad:* Los dispositivos móviles, las redes sociales y la IA lo transformaron todo.

Las predicciones de impacto fallaron en ambas direcciones: unas veces sobreestimando la disrupción, otras subestimándola.

### Errores en la dirección de las capacidades

Los expertos juzgaron mal qué problemas resultarían tratables.

- **La paradoja de Moravec.** Los investigadores esperaban que el razonamiento de alto nivel (ajedrez, matemáticas) fuera difícil y la percepción de bajo nivel (visión, caminar) fuera fácil. Sucedió lo contrario.

- **Lenguaje frente a razonamiento.** Muchos esperaban que los sistemas de razonamiento formal condujeran a la comprensión del lenguaje. En cambio, los modelos de lenguaje estadísticos desarrollaron capacidades que sorprendieron a los investigadores clásicos de IA.

- **Aprendizaje profundo.** A lo largo de los años 2000, la corriente principal de la IA descartó en gran medida el aprendizaje profundo como una dirección poco prometedora. Ahora domina el campo.

## Por qué los expertos se equivocan

El fracaso predictivo de los expertos tiene causas sistemáticas.

**Confusión de clase de referencia.** Los expertos extrapolan a partir del progreso reciente. Pero la clase de referencia relevante puede diferir del período de extrapolación. El progreso se acelera o desacelera de forma impredecible.

**Las partes fáciles llegan primero.** El progreso temprano suele ser más fácil que el progreso posterior. Confundir la dificultad de los hitos iniciales con la dificultad de la tarea completa produce exceso de confianza.

**Distorsiones de incentivos.** Las predicciones optimistas atraen financiación, atención y prestigio. Las predicciones pesimistas son aburridas. Los expertos están presionados para ser interesantes, no precisos.

**Anclaje.** Una vez que una predicción se ha hecho pública, los expertos se resisten a actualizarla. Admitir el error es costoso. Las predicciones se convierten en compromisos.

**Experiencia estrecha.** Los expertos en IA conocen la IA. Puede que no conozcan la economía, la política o la sociología lo suficiente como para predecir impactos sociales.

**Determinismo tecnológico.** Los expertos suelen asumir que la trayectoria de la tecnología es fija y que las fuerzas externas se ajustan. En realidad, fuerzas sociales, económicas y políticas dan forma al desarrollo tecnológico.

## El momento actual

El discurso actual sobre IA incluye predicciones seguras en todo el espectro.

**Predicciones catastrofistas.** Algunos expertos predicen que la IA transformadora plantea un riesgo existencial en años o décadas. Expresan alta confianza.

**Predicciones desdeñosas.** Otros expertos desestiman estas preocupaciones por infundadas, expresando alta confianza en que los sistemas actuales están lejos de ser peligrosos.

**Predicciones de capacidades.** Las predicciones sobre cuándo la IA alcanzará capacidades específicas (AGI, superinteligencia) van desde inminente hasta nunca.

Dado el registro histórico, ¿qué deberíamos hacer con estas afirmaciones?

## Lecciones para las predicciones actuales

La historia de las predicciones fallidas no nos dice cuáles de las predicciones actuales son erróneas. Nos dice cómo manejar las predicciones de forma adecuada.

**Descontar la confianza.** Cuando un experto expresa alta confianza en una predicción sobre IA, descuéntela. La calibración histórica es deficiente. La incertidumbre apropiada es más amplia de lo que los expertos suelen expresar.

**Considerar trayectorias.** Algunos pronosticadores tienen mejores trayectorias que otros. Pero incluso las buenas trayectorias en un dominio pueden no transferirse a situaciones novedosas.

**Ponderar las colas.** Las predicciones sobre medianas (¿cuándo llegará la capacidad X?) pueden ser menos importantes que las predicciones sobre las colas (¿cuáles son los mejores y peores escenarios?). Los cisnes negros importan.

**Distinguir capacidad de impacto.** Predecir lo que la IA puede hacer y predecir qué efecto tendrá son problemas diferentes. El segundo es más difícil.

**Observar las estructuras de incentivos.** ¿Quién se beneficia de que se crea la predicción? Las predicciones que se alinean con los intereses del predictor merecen un escrutinio adicional.

**Buscar el desacuerdo.** Cuando los expertos discrepan, no todos pueden tener razón. Comprender por qué persiste el desacuerdo es más informativo que tomar partido.

## Qué significa esto para la gobernanza

La humildad epistémica tiene implicaciones para la gobernanza.

**No esperar a la certeza.** Si esperamos hasta que las predicciones sean seguras, esperamos demasiado. La gobernanza debe avanzar bajo incertidumbre.

**Construir sistemas robustos.** Una gobernanza diseñada para un único futuro previsto puede fallar si esa predicción es errónea. La gobernanza robusta funciona razonablemente en una variedad de escenarios.

**Preservar la opcionalidad.** Comprometerse irrevocablemente con una predicción cierra opciones si esa predicción falla. La gobernanza debería mantener flexibilidad.

**Monitorizar y adaptar.** Dado que las predicciones serán erróneas, la gobernanza debe monitorizar la realidad y adaptarse. Las reglas estáticas diseñadas para condiciones previstas divergirán de las condiciones reales.

**Institucionalizar la humildad epistémica.** Los procesos de gobernanza deberían incluir mecanismos para visibilizar la incertidumbre, cuestionar las afirmaciones seguras y actualizar con base en la evidencia.

**Distinguir niveles de certeza.** Algunas afirmaciones merecen más confianza que otras. "La IA puede generar texto" es más seguro que "la IA causará la extinción humana para 2040". La gobernanza debería rastrear estas distinciones.

## La paradoja de la humildad

La humildad epistémica conlleva sus propios riesgos.

Si nos volvemos demasiado inciertos sobre las trayectorias de la IA, podemos no actuar cuando la acción está justificada. La parálisis también es un modo de fallo.

Si descartamos las predicciones de los expertos por completo, perdemos el valor de la experiencia. Los expertos se equivocan a menudo, pero siguen sabiendo más que los no expertos.

La solución no es ignorar las predicciones sino manejarlas adecuadamente: como conjeturas informadas, no como certezas; como entradas para las decisiones, no como sus determinantes; como hipótesis que monitorizar, no como hechos que asumir.

## Conclusión

La historia de las predicciones sobre IA es una historia de errores seguros. Los fundadores del campo, trabajando desde una profunda experiencia, hicieron predicciones que fallaron por décadas.

Esta historia no nos dice que las predicciones actuales sean erróneas. Nos dice que las predicciones actuales podrían ser erróneas, incluso cuando se expresan con confianza por parte de expertos acreditados.

Para la gobernanza, esto aconseja robustez sobre optimización, adaptación sobre planificación estática, y humildad sobre certeza. Deberíamos actuar pese a la incertidumbre, pero actuar de formas que reconozcan que nuestras predicciones pueden fallar.

La misma humildad epistémica se aplica reflexivamente: este análisis también podría estar equivocado. Quizás las predicciones actuales sean más fiables que las pasadas. Quizás el campo haya aprendido. Mantener incluso esta meta-afirmación con la incertidumbre apropiada es lo que la humildad exige.

## Investigación relacionada

- [Why AI Safety Researchers Disagree: A Taxonomy of Worldviews](/research/064-ai-safety-worldviews/)
- [The Capability Overhang Problem](/research/009-capability-overhang/)
- [Simulating Governance: Using AI to Stress-Test AI Regulations](/research/072-simulating-governance/)
