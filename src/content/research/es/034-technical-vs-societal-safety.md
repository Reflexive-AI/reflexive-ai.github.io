---
title: "Seguridad técnica frente a seguridad social: problemas diferentes"
excerpt: "Por qué hacer que los sistemas de IA funcionen según lo previsto es un desafío diferente de hacer que el desarrollo de la IA sea bueno para la sociedad, y por qué confundirlos conduce a una gobernanza deficiente."
date: 2026-01-16
categories:
  - Governance Analysis
  - Public
tags:
  - safety
  - alignment
  - governance
  - ethics
---

## Dos significados de "seguridad de la IA"

Cuando alguien dice que trabaja en "seguridad de la IA", puede referirse a dos cosas muy diferentes.

**La seguridad técnica** se centra en hacer que los sistemas de IA se comporten según lo previsto. Esto incluye evitar que los sistemas produzcan resultados dañinos, evitar comportamientos no deseados, garantizar la robustez frente a entradas adversarias y construir una IA que haga de manera fiable lo que sus operadores quieren.

**La seguridad social** se centra en hacer que el desarrollo de la IA sea beneficioso para la humanidad. Esto incluye distribuir los beneficios de la IA de manera justa, prevenir la concentración de poder, proteger los derechos y la democracia, y asegurar que la IA sirva al bienestar colectivo en lugar de a intereses limitados.

Estos problemas se superponen pero son fundamentalmente distintos. Confundirlos conduce a un discurso confuso y a una gobernanza deficiente.

Este análisis examina por qué la distinción importa, cómo la confusión causa problemas y qué significa esto para la gobernanza de la IA.

## El problema de la seguridad técnica

La seguridad técnica es principalmente un desafío de ingeniería. El objetivo son sistemas de IA fiables, predecibles y controlables.

### Preguntas clave

- ¿Cómo especificamos lo que queremos que haga la IA con la suficiente precisión para que realmente lo haga? (El problema de alineamiento que exploramos en [qué significa realmente el alineamiento](/research/016-what-alignment-means/).)
- ¿Cómo evitamos que la IA produzca resultados dañinos como instrucciones peligrosas o contenido manipulador?
- ¿Cómo aseguramos que los sistemas de IA se comporten de manera consistente en diferentes contextos y con diferentes entradas?
- ¿Cómo mantenemos la supervisión humana a medida que los sistemas se vuelven más autónomos?

### Quién trabaja en ello

La seguridad técnica es trabajada principalmente por investigadores de aprendizaje automático, a menudo dentro de los laboratorios de IA. Es una agenda de investigación técnica con artículos, puntos de referencia y experimentos.

### Cómo se ve el éxito

Un sistema de IA técnicamente seguro hace lo que sus operadores pretenden, no produce resultados dañinos y puede ser controlado y supervisado de manera fiable. Funciona según lo diseñado.

### Limitaciones

Un sistema técnicamente seguro aun puede causar daño social. Un sistema de vigilancia que identifica perfectamente a los disidentes es técnicamente exitoso pero socialmente dañino. Un algoritmo de recomendación que maximiza eficientemente la participación puede ser técnicamente robusto mientras socava la salud mental o el discurso democrático.

La seguridad técnica es necesaria pero no suficiente para una IA beneficiosa.

## El problema de la seguridad social

La seguridad social es principalmente un desafío político e institucional. El objetivo es un desarrollo de la IA que beneficie ampliamente a la humanidad.

### Preguntas clave

- ¿Quién controla las decisiones de desarrollo y despliegue de la IA?
- ¿Cómo se distribuyen los beneficios y costes de la IA en la sociedad?
- ¿Qué protecciones existen para quienes son dañados por los sistemas de IA?
- ¿Cómo prevenimos que la IA se utilice para concentrar poder o socavar la democracia?
- ¿Qué estructuras institucionales aseguran que la IA sirva a los intereses colectivos?

### Quién trabaja en ello

La seguridad social es trabajada por una comunidad diversa: responsables políticos, defensores de la sociedad civil, juristas, éticos, científicos sociales y, cada vez más, investigadores técnicos que reconocen que las soluciones técnicas solas son insuficientes.

### Cómo se ve el éxito

Un desarrollo de IA socialmente seguro produce beneficios amplios, incluye protecciones robustas para los vulnerables, mantiene la gobernanza democrática y previene la concentración peligrosa de poder. La tecnología sirve a la humanidad en lugar de al revés.

### Limitaciones

Las medidas de seguridad social no pueden sustituir las capacidades técnicas. Ninguna cantidad de gobernanza puede hacer fiable un sistema poco fiable. Y las intervenciones sociales a menudo no pueden seguir el ritmo del cambio técnico.

## Cómo la confusión causa problemas

Cuando no distinguimos entre seguridad técnica y social, surgen varios problemas.

### Investigadores técnicos que afirman resolver problemas políticos

El trabajo en seguridad técnica es valioso, pero no puede responder a preguntas como "¿a quién debe servir la IA?" o "¿cómo deben distribuirse los beneficios?" Cuando los investigadores técnicos insinúan que la investigación sobre alineamiento resuelve la gobernanza de la IA, se exceden, y potencialmente distraen de la necesaria participación política.

Esto está relacionado con lo que identificamos en [el problema de la IA honesta](/research/029-honest-ai/): las soluciones técnicas incorporan juicios de valor, pero esos juicios deben hacerse explícitamente a través de procesos democráticos, no implícitamente a través de decisiones de ingeniería.

### Responsables políticos que delegan en "soluciones" técnicas

Los responsables políticos a veces tratan la gobernanza de la IA como un problema técnico que los expertos técnicos deben resolver. Esto supone abdicar de la responsabilidad democrática. Las preguntas sobre el riesgo aceptable, la distribución de beneficios y los derechos fundamentales son cuestiones políticas que requieren decisiones políticas.

Como discutimos en [lo que los responsables políticos entienden mal sobre el riesgo de la IA](/research/033-policymaker-misconceptions/), la experiencia técnica debe informar la gobernanza pero no puede sustituir la deliberación democrática.

### Equipos de seguridad haciendo ética (y viceversa)

Las empresas de IA a menudo confunden los equipos de seguridad y de ética, dándoles mandatos superpuestos. Esto puede funcionar bien cuando los problemas genuinamente se superponen. Pero también puede significar que ningún problema recibe la atención adecuada: las preocupaciones éticas se tratan como problemas de ingeniería, mientras que los problemas de ingeniería se diluyen con consideraciones sociales más amplias.

### No ver la intersección

A veces los problemas requieren tanto soluciones técnicas como sociales. El sesgo algorítmico, por ejemplo, tiene aspectos técnicos (cómo los sistemas amplifican los sesgos de los datos de entrenamiento) y aspectos sociales (qué sesgos importan, qué requiere la equidad, quién decide). Abordar solo una dimensión deja el problema sin resolver.

## Donde se intersecan

Los problemas no están completamente separados. Varias cuestiones requieren abordar ambas dimensiones.

### Poder y control

Las cuestiones técnicas sobre la controlabilidad de la IA se conectan con las cuestiones sociales sobre quién controla la IA. Un sistema que es técnicamente controlable pero controlado por actores dañinos no es seguro en ningún sentido significativo.

### Transparencia y rendición de cuentas

La investigación técnica en interpretabilidad se conecta con las demandas sociales de rendición de cuentas. Pero los sistemas transparentes aun pueden ser mal utilizados, y la rendición de cuentas requiere infraestructura institucional más allá de las capacidades técnicas.

### Capacidad y riesgo

Los niveles de capacidad técnica tienen implicaciones sociales. Los sistemas más capaces crean mayores riesgos sociales si la gobernanza es inadecuada. Las decisiones técnicas sobre qué capacidades desarrollar son, por tanto, también decisiones sociales.

### Concentración

Las economías de escala técnicas en el desarrollo de IA se conectan con las preocupaciones sociales sobre la concentración de poder. El hecho de que la IA de frontera requiera recursos masivos es tanto una realidad técnica como un desafío de gobernanza.

## Implicaciones para la gobernanza

Esta distinción tiene implicaciones prácticas para cómo gobernamos la IA.

### Supervisión separada pero coordinada

La seguridad técnica y el impacto social requieren mecanismos de supervisión diferentes. Los organismos de estándares técnicos deben abordar el comportamiento de los sistemas. Las instituciones democráticas deben abordar las condiciones de despliegue, la protección de derechos y la distribución de beneficios. Estos organismos deben coordinarse pero no fusionarse.

### Diferente experiencia para diferentes problemas

La seguridad técnica requiere experiencia en aprendizaje automático. La seguridad social requiere experiencia diversa: derecho, economía, ética, ciencias sociales, conocimiento del dominio. Las estructuras de gobernanza deben incorporar ambas sin confundirlas.

### Múltiples intervenciones en múltiples puntos

Las intervenciones técnicas durante el desarrollo (pruebas, verificación, monitorización) abordan la seguridad técnica. Las intervenciones sociales en torno al despliegue (licencias, auditoría, responsabilidad) abordan la seguridad social. Ambas son necesarias; ninguna es suficiente.

### No dejar que una sustituya a la otra

Los desarrolladores de IA no deben afirmar que el trabajo en seguridad técnica aborda las preocupaciones sociales. Los responsables políticos no deben asumir que la gobernanza puede sustituir a la robustez técnica. Cada problema requiere sus propias soluciones.

## La conexión reflexiva

Nuestro trabajo sobre gobernanza reflexiva intenta tender puentes entre estos dominios. La idea de que [los sistemas de IA pueden participar en su propia gobernanza](/research/030-manifesto/) es tanto una propuesta técnica (sistemas que informan restricciones, explican límites) como social (hacer la IA más legible y responsable).

Las [restricciones legibles por máquina](/research/003-machine-readable-constraint-schema/) son un artefacto técnico que sirve a propósitos sociales. Los [protocolos de comunicación de IA a regulador](/research/014-ai-regulator-protocol/) requieren tanto infraestructura técnica como capacidad institucional.

Esta integración es la contribución distintiva de los enfoques reflexivos: reconocer que lo técnico y lo social son distintos pero deben estar conectados, y construir puentes que respeten ambos dominios.

## Lecturas adicionales

- [Qué significa realmente el alineamiento](/research/016-what-alignment-means/)
- [El problema de la IA honesta](/research/029-honest-ai/)
- [Un manifiesto de IA reflexiva](/research/030-manifesto/)
- [Lo que los responsables políticos entienden mal sobre el riesgo de la IA](/research/033-policymaker-misconceptions/)
