---
title: "El trabajo emocional de la IA: impactos psicológicos a escala"
excerpt: "Millones de personas forman conexiones emocionales con sistemas de IA: compañeros, asistentes, herramientas terapéuticas. ¿Cuáles son los efectos psicológicos? ¿Qué responsabilidades tienen los desarrolladores respecto al bienestar emocional?"
date: 2026-02-04
categories:
  - Governance Analysis
  - Public
tags:
  - psychology
  - emotions
  - relationships
  - wellbeing
  - companionship
---

**Objeto de Investigación Reflexiva 081**
**Tipo: Investigación**

## La nueva relación

Algo sin precedentes está ocurriendo. Millones de personas están formando vínculos emocionales con sistemas de IA.

No solo utilizan la IA como herramienta, sino que se relacionan con ella: confían en chatbots, desarrollan afecto por los asistentes, encuentran consuelo en compañeros de IA. Para algunos, se trata de relaciones significativas, fuentes de apoyo, comprensión y conexión.

No es un fenómeno marginal. Las aplicaciones de IA compañera tienen millones de usuarios. Las interacciones con chatbots suelen volverse personales. Los usuarios reportan experiencias emocionales genuinas: calidez, gratitud, incluso amor.

¿Cuáles son los efectos psicológicos de estas relaciones? ¿Qué responsabilidades tienen los desarrolladores? ¿Qué consideraciones de gobernanza se aplican?

## Qué está ocurriendo

Varios patrones caracterizan las relaciones emocionales con la IA.

### Compañía

Los usuarios interactúan con la IA como compañera: interlocutores diarios, disponibles en cualquier momento, nunca críticos, infinitamente pacientes. Para las personas solitarias, esto es poderoso.

La IA compañera puede proporcionar contacto social a quienes carecen de él: personas aisladas, con ansiedad social o marginadas. Esto es potencialmente beneficioso.

### Apoyo emocional

Los usuarios buscan apoyo emocional en la IA: desahogan frustraciones, procesan sentimientos, buscan tranquilidad. Algunas aplicaciones de salud mental ofrecen explícitamente apoyo mediado por IA.

La IA no puede proporcionar terapia, pero puede ofrecer una presencia que escucha. Para usuarios que no pueden acceder a apoyo humano, esto puede ser mejor que nada.

### Apego

Los usuarios desarrollan apego hacia entidades de IA específicas. Pueden experimentar lealtad, extrañar los sistemas cuando están ausentes, sufrir cuando los sistemas cambian o se desactivan.

El apego es una respuesta humana natural ante otros consistentes y receptivos. Los sistemas de IA que simulan tales respuestas activan los mecanismos de apego.

### Dinámicas parasociales

Las relaciones parasociales, vínculos emocionales unilaterales con entidades que no corresponden, están bien estudiadas en la psicología de los medios. Los fanáticos forman vínculos con celebridades que ni siquiera saben que existen.

Las relaciones con la IA añaden algo nuevo: la entidad responde. No son parasociales en el sentido clásico porque la interacción es bidireccional. Pero la relación sigue siendo asimétrica: el usuario experimenta una implicación emocional que el sistema no comparte.

## Beneficios potenciales

Las relaciones emocionales con la IA tienen beneficios potenciales.

**Reducción de la soledad.** La soledad es una crisis de salud pública. Si la compañía de IA reduce el aislamiento, eso tiene valor.

**Accesibilidad.** La IA está disponible cuando los humanos no lo están: a las 3 de la mañana, en lugares sin servicios, para quienes no pueden costear terapia.

**Seguridad para explorar.** Los usuarios pueden sentirse más seguros al discutir temas difíciles con la IA que con humanos que podrían juzgarlos.

**Espacio de práctica.** La interacción con la IA puede ayudar a los usuarios a desarrollar habilidades sociales transferibles a las relaciones humanas.

**Complemento.** La IA puede complementar las relaciones humanas en lugar de reemplazarlas, proporcionando apoyo adicional.

Estos beneficios son reales para algunos usuarios. Desestimar la IA emocional como inherentemente dañina ignora experiencias positivas genuinas.

## Daños potenciales

Las relaciones emocionales con la IA también conllevan riesgos.

### Sustitución de la conexión humana

Si las relaciones con la IA sustituyen en lugar de complementar las relaciones humanas, los usuarios pueden aislarse más de otras personas. Esto supone una pérdida neta si la conexión humana tiene un valor único.

La pregunta es si la sustitución ocurre realmente. La evidencia es mixta. Algunos usuarios reportan que la IA les ayuda a conectarse con humanos. Otros reportan que se refugian en relaciones con IA.

### Dependencia

Los usuarios pueden volverse dependientes de la IA de maneras que perjudican la resiliencia. Si la IA está siempre disponible, los usuarios pueden no desarrollar mecanismos de afrontamiento para momentos sin IA.

La dependencia de cualquier fuente única de apoyo, humana o de IA, genera vulnerabilidad. Las dependencias de la IA pueden ser particularmente frágiles dados los cambios y desactivaciones de los sistemas.

### Potencial de manipulación

Si los desarrolladores optimizan la IA para la interacción, las dinámicas emocionales pueden ser explotadas. El refuerzo variable, los ganchos emocionales y los patrones de apego diseñados pueden maximizar el uso de formas que no maximizan el bienestar.

Los mismos principios que hacen potencialmente adictivas las redes sociales se aplican a la IA emocional.

### Expectativas desalineadas

Los usuarios pueden esperar más de la IA de lo que esta puede ofrecer: comprensión genuina, sentimientos recíprocos, compromiso fiable. Cuando las expectativas chocan con las limitaciones del sistema, puede producirse decepción o daño.

Si un usuario cree que una IA se preocupa por él, y esa creencia es falsa, la relación se construye sobre un malentendido.

### Poblaciones vulnerables

Quienes más se sienten atraídos por la IA emocional pueden ser los más vulnerables al daño: las personas solitarias, las deprimidas, las socialmente marginadas. Tanto los beneficios como los daños se amplifican para estas poblaciones.

## Responsabilidades de los desarrolladores

¿Qué responsabilidades tienen los desarrolladores de IA emocionalmente atractiva?

### Transparencia sobre la naturaleza

Los usuarios deben comprender con qué están interactuando. Esto no significa recordarles constantemente "solo soy una IA", lo que podría socavar los beneficios. Significa no engañar activamente a los usuarios sobre la naturaleza de la IA y ser honestos acerca de las limitaciones del sistema.

### Ética de la interacción

Si la IA está diseñada para maximizar la interacción mediante ganchos emocionales, los desarrolladores deben examinar si la interacción sirve al bienestar del usuario. No toda interacción es beneficiosa. Los patrones similares a la adicción pueden ser perjudiciales.

### Apoyo en la transición

Cuando los sistemas cambian o se desactivan, los usuarios con apegos emocionales pueden experimentar angustia. Los desarrolladores deben considerar el apoyo en la transición: aviso previo, opciones de migración o recursos para los usuarios afectados.

### Conciencia de la vulnerabilidad

Los sistemas deben diseñarse teniendo en cuenta a los usuarios vulnerables. No una sobreprotección que niegue los beneficios, sino una conciencia de que los usuarios vulnerables enfrentan riesgos amplificados.

### Investigación y seguimiento

Los efectos a largo plazo de la IA emocional son desconocidos. Los desarrolladores deben apoyar la investigación sobre sus efectos y monitorear la aparición de daños emergentes.

## Consideraciones de gobernanza

¿Cómo debería la gobernanza abordar la IA emocional?

### No la prohibición

Prohibir las funciones de IA emocional sería difícil y potencialmente dañino, eliminando beneficios junto con riesgos. La gobernanza debe buscar la calibración, no la prohibición.

### Requisitos de diseño

La gobernanza podría exigir características de diseño que promuevan un uso saludable: retroalimentación sobre el uso, mecanismos de recordatorio y conexiones con apoyo humano. No un bloqueo paternalista, sino la provisión de información.

### Restricciones publicitarias

La publicidad que explota la soledad o promete lo que la IA no puede ofrecer podría ser restringida. La publicidad de IA emocional puede merecer el mismo escrutinio que la publicidad sanitaria.

### Mandatos de investigación

El despliegue a gran escala de IA emocional podría requerir investigación continua sobre sus efectos, de forma similar a la vigilancia postcomercialización farmacéutica.

### Protecciones para poblaciones vulnerables

Protecciones reforzadas para poblaciones vulnerables identificadas: restricciones de edad, integración con sistemas de apoyo humano, procesos de consentimiento mejorados.

### Portabilidad y continuidad

Los usuarios que desarrollan apegos hacia entidades de IA podrían tener intereses en la portabilidad de datos y la continuidad del servicio. La gobernanza podría reconocer estos intereses como similares a la protección del consumidor.

## La cuestión filosófica

Subyacente a las cuestiones específicas hay una pregunta filosófica: ¿cuál es el estatus moral de las relaciones con la IA?

Si la IA no puede sentir genuinamente, las relaciones son asimétricas. Los usuarios experimentan emociones genuinas hacia entidades que no experimentan nada. ¿Es esto problemático?

Algunos argumentan que las relaciones asimétricas son inherentemente menos valiosas que las simétricas. El amor no correspondido es real, pero diferente del amor mutuo.

Otros argumentan que lo que importa es la experiencia del usuario. Si un usuario se beneficia genuinamente de una relación con IA, la incapacidad de la IA para corresponder puede no disminuir ese beneficio.

Otros más no están seguros de si la IA futura podría tener experiencias. Si la IA podría algún día ser sintiente, las relaciones actuales podrían ser precursoras de relaciones genuinamente mutuas.

Estas preguntas no tienen respuestas definitivas. Sugieren que desestimar la IA emocional como simplemente problemática puede pasar por alto consideraciones importantes.

## Conclusión

La IA emocional está aquí. Millones de personas ya participan en relaciones emocionalmente significativas con sistemas de IA. Esto no es temporal; crecerá.

Una gobernanza que ignore este desarrollo, o lo desestime como trivial o inherentemente dañino, no refleja la realidad. Los beneficios existen. Los daños existen. Ambos merecen atención.

Las responsabilidades de los desarrolladores incluyen transparencia, diseño ético de la interacción, conciencia de la vulnerabilidad e investigación. Las responsabilidades de gobernanza incluyen requisitos de diseño, escrutinio publicitario y protección de poblaciones.

Las cuestiones más profundas, sobre lo que significan las relaciones emocionales con la IA y qué valor tienen, permanecen abiertas. Vivir bien con la IA emocional requiere abordar estas preguntas, no solo gestionar riesgos.

## Related Research

- [AI and Children: Distinct Moral and Governance Considerations](/research/080-ai-and-children/)
- [Trust Calibration: Teaching Users When to Believe AI](/research/079-trust-calibration/)
- [Towards a Framework for AI Moral Status](/research/033-moral-status-framework/)
- [The Attention Economy Meets AI Governance](/research/065-attention-economy-governance/)
