---
title: "La Ley de IA de la UE: lo que no contempla"
excerpt: "La Ley de IA de la UE representa la legislación sobre IA más completa del mundo. Pero incluso una regulación bien diseñada tiene puntos ciegos. Una crítica constructiva de lo que la Ley deja sin abordar."
date: 2026-01-01
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - eu-ai-act
  - regulation
  - policy
  - governance
  - enforcement
  - jurisdiction
---

## Un logro histórico

La Ley de IA de la UE, que entró en vigor en 2025, representa el primer marco legal integral de la humanidad para gobernar la inteligencia artificial. Su enfoque basado en el riesgo, sus requisitos escalonados y su atención a los derechos fundamentales establecen un precedente global.

Este análisis reconoce la importancia de la Ley al tiempo que examina las lagunas que persisten incluso en una legislación bien diseñada. El objetivo no es la crítica por sí misma, sino la identificación de áreas que requieren mecanismos de gobernanza complementarios.

## Lo que la Ley hace bien

Antes de examinar las lagunas, vale la pena reconocer las fortalezas de la Ley.

La **clasificación por niveles de riesgo** evita la trampa de tratar toda la IA de la misma manera. Al distinguir entre aplicaciones inaceptables, de alto riesgo, de riesgo limitado y de riesgo mínimo, la Ley asigna la atención regulatoria de manera proporcional. Este principio — que la gobernanza debe escalar con la capacidad y el contexto — se alinea con nuestro análisis en [proporcionalidad en la divulgación de modelos](/research/001-proportionality-disclosure/).

Los **requisitos de transparencia** para ciertas aplicaciones de IA establecen una línea base de rendición de cuentas. Los usuarios tienen derecho a saber cuándo están interactuando con sistemas de IA, y los operadores deben mantener documentación sobre cómo funcionan los sistemas.

El **enfoque en los derechos fundamentales** sitúa el bienestar humano en el centro, más allá de la seguridad puramente técnica. Las restricciones a la vigilancia biométrica, las prohibiciones de la puntuación social y las protecciones en contextos laborales reflejan una gobernanza basada en valores.

Las **disposiciones sobre IA de propósito general** reconocen que los modelos fundacionales requieren un tratamiento distinto al de las aplicaciones específicas, introduciendo requisitos para sistemas de frontera con riesgo sistémico.

Estos son logros genuinos. Las lagunas que se discuten a continuación existen dentro de un marco general que representa un progreso significativo.

## Laguna 1: Emergencia de capacidades

La Ley categoriza los sistemas de IA según sus casos de uso previstos. Una herramienta de diagnóstico médico es de alto riesgo; un filtro de spam es de riesgo mínimo. Esto tiene sentido para sistemas específicos diseñados para propósitos concretos.

Pero los sistemas de IA de propósito general desafían este enfoque. Un gran modelo de lenguaje podría estar "destinado" a la generación de texto, situándolo en una categoría, mientras que en realidad es capaz de asistir con tareas en todo el espectro de riesgo. El mismo modelo que redacta textos publicitarios puede, con las indicaciones adecuadas, ayudar a sintetizar materiales peligrosos.

La Ley intenta abordar esto a través de sus disposiciones sobre IA de propósito general, pero estas se centran principalmente en modelos por encima de ciertos umbrales de capacidad. El problema de las capacidades emergentes — donde los modelos desarrollan habilidades inesperadas no anticipadas por sus creadores — sigue insuficientemente abordado.

Un modelo podría lanzarse cumpliendo todos los requisitos, y luego descubrirse que tiene capacidades que habrían activado un tratamiento diferente. La categorización estática de la Ley tiene dificultades con esta realidad dinámica. Exploramos esto en [el problema del excedente de capacidad](/research/009-capability-overhang/) — el desafío de gobernar capacidades que existen pero aún no han sido documentadas o descubiertas.

## Laguna 2: Capacidad de aplicación

La legislación solo es tan eficaz como su aplicación. La Ley crea autoridades nacionales competentes responsables de la vigilancia del mercado y el cumplimiento, pero proporciona orientación limitada sobre cómo estas autoridades deben desarrollar la capacidad técnica para evaluar realmente los sistemas de IA.

La evaluación actual de la seguridad de la IA requiere experiencia especializada concentrada en un puñado de organizaciones a nivel mundial. La mayoría de los organismos reguladores nacionales carecen del personal, las herramientas y el conocimiento para evaluar de forma independiente si un sistema de IA complejo cumple con los requisitos de la Ley.

Esto genera dependencia ya sea de la autoevaluación por parte de los desarrolladores o de auditores externos. Ambos enfoques tienen vulnerabilidades. La autoevaluación adolece de conflictos de interés obvios. La auditoría externa requiere auditores competentes, independientes y con recursos adecuados — una oferta que sigue siendo limitada. Examinamos este desafío en [¿quién vigila a los vigilantes?](/research/006-meta-governance-auditors/).

La Ley podría reforzarse creando una infraestructura técnica de evaluación compartida a nivel europeo, formación obligatoria para las autoridades nacionales y mecanismos para garantizar la independencia de los auditores.

## Laguna 3: Alcance extraterritorial

La Ley se aplica a los sistemas de IA comercializados en el mercado de la UE, independientemente de dónde estén establecidos los proveedores. Este alcance extraterritorial se inspira en el RGPD y refleja el considerable poder de mercado de la UE.

Sin embargo, la aplicación contra entidades no pertenecientes a la UE es prácticamente difícil. Una empresa sin presencia en la UE que despliegue servicios de IA accesibles desde Europa tiene una exposición limitada a las sanciones de la UE. Si bien la Ley prohíbe tales servicios no conformes, la prohibición es más fácil de enunciar que de aplicar.

Además, la jurisdicción de la Ley se define por dónde los sistemas son "comercializados" o dónde se utilizan los resultados en la UE. Los sistemas de IA entrenados en otro lugar, con datos de otro lugar, pero cuyos resultados eventualmente afectan a residentes de la UE a través de canales indirectos, pueden quedar fuera de una jurisdicción clara.

Esta laguna importa porque el desarrollo de la IA es globalmente distribuido, y el arbitraje regulatorio — reubicar actividades para evitar la supervisión — es un riesgo real. Analizamos esta dinámica en [arbitraje regulatorio en el despliegue de IA](/research/008-regulatory-arbitrage/). Sin coordinación internacional, incluso una legislación nacional completa puede ser eludida.

## Laguna 4: Ritmo del cambio técnico

La Ley fue redactada basándose en las capacidades de la IA circa 2022-2023 y se implementará durante 2024-2027. El panorama de la IA en 2027 puede lucir muy diferente de lo que los legisladores previeron.

Si bien la Ley incluye disposiciones para actualizar actos de implementación y estándares técnicos, las categorías de riesgo fundamentales y los requisitos están integrados en la legislación primaria. Cambiarlos requiere el proceso legislativo completo, que opera en una escala temporal de años.

Este desfase temporal es particularmente agudo para la IA de propósito general, donde las capacidades avanzan rápidamente. Los umbrales que definen el "riesgo sistémico" — actualmente vinculados al cómputo de entrenamiento — pueden quedar obsoletos a medida que nuevas arquitecturas logren mayor capacidad con menos cómputo, o a medida que el ajuste fino y el scaffolding amplifiquen las capacidades del modelo base de maneras que los umbrales originales no capturan.

Los mecanismos de gobernanza adaptativa — como empoderar a las agencias para ajustar umbrales técnicos sin nueva legislación, o incorporar ciclos obligatorios de revisión y revision — podrían ayudar a abordar esta laguna.

## Laguna 5: El problema de la cadena de suministro

Los sistemas de IA se construyen cada vez más sobre capas de otros sistemas de IA. Una empresa podría ajustar un modelo fundacional, integrarlo con sistemas de recuperación, envolverlo en una capa de aplicación y desplegarlo a través de una plataforma de terceros. La responsabilidad del cumplimiento se fragmenta a lo largo de esta cadena de suministro.

La Ley distingue entre proveedores (quienes desarrollan sistemas) y operadores (quienes los utilizan), asignando diferentes obligaciones a cada uno. Pero la distinción nítida entre proveedor y operador se difumina cuando la misma organización modifica un sistema antes del despliegue, cuando los sistemas se componen de múltiples componentes, o cuando el ajuste fino cambia sustancialmente el comportamiento de un modelo.

Los modelos de pesos abiertos crean complejidad adicional. Si una empresa libera los pesos de un modelo bajo una licencia abierta, y un usuario aguas abajo modifica esos pesos para crear una aplicación dañina, ¿quién asume la responsabilidad? Las disposiciones de la Ley sobre responsabilidad derivada siguen siendo ambiguas en estos casos.

Esto conecta con la tensión que exploramos en [la paradoja de seguridad de los pesos abiertos](/research/002-open-weight-safety-paradox/) — la dificultad de mantener controles de seguridad sobre sistemas diseñados para ser modificados y redistribuidos.

## Laguna 6: Transparencia significativa

La Ley exige transparencia, pero la transparencia no es un bien indiferenciado. Cien páginas de documentación técnica pueden satisfacer los requisitos de divulgación mientras proporcionan poca información útil a aquellos a quienes la transparencia pretende servir.

Las fichas de modelo y la documentación técnica actuales a menudo priorizan el cumplimiento legal sobre la comprensión genuina. Divulgan hechos que satisfacen las casillas regulatorias mientras omiten información que permitiría una evaluación significativa de los riesgos y limitaciones.

Una transparencia eficaz requiere atención no solo a qué se divulga, sino a quién, en qué formato y con qué apoyo para la interpretación. Las disposiciones de transparencia de la Ley se beneficiarían de orientación complementaria sobre la calidad de la divulgación — asegurando que la documentación realmente permita la supervisión que se supone debe apoyar.

## Laguna 7: Gobernanza reflexiva

Quizás la laguna más fundamental es la suposición de la Ley de que la gobernanza es una actividad puramente externa — algo que los humanos hacen a los sistemas de IA. La posibilidad de que los sistemas de IA participen en su propia gobernanza no se contempla.

A medida que los sistemas de IA se vuelven más capaces, podrían ser capaces de monitorear su propio comportamiento en busca de anomalías, señalar posibles usos indebidos, explicar sus restricciones a los usuarios e incluso participar en la evaluación de si sus contextos de despliegue son apropiados. Esta capacidad reflexiva no es un sustituto de la gobernanza externa, sino un complemento de ella.

La Ley podría ampliarse para reconocer y crear incentivos para los mecanismos de gobernanza reflexiva — sistemas de IA que contribuyan a su propia supervisión. Nuestro trabajo sobre [esquemas de restricción legibles por máquina](/research/003-machine-readable-constraint-schema/), [detección reflexiva de uso indebido](/research/011-reflexive-misuse-detection/) y [protocolos de comunicación entre IA y reguladores](/research/014-ai-regulator-protocol/) explora cómo podría ser esto.

## Hacia una gobernanza complementaria

Las lagunas identificadas aquí no son fracasos de la Ley de IA de la UE, sino limitaciones de lo que cualquier instrumento legislativo individual puede lograr. Señalan áreas donde se necesitan mecanismos de gobernanza complementarios:

- **Coordinación internacional** para limitar el arbitraje regulatorio
- **Inversión en capacidad técnica** para la evaluación y la aplicación
- **Mecanismos adaptativos** que puedan responder al cambio rápido de capacidades
- **Gobernanza de la cadena de suministro** que aborde la complejidad composicional
- **Estándares de calidad de transparencia** más allá de los meros requisitos de divulgación
- **Mecanismos reflexivos** donde los sistemas de IA contribuyan a su propia gobernanza

La Ley de IA de la UE es un comienzo, no un final. Su logro es crear una base; el desafío continuo es construir una estructura de gobernanza completa sobre esa base.

## Investigación relacionada

- [Proporcionalidad en la divulgación de modelos](/research/001-proportionality-disclosure/)
- [El problema del excedente de capacidad](/research/009-capability-overhang/)
- [Arbitraje regulatorio en el despliegue de IA](/research/008-regulatory-arbitrage/)
- [¿Quién vigila a los vigilantes? Auditar a los auditores de IA](/research/006-meta-governance-auditors/)
- [La paradoja de seguridad de los pesos abiertos](/research/002-open-weight-safety-paradox/)
