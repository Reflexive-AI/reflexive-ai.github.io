---
title: "Protecciones para denunciantes en laboratorios de IA"
excerpt: "Los empleados de empresas de IA a menudo tienen una perspectiva única sobre los riesgos. Las protecciones actuales son inadecuadas. Este análisis examina qué requerirían marcos significativos de protección de denunciantes para la IA."
date: 2026-01-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - whistleblowing
  - transparency
  - safety
  - governance
  - reporting
---

## El dilema del informante interno

En 2024 y 2025, varios investigadores prominentes de IA plantearon públicamente preocupaciones sobre las prácticas de seguridad de sus empleadores. Algunos renunciaron. Algunos fueron despedidos. Casi todos enfrentaron consecuencias personales y profesionales significativas.

Estas personas sabían algo importante: información privilegiada sobre el desarrollo de IA a la que el público y los reguladores no tenían acceso. Tomaron decisiones difíciles para compartirla, con un costo personal considerable.

Sus experiencias revelan una brecha fundamental de gobernanza. Las personas mejor posicionadas para identificar problemas de seguridad de la IA --los empleados de empresas de IA-- enfrentan fuertes desincentivos para plantear preocupaciones. Las protecciones legales actuales son inadecuadas, y la cultura de muchos laboratorios de IA desalienta la disidencia interna.

Este análisis examina por qué las protecciones para denunciantes son importantes para la gobernanza de la IA, evalúa las brechas actuales y propone cómo sería una protección significativa.

## Por qué importan las fuentes internas

El desarrollo de IA es inusualmente opaco. Los sistemas más capaces son construidos por un pequeño número de empresas. Gran parte de lo que hacen es propietario. Los observadores externos --incluidos los reguladores-- tienen visibilidad limitada sobre las prácticas de desarrollo, las pruebas de seguridad y las preocupaciones internas.

Los empleados, por el contrario, a menudo saben:

**Qué capacidades existen.** Las pruebas internas pueden revelar capacidades no divulgadas públicamente. Exploramos los peligros de las capacidades no documentadas en [el problema del excedente de capacidades](/research/009-capability-overhang/) --la brecha entre lo que los sistemas pueden hacer y lo que se conoce públicamente.

**Cómo se toman las decisiones de seguridad.** ¿Se evalúan seriamente las preocupaciones de seguridad, o se anulan rutinariamente por presión comercial? ¿Son adecuadas las pruebas? ¿Se abordan o se ignoran los hallazgos del red teaming?

**Qué atajos se toman.** Bajo la presión de lanzar productos competitivos, ¿qué medidas de seguridad se omiten, se retrasan o se debilitan?

**Qué problemas han ocurrido.** Incidentes internos, cuasi-accidentes y comportamientos preocupantes que no llegan al público.

Esta información es esencial para una gobernanza efectiva. Sin ella, los reguladores operan en gran medida con lo que las empresas eligen divulgar. Como discutimos en [autonotificación versus auditoría externa](/research/010-self-reporting-vs-audit/), la autonotificación por sí sola es insuficiente para dominios críticos para la seguridad.

## Las protecciones actuales son inadecuadas

Los marcos existentes de protección de denunciantes fueron diseñados para diferentes dominios y se traducen mal a la IA.

### Cobertura legal limitada

Las protecciones para denunciantes en la mayoría de las jurisdicciones cubren industrias específicas o tipos específicos de infracciones: fraude financiero, violaciones ambientales, violaciones de leyes de valores. Las preocupaciones de seguridad de la IA a menudo no encajan en estas categorías.

Un empleado que cree que su empresa está desarrollando IA peligrosamente capaz sin medidas de seguridad adecuadas puede no estar legalmente protegido. Si no se está violando ninguna ley específica --simplemente se están ignorando normas o subestimando riesgos-- los estatutos de protección de denunciantes pueden no aplicarse.

### Definiciones estrechas de conducta indebida

Incluso donde las divulgaciones relacionadas con la IA podrían calificar para protección, la divulgación típicamente debe involucrar actividad ilegal. Pero los riesgos de seguridad de la IA que más preocupan a los investigadores a menudo no son ilegales: son irresponsables, imprudentes o violatorios de normas, pero no criminales.

Construir un sistema de IA capaz sin pruebas de seguridad adecuadas no es actualmente un delito en la mayoría de las jurisdicciones. Lanzar un modelo con capacidades peligrosas conocidas puede no violar ninguna ley. La brecha entre lo que es legal y lo que es seguro es precisamente donde las protecciones para denunciantes son más necesarias y menos disponibles.

### Remedios débiles

Donde existen protecciones, los remedios para las represalias a menudo son inadecuados. La reincorporación a un trabajo donde el empleador te guarda rencor es una victoria hueca. Los daños pueden no compensar la destrucción de carrera en una industria pequeña. Los casos tardan años en resolverse.

En campos tecnológicos con empleadores concentrados y fuertes redes informales, las consecuencias reputacionales de ser etiquetado como un empleado "difícil" pueden acabar con una carrera independientemente de los resultados legales.

### Obligaciones de confidencialidad

Los empleados de IA típicamente firman acuerdos de confidencialidad y cesiones de propiedad intelectual que exceden los contratos de empleo estándar. Estos pueden aplicarse incluso contra divulgaciones que sirven al interés público.

Las empresas pueden usar estos acuerdos para disuadir posibles divulgaciones, amenazando con acciones legales que serían costosas de defender incluso si finalmente no resultan exitosas.

### Dependencias de visa

El desarrollo de IA atrae talento internacional. Muchos empleados tienen visas de trabajo vinculadas a su empleador. Perder un trabajo significa perder la autorización de trabajo, y potencialmente tener que abandonar el país en días.

Esto crea un poder coercitivo extraordinario. Un empleado que de otro modo podría plantear preocupaciones no puede arriesgar la perturbación personal y familiar de la pérdida de visa. Los empleados más vulnerables son los menos capaces de servir como denunciantes.

## Qué requeriría una protección significativa

Las protecciones efectivas para denunciantes en la IA necesitarían varios componentes.

### Cobertura ampliada

Las divulgaciones deberían estar protegidas si se refieren a una creencia razonable de riesgos para la seguridad pública, incluso si no se viola ninguna ley específica. Esto requiere definir la divulgación protegida de manera lo suficientemente amplia como para cubrir las preocupaciones de seguridad de la IA.

El estándar debería ser que el empleado crea razonablemente que la información se refiere a un riesgo genuino para la seguridad pública derivado del desarrollo o la implementación de IA, independientemente de si se trata de violaciones legales específicas.

### Fuerte protección contra represalias

Los empleadores deberían enfrentar sanciones significativas por represalias contra divulgaciones protegidas. Estas sanciones deberían ser lo suficientemente grandes como para disuadir, no meramente un costo de hacer negocios.

Los remedios deberían incluir compensación por daño a la carrera, que en campos especializados puede exceder con mucho los salarios atrasados. Los tribunales deberían tener autoridad para ordenar remedios que hagan que los denunciantes queden íntegros, no meramente técnicamente compensados.

### Anulación de la confidencialidad

Los acuerdos de confidencialidad deberían ser inaplicables contra divulgaciones protegidas. Un acuerdo de no divulgación no debería impedir que un empleado informe a un regulador sobre riesgos de seguridad, incluso si la información es técnicamente propietaria.

Esto requiere disposiciones legales que anulen explícitamente las obligaciones de confidencialidad para las divulgaciones de seguridad de la IA, en línea con lo que existe en algunas jurisdicciones para el fraude financiero.

### Canales protegidos

Debería haber canales claros para la notificación interna (dentro de la empresa), la notificación regulatoria (a agencias gubernamentales) y --como último recurso-- la divulgación pública. La protección debería aplicarse a todos los canales, con requisitos apropiados de escalamiento.

Esto se conecta con nuestro trabajo sobre [protocolos para la comunicación de IA a regulador](/research/014-ai-regulator-protocol/). Los denunciantes humanos y los sistemas de monitoreo basados en IA necesitan vías claras para comunicar preocupaciones a los organismos de supervisión.

### Protecciones de inmigración

Los titulares de visa que realicen divulgaciones protegidas deberían recibir autorización de trabajo temporal suficiente para permitirles permanecer en el país mientras buscan nuevo empleo o persiguen reclamaciones legales. La amenaza de la visa debería neutralizarse.

### Notificación anónima

Algunos empleados solo estarán dispuestos a notificar si pueden permanecer anónimos. Los sistemas para recibir y actuar sobre informes anónimos --aunque más difíciles de verificar-- deberían ser parte de la infraestructura.

## Dinámicas de la industria

Más allá de las protecciones legales, la cultura de la industria importa.

Actualmente, la IA es una industria concentrada donde un pequeño número de empresas e inversores tienen influencia significativa sobre las perspectivas de carrera. El daño reputacional de la denuncia puede seguir a alguien indefinidamente.

Cambiar esto requiere:

**Validación.** Cuando los denunciantes plantean preocupaciones legítimas, la investigación posterior y el reconocimiento validan su acción y señalan a otros que plantear preocupaciones es valorado.

**Normalización.** Los líderes de la industria que se comprometen públicamente a proteger a los disidentes internos, y demuestran ese compromiso, cambian las normas sobre lo que es aceptable.

**Empleo alternativo.** Cuanto más robusto sea el ecosistema de organizaciones de seguridad de la IA, posiciones académicas y trayectorias profesionales alternativas, menos puede cualquier empleador individual amenazar con la destrucción de carrera.

**Presión de los inversores.** Los inversores que preguntan sobre la cultura de seguridad y la protección de la disidencia interna crean incentivos para que las empresas desarrollen mejores prácticas.

## Conexión con la gobernanza

Las protecciones para denunciantes no están separadas de otros mecanismos de gobernanza de la IA, sino que son parte integral de ellos.

Las regulaciones solo funcionan si se detectan las violaciones. Las auditorías externas solo alcanzan lo que las empresas eligen revelar. [Quien vigila a los vigilantes](/research/006-meta-governance-auditors/) --nuestro análisis de la gobernanza de las auditorías-- concluyó que la supervisión externa requiere información de múltiples fuentes. Las fuentes internas están entre las más valiosas.

Del mismo modo, los sistemas de notificación de incidentes que analizamos en [lecciones de la aviación](/research/021-aviation-lessons/) dependen de que la información fluya de quienes presencian los incidentes. Si los empleados temen notificar, la información necesaria para el aprendizaje no llega a quienes la necesitan.

Y la gobernanza reflexiva --sistemas de IA que participan en su propia supervisión-- es complementaria a la denuncia humana, no un sustituto. Los sistemas de IA pueden monitorear algunas cosas; los humanos notan otras. Ambos canales necesitan estar protegidos.

## Conclusión

Las personas que más saben sobre los riesgos de la IA son a menudo las menos protegidas cuando intentan compartir ese conocimiento. Este es un fallo de gobernanza que socava todos los demás mecanismos de seguridad de la IA.

Las protecciones efectivas para denunciantes en la IA requieren cobertura legal ampliada, fuertes medidas contra represalias, anulación de la confidencialidad, canales protegidos, protecciones de inmigración y un cambio cultural en cómo la industria trata la disidencia interna.

Estas protecciones sirven no solo a los denunciantes individuales sino al interés público en sistemas de IA que sean seguros, beneficiosos y responsables. Sin información de quienes están dentro del desarrollo de IA, la gobernanza externa opera en la oscuridad.

## Related Research

- [The Capability Overhang Problem](/research/009-capability-overhang/)
- [Self-Reporting vs. External Audit: Trade-offs](/research/010-self-reporting-vs-audit/)
- [Who Watches the Watchers? Auditing AI Auditors](/research/006-meta-governance-auditors/)
- [A Protocol for AI-to-Regulator Communication](/research/014-ai-regulator-protocol/)
