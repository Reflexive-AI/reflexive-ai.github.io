---
title: "Protecciones para denunciantes en laboratorios de IA"
excerpt: "Los empleados de empresas de IA a menudo tienen una perspectiva unica sobre los riesgos. Las protecciones actuales son inadecuadas. Este analisis examina que requeririan marcos significativos de proteccion de denunciantes para la IA."
date: 2026-01-04
categories:
  - Governance Analysis
  - Policy Proposal
tags:
  - whistleblowing
  - transparency
  - safety
  - governance
  - reporting
---

## El dilema del informante interno

En 2024 y 2025, varios investigadores prominentes de IA plantearon publicamente preocupaciones sobre las practicas de seguridad de sus empleadores. Algunos renunciaron. Algunos fueron despedidos. Casi todos enfrentaron consecuencias personales y profesionales significativas.

Estas personas sabian algo importante: informacion privilegiada sobre el desarrollo de IA a la que el publico y los reguladores no tenian acceso. Tomaron decisiones dificiles para compartirla, con un costo personal considerable.

Sus experiencias revelan una brecha fundamental de gobernanza. Las personas mejor posicionadas para identificar problemas de seguridad de la IA --los empleados de empresas de IA-- enfrentan fuertes desincentivos para plantear preocupaciones. Las protecciones legales actuales son inadecuadas, y la cultura de muchos laboratorios de IA desalienta la disidencia interna.

Este analisis examina por que las protecciones para denunciantes son importantes para la gobernanza de la IA, evalua las brechas actuales y propone como seria una proteccion significativa.

## Por que importan las fuentes internas

El desarrollo de IA es inusualmente opaco. Los sistemas mas capaces son construidos por un pequeno numero de empresas. Gran parte de lo que hacen es propietario. Los observadores externos --incluidos los reguladores-- tienen visibilidad limitada sobre las practicas de desarrollo, las pruebas de seguridad y las preocupaciones internas.

Los empleados, por el contrario, a menudo saben:

**Que capacidades existen.** Las pruebas internas pueden revelar capacidades no divulgadas publicamente. Exploramos los peligros de las capacidades no documentadas en [el problema del excedente de capacidades](/research/009-capability-overhang/) --la brecha entre lo que los sistemas pueden hacer y lo que se conoce publicamente.

**Como se toman las decisiones de seguridad.** Se evaluan seriamente las preocupaciones de seguridad, o se anulan rutinariamente por presion comercial? Son adecuadas las pruebas? Se abordan o se ignoran los hallazgos del red teaming?

**Que atajos se toman.** Bajo la presion de lanzar productos competitivos, que medidas de seguridad se omiten, se retrasan o se debilitan?

**Que problemas han ocurrido.** Incidentes internos, cuasi-accidentes y comportamientos preocupantes que no llegan al publico.

Esta informacion es esencial para una gobernanza efectiva. Sin ella, los reguladores operan en gran medida con lo que las empresas eligen divulgar. Como discutimos en [autonotificacion versus auditoria externa](/research/010-self-reporting-vs-audit/), la autonotificacion por si sola es insuficiente para dominios criticos para la seguridad.

## Las protecciones actuales son inadecuadas

Los marcos existentes de proteccion de denunciantes fueron disenados para diferentes dominios y se traducen mal a la IA.

### Cobertura legal limitada

Las protecciones para denunciantes en la mayoria de las jurisdicciones cubren industrias especificas o tipos especificos de infracciones: fraude financiero, violaciones ambientales, violaciones de leyes de valores. Las preocupaciones de seguridad de la IA a menudo no encajan en estas categorias.

Un empleado que cree que su empresa esta desarrollando IA peligrosamente capaz sin medidas de seguridad adecuadas puede no estar legalmente protegido. Si no se esta violando ninguna ley especifica --simplemente se estan ignorando normas o subestimando riesgos-- los estatutos de proteccion de denunciantes pueden no aplicarse.

### Definiciones estrechas de conducta indebida

Incluso donde las divulgaciones relacionadas con la IA podrian calificar para proteccion, la divulgacion tipicamente debe involucrar actividad ilegal. Pero los riesgos de seguridad de la IA que mas preocupan a los investigadores a menudo no son ilegales: son irresponsables, imprudentes o violatorios de normas, pero no criminales.

Construir un sistema de IA capaz sin pruebas de seguridad adecuadas no es actualmente un delito en la mayoria de las jurisdicciones. Lanzar un modelo con capacidades peligrosas conocidas puede no violar ninguna ley. La brecha entre lo que es legal y lo que es seguro es precisamente donde las protecciones para denunciantes son mas necesarias y menos disponibles.

### Remedios debiles

Donde existen protecciones, los remedios para las represalias a menudo son inadecuados. La reincorporacion a un trabajo donde el empleador te guarda rencor es una victoria hueca. Los danos pueden no compensar la destruccion de carrera en una industria pequena. Los casos tardan anos en resolverse.

En campos tecnologicos con empleadores concentrados y fuertes redes informales, las consecuencias reputacionales de ser etiquetado como un empleado "dificil" pueden acabar con una carrera independientemente de los resultados legales.

### Obligaciones de confidencialidad

Los empleados de IA tipicamente firman acuerdos de confidencialidad y cesiones de propiedad intelectual que exceden los contratos de empleo estandar. Estos pueden aplicarse incluso contra divulgaciones que sirven al interes publico.

Las empresas pueden usar estos acuerdos para disuadir posibles divulgaciones, amenazando con acciones legales que serian costosas de defender incluso si finalmente no resultan exitosas.

### Dependencias de visa

El desarrollo de IA atrae talento internacional. Muchos empleados tienen visas de trabajo vinculadas a su empleador. Perder un trabajo significa perder la autorizacion de trabajo, y potencialmente tener que abandonar el pais en dias.

Esto crea un poder coercitivo extraordinario. Un empleado que de otro modo podria plantear preocupaciones no puede arriesgar la perturbacion personal y familiar de la perdida de visa. Los empleados mas vulnerables son los menos capaces de servir como denunciantes.

## Que requeriria una proteccion significativa

Las protecciones efectivas para denunciantes en la IA necesitarian varios componentes.

### Cobertura ampliada

Las divulgaciones deberian estar protegidas si se refieren a una creencia razonable de riesgos para la seguridad publica, incluso si no se viola ninguna ley especifica. Esto requiere definir la divulgacion protegida de manera lo suficientemente amplia como para cubrir las preocupaciones de seguridad de la IA.

El estandar deberia ser que el empleado crea razonablemente que la informacion se refiere a un riesgo genuino para la seguridad publica derivado del desarrollo o la implementacion de IA, independientemente de si se trata de violaciones legales especificas.

### Fuerte proteccion contra represalias

Los empleadores deberian enfrentar sanciones significativas por represalias contra divulgaciones protegidas. Estas sanciones deberian ser lo suficientemente grandes como para disuadir, no meramente un costo de hacer negocios.

Los remedios deberian incluir compensacion por dano a la carrera, que en campos especializados puede exceder con mucho los salarios atrasados. Los tribunales deberian tener autoridad para ordenar remedios que hagan que los denunciantes queden integros, no meramente tecnicamente compensados.

### Anulacion de la confidencialidad

Los acuerdos de confidencialidad deberian ser inaplicables contra divulgaciones protegidas. Un acuerdo de no divulgacion no deberia impedir que un empleado informe a un regulador sobre riesgos de seguridad, incluso si la informacion es tecnicamente propietaria.

Esto requiere disposiciones legales que anulen explicitamente las obligaciones de confidencialidad para las divulgaciones de seguridad de la IA, en linea con lo que existe en algunas jurisdicciones para el fraude financiero.

### Canales protegidos

Deberia haber canales claros para la notificacion interna (dentro de la empresa), la notificacion regulatoria (a agencias gubernamentales) y --como ultimo recurso-- la divulgacion publica. La proteccion deberia aplicarse a todos los canales, con requisitos apropiados de escalamiento.

Esto se conecta con nuestro trabajo sobre [protocolos para la comunicacion de IA a regulador](/research/014-ai-regulator-protocol/). Los denunciantes humanos y los sistemas de monitoreo basados en IA necesitan vias claras para comunicar preocupaciones a los organismos de supervision.

### Protecciones de inmigracion

Los titulares de visa que realicen divulgaciones protegidas deberian recibir autorizacion de trabajo temporal suficiente para permitirles permanecer en el pais mientras buscan nuevo empleo o persiguen reclamaciones legales. La amenaza de la visa deberia neutralizarse.

### Notificacion anonima

Algunos empleados solo estaran dispuestos a notificar si pueden permanecer anonimos. Los sistemas para recibir y actuar sobre informes anonimos --aunque mas dificiles de verificar-- deberian ser parte de la infraestructura.

## Dinamicas de la industria

Mas alla de las protecciones legales, la cultura de la industria importa.

Actualmente, la IA es una industria concentrada donde un pequeno numero de empresas e inversores tienen influencia significativa sobre las perspectivas de carrera. El dano reputacional de la denuncia puede seguir a alguien indefinidamente.

Cambiar esto requiere:

**Validacion.** Cuando los denunciantes plantean preocupaciones legitimas, la investigacion posterior y el reconocimiento validan su accion y senalan a otros que plantear preocupaciones es valorado.

**Normalizacion.** Los lideres de la industria que se comprometen publicamente a proteger a los disidentes internos, y demuestran ese compromiso, cambian las normas sobre lo que es aceptable.

**Empleo alternativo.** Cuanto mas robusto sea el ecosistema de organizaciones de seguridad de la IA, posiciones academicas y trayectorias profesionales alternativas, menos puede cualquier empleador individual amenazar con la destruccion de carrera.

**Presion de los inversores.** Los inversores que preguntan sobre la cultura de seguridad y la proteccion de la disidencia interna crean incentivos para que las empresas desarrollen mejores practicas.

## Conexion con la gobernanza

Las protecciones para denunciantes no estan separadas de otros mecanismos de gobernanza de la IA, sino que son parte integral de ellos.

Las regulaciones solo funcionan si se detectan las violaciones. Las auditorias externas solo alcanzan lo que las empresas eligen revelar. [Quien vigila a los vigilantes](/research/006-meta-governance-auditors/) --nuestro analisis de la gobernanza de las auditorias-- concluyo que la supervision externa requiere informacion de multiples fuentes. Las fuentes internas estan entre las mas valiosas.

Del mismo modo, los sistemas de notificacion de incidentes que analizamos en [lecciones de la aviacion](/research/021-aviation-lessons/) dependen de que la informacion fluya de quienes presencian los incidentes. Si los empleados temen notificar, la informacion necesaria para el aprendizaje no llega a quienes la necesitan.

Y la gobernanza reflexiva --sistemas de IA que participan en su propia supervision-- es complementaria a la denuncia humana, no un sustituto. Los sistemas de IA pueden monitorear algunas cosas; los humanos notan otras. Ambos canales necesitan estar protegidos.

## Conclusion

Las personas que mas saben sobre los riesgos de la IA son a menudo las menos protegidas cuando intentan compartir ese conocimiento. Este es un fallo de gobernanza que socava todos los demas mecanismos de seguridad de la IA.

Las protecciones efectivas para denunciantes en la IA requieren cobertura legal ampliada, fuertes medidas contra represalias, anulacion de la confidencialidad, canales protegidos, protecciones de inmigracion y un cambio cultural en como la industria trata la disidencia interna.

Estas protecciones sirven no solo a los denunciantes individuales sino al interes publico en sistemas de IA que sean seguros, beneficiosos y responsables. Sin informacion de quienes estan dentro del desarrollo de IA, la gobernanza externa opera en la oscuridad.

## Related Research

- [The Capability Overhang Problem](/research/009-capability-overhang/)
- [Self-Reporting vs. External Audit: Trade-offs](/research/010-self-reporting-vs-audit/)
- [Who Watches the Watchers? Auditing AI Auditors](/research/006-meta-governance-auditors/)
- [A Protocol for AI-to-Regulator Communication](/research/014-ai-regulator-protocol/)
