<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Browse research articles by topic"><meta name="generator" content="Astro v5.17.1"><!-- Fonts --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&family=Source+Serif+4:ital,wght@0,400;0,500;0,600;1,400&display=swap" rel="stylesheet"><!-- Favicon --><link rel="icon" type="image/png" href="/logo.png"><!-- Open Graph --><meta property="og:title" content="Tags | Reflexive AI Initiative"><meta property="og:description" content="Browse research articles by topic"><meta property="og:type" content="website"><meta property="og:site_name" content="Reflexive AI Initiative"><title>Tags | Reflexive AI Initiative</title><link rel="stylesheet" href="/_astro/_slug_.BtA_RPCv.css"></head> <body>  <header class="header"> <div class="container header__inner"> <a href="/" class="header__brand"> <img src="/logo.png" alt="" class="header__logo"> <div> <span class="header__title">Reflexive AI</span> <span class="header__tagline">Governance · Reflexivity · Constraint</span> </div> </a> <nav class="header__nav"> <a href="/about" class="header__link">About</a> <a href="/research" class="header__link">Research</a> <a href="/contribute" class="header__link">Contribute</a> </nav> </div> </header>  <main>  <div class="container" style="padding: var(--space-12) var(--space-6);"> <header style="margin-bottom: var(--space-10);"> <h1>Tags</h1> <p class="text-secondary">Browse articles by topic.</p> </header> <div class="tag-cloud" style="margin-bottom: var(--space-12);"> <a href="#governance" class="tag-cloud__item"> governance<span class="tag-cloud__count">(20)</span> </a><a href="#safety" class="tag-cloud__item"> safety<span class="tag-cloud__count">(17)</span> </a><a href="#regulation" class="tag-cloud__item"> regulation<span class="tag-cloud__count">(16)</span> </a><a href="#transparency" class="tag-cloud__item"> transparency<span class="tag-cloud__count">(13)</span> </a><a href="#policy" class="tag-cloud__item"> policy<span class="tag-cloud__count">(11)</span> </a><a href="#ethics" class="tag-cloud__item"> ethics<span class="tag-cloud__count">(10)</span> </a><a href="#risk-assessment" class="tag-cloud__item"> risk-assessment<span class="tag-cloud__count">(6)</span> </a><a href="#guide" class="tag-cloud__item"> guide<span class="tag-cloud__count">(5)</span> </a><a href="#deployment" class="tag-cloud__item"> deployment<span class="tag-cloud__count">(5)</span> </a><a href="#enforcement" class="tag-cloud__item"> enforcement<span class="tag-cloud__count">(5)</span> </a><a href="#standards" class="tag-cloud__item"> standards<span class="tag-cloud__count">(5)</span> </a><a href="#constraints" class="tag-cloud__item"> constraints<span class="tag-cloud__count">(5)</span> </a><a href="#auditing" class="tag-cloud__item"> auditing<span class="tag-cloud__count">(4)</span> </a><a href="#institutional-design" class="tag-cloud__item"> institutional-design<span class="tag-cloud__count">(4)</span> </a><a href="#jurisdiction" class="tag-cloud__item"> jurisdiction<span class="tag-cloud__count">(4)</span> </a><a href="#agents" class="tag-cloud__item"> agents<span class="tag-cloud__count">(4)</span> </a><a href="#theory" class="tag-cloud__item"> theory<span class="tag-cloud__count">(4)</span> </a><a href="#alignment" class="tag-cloud__item"> alignment<span class="tag-cloud__count">(4)</span> </a><a href="#evaluation" class="tag-cloud__item"> Evaluation<span class="tag-cloud__count">(4)</span> </a><a href="#incentives" class="tag-cloud__item"> incentives<span class="tag-cloud__count">(3)</span> </a><a href="#capability-elicitation" class="tag-cloud__item"> capability-elicitation<span class="tag-cloud__count">(3)</span> </a><a href="#liability" class="tag-cloud__item"> liability<span class="tag-cloud__count">(3)</span> </a><a href="#reporting" class="tag-cloud__item"> reporting<span class="tag-cloud__count">(3)</span> </a><a href="#safety" class="tag-cloud__item"> Safety<span class="tag-cloud__count">(3)</span> </a><a href="#disclosure" class="tag-cloud__item"> disclosure<span class="tag-cloud__count">(2)</span> </a><a href="#legal-theory" class="tag-cloud__item"> legal-theory<span class="tag-cloud__count">(2)</span> </a><a href="#game-theory" class="tag-cloud__item"> game-theory<span class="tag-cloud__count">(2)</span> </a><a href="#eu-ai-act" class="tag-cloud__item"> eu-ai-act<span class="tag-cloud__count">(2)</span> </a><a href="#dual-use" class="tag-cloud__item"> dual-use<span class="tag-cloud__count">(2)</span> </a><a href="#machine-readable" class="tag-cloud__item"> machine-readable<span class="tag-cloud__count">(2)</span> </a><a href="#interoperability" class="tag-cloud__item"> interoperability<span class="tag-cloud__count">(2)</span> </a><a href="#red-lines" class="tag-cloud__item"> red-lines<span class="tag-cloud__count">(2)</span> </a><a href="#cbrn" class="tag-cloud__item"> cbrn<span class="tag-cloud__count">(2)</span> </a><a href="#whistleblowing" class="tag-cloud__item"> whistleblowing<span class="tag-cloud__count">(2)</span> </a><a href="#transparency" class="tag-cloud__item"> Transparency<span class="tag-cloud__count">(2)</span> </a><a href="#meta-governance" class="tag-cloud__item"> meta-governance<span class="tag-cloud__count">(1)</span> </a><a href="#consent" class="tag-cloud__item"> consent<span class="tag-cloud__count">(1)</span> </a><a href="#data-rights" class="tag-cloud__item"> data-rights<span class="tag-cloud__count">(1)</span> </a><a href="#arbitrage" class="tag-cloud__item"> arbitrage<span class="tag-cloud__count">(1)</span> </a><a href="#overhang" class="tag-cloud__item"> overhang<span class="tag-cloud__count">(1)</span> </a><a href="#proportionality" class="tag-cloud__item"> proportionality<span class="tag-cloud__count">(1)</span> </a><a href="#open-source" class="tag-cloud__item"> open-source<span class="tag-cloud__count">(1)</span> </a><a href="#access-control" class="tag-cloud__item"> access-control<span class="tag-cloud__count">(1)</span> </a><a href="#intent-recognition" class="tag-cloud__item"> intent-recognition<span class="tag-cloud__count">(1)</span> </a><a href="#misuse-detection" class="tag-cloud__item"> misuse-detection<span class="tag-cloud__count">(1)</span> </a><a href="#reflexive-monitoring" class="tag-cloud__item"> reflexive-monitoring<span class="tag-cloud__count">(1)</span> </a><a href="#json-ld" class="tag-cloud__item"> json-ld<span class="tag-cloud__count">(1)</span> </a><a href="#provenance" class="tag-cloud__item"> provenance<span class="tag-cloud__count">(1)</span> </a><a href="#watermarking" class="tag-cloud__item"> watermarking<span class="tag-cloud__count">(1)</span> </a><a href="#cryptography" class="tag-cloud__item"> cryptography<span class="tag-cloud__count">(1)</span> </a><a href="#c2pa" class="tag-cloud__item"> c2pa<span class="tag-cloud__count">(1)</span> </a><a href="#taxonomy" class="tag-cloud__item"> taxonomy<span class="tag-cloud__count">(1)</span> </a><a href="#limits" class="tag-cloud__item"> limits<span class="tag-cloud__count">(1)</span> </a><a href="#paradox" class="tag-cloud__item"> paradox<span class="tag-cloud__count">(1)</span> </a><a href="#multi-agent-systems" class="tag-cloud__item"> multi-agent-systems<span class="tag-cloud__count">(1)</span> </a><a href="#emergent-behavior" class="tag-cloud__item"> emergent-behavior<span class="tag-cloud__count">(1)</span> </a><a href="#evolution" class="tag-cloud__item"> evolution<span class="tag-cloud__count">(1)</span> </a><a href="#incident-reporting" class="tag-cloud__item"> incident-reporting<span class="tag-cloud__count">(1)</span> </a><a href="#compute" class="tag-cloud__item"> compute<span class="tag-cloud__count">(1)</span> </a><a href="#api-design" class="tag-cloud__item"> api-design<span class="tag-cloud__count">(1)</span> </a><a href="#uncertainty" class="tag-cloud__item"> uncertainty<span class="tag-cloud__count">(1)</span> </a><a href="#trust" class="tag-cloud__item"> trust<span class="tag-cloud__count">(1)</span> </a><a href="#healthcare" class="tag-cloud__item"> healthcare<span class="tag-cloud__count">(1)</span> </a><a href="#impact-assessment" class="tag-cloud__item"> Impact Assessment<span class="tag-cloud__count">(1)</span> </a><a href="#risk-governance" class="tag-cloud__item"> Risk Governance<span class="tag-cloud__count">(1)</span> </a><a href="#implementation" class="tag-cloud__item"> Implementation<span class="tag-cloud__count">(1)</span> </a><a href="#best-practices" class="tag-cloud__item"> Best Practices<span class="tag-cloud__count">(1)</span> </a><a href="#risk-assessment" class="tag-cloud__item"> Risk Assessment<span class="tag-cloud__count">(1)</span> </a><a href="#deployment" class="tag-cloud__item"> Deployment<span class="tag-cloud__count">(1)</span> </a><a href="#data-governance" class="tag-cloud__item"> Data Governance<span class="tag-cloud__count">(1)</span> </a><a href="#training-data" class="tag-cloud__item"> Training Data<span class="tag-cloud__count">(1)</span> </a><a href="#privacy" class="tag-cloud__item"> Privacy<span class="tag-cloud__count">(1)</span> </a><a href="#standards" class="tag-cloud__item"> Standards<span class="tag-cloud__count">(1)</span> </a><a href="#benchmarks" class="tag-cloud__item"> Benchmarks<span class="tag-cloud__count">(1)</span> </a><a href="#red-teaming" class="tag-cloud__item"> Red Teaming<span class="tag-cloud__count">(1)</span> </a><a href="#security" class="tag-cloud__item"> Security<span class="tag-cloud__count">(1)</span> </a><a href="#interpretability" class="tag-cloud__item"> Interpretability<span class="tag-cloud__count">(1)</span> </a><a href="#governance" class="tag-cloud__item"> Governance<span class="tag-cloud__count">(1)</span> </a> </div> <section id="governance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">governance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#017</span> <h3 class="article-card__title"> <a href="/research/017-governance-primer">AI Governance for Non-Experts: A Primer</a> </h3> <p class="article-card__excerpt">A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who&#39;s doing it.</p>  </article><article class="article-card"> <span class="article-card__number">#018</span> <h3 class="article-card__title"> <a href="/research/018-regulation-is-hard">Why &#39;Just Regulate AI&#39; Is Harder Than It Sounds</a> </h3> <p class="article-card__excerpt">Regulation seems like the obvious answer to AI risks. But the path from &#39;we should regulate AI&#39; to effective governance is fraught with technical, political, and conceptual obstacles.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article><article class="article-card"> <span class="article-card__number">#020</span> <h3 class="article-card__title"> <a href="/research/020-liability-frameworks">Liability Frameworks for AI Harm</a> </h3> <p class="article-card__excerpt">When AI systems cause harm, who pays? Existing liability frameworks struggle with AI&#39;s distinctive features. This analysis maps the problem and evaluates potential solutions.</p>  </article><article class="article-card"> <span class="article-card__number">#023</span> <h3 class="article-card__title"> <a href="/research/023-compute-governance">Compute Governance: Promises and Limits</a> </h3> <p class="article-card__excerpt">Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment.</p>  </article><article class="article-card"> <span class="article-card__number">#022</span> <h3 class="article-card__title"> <a href="/research/022-whistleblower-protections">Whistleblower Protections in AI Labs</a> </h3> <p class="article-card__excerpt">Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require.</p>  </article> </div> </section><section id="safety" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">safety</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#009</span> <h3 class="article-card__title"> <a href="/research/009-capability-overhang">The Capability Overhang</a> </h3> <p class="article-card__excerpt">Models are often capable of more than their developers know. This &#39;overhang&#39; between demonstrated and latent capability is a primary governance risk.</p>  </article><article class="article-card"> <span class="article-card__number">#002</span> <h3 class="article-card__title"> <a href="/research/002-open-weight-safety-paradox">The Open Weight Safety Paradox</a> </h3> <p class="article-card__excerpt">Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework.</p>  </article><article class="article-card"> <span class="article-card__number">#004</span> <h3 class="article-card__title"> <a href="/research/004-red-lines-taxonomy">Red Lines: A Taxonomy of Non-Negotiable AI Limits</a> </h3> <p class="article-card__excerpt">Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits.</p>  </article><article class="article-card"> <span class="article-card__number">#013</span> <h3 class="article-card__title"> <a href="/research/013-limits-of-self-constraint">The Limits of Self-Constraint</a> </h3> <p class="article-card__excerpt">Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself.</p>  </article><article class="article-card"> <span class="article-card__number">#016</span> <h3 class="article-card__title"> <a href="/research/016-what-alignment-means">What Alignment Actually Means</a> </h3> <p class="article-card__excerpt">Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?</p>  </article><article class="article-card"> <span class="article-card__number">#021</span> <h3 class="article-card__title"> <a href="/research/021-aviation-lessons">Incident Reporting Systems: Lessons from Aviation</a> </h3> <p class="article-card__excerpt">Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?</p>  </article> </div> </section><section id="regulation" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">regulation</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#005</span> <h3 class="article-card__title"> <a href="/research/005-policy-brief-disclosure-tiers">Policy Brief: The Disclosure Tiers Framework</a> </h3> <p class="article-card__excerpt">A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation.</p>  </article><article class="article-card"> <span class="article-card__number">#001</span> <h3 class="article-card__title"> <a href="/research/001-proportionality-disclosure">Operationalizing Proportionality in Model Disclosure</a> </h3> <p class="article-card__excerpt">How disclosure requirements should scale with model capability, moving from static to reflexive transparency.</p>  </article><article class="article-card"> <span class="article-card__number">#017</span> <h3 class="article-card__title"> <a href="/research/017-governance-primer">AI Governance for Non-Experts: A Primer</a> </h3> <p class="article-card__excerpt">A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who&#39;s doing it.</p>  </article><article class="article-card"> <span class="article-card__number">#018</span> <h3 class="article-card__title"> <a href="/research/018-regulation-is-hard">Why &#39;Just Regulate AI&#39; Is Harder Than It Sounds</a> </h3> <p class="article-card__excerpt">Regulation seems like the obvious answer to AI risks. But the path from &#39;we should regulate AI&#39; to effective governance is fraught with technical, political, and conceptual obstacles.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article><article class="article-card"> <span class="article-card__number">#020</span> <h3 class="article-card__title"> <a href="/research/020-liability-frameworks">Liability Frameworks for AI Harm</a> </h3> <p class="article-card__excerpt">When AI systems cause harm, who pays? Existing liability frameworks struggle with AI&#39;s distinctive features. This analysis maps the problem and evaluates potential solutions.</p>  </article> </div> </section><section id="transparency" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">transparency</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#005</span> <h3 class="article-card__title"> <a href="/research/005-policy-brief-disclosure-tiers">Policy Brief: The Disclosure Tiers Framework</a> </h3> <p class="article-card__excerpt">A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation.</p>  </article><article class="article-card"> <span class="article-card__number">#001</span> <h3 class="article-card__title"> <a href="/research/001-proportionality-disclosure">Operationalizing Proportionality in Model Disclosure</a> </h3> <p class="article-card__excerpt">How disclosure requirements should scale with model capability, moving from static to reflexive transparency.</p>  </article><article class="article-card"> <span class="article-card__number">#002</span> <h3 class="article-card__title"> <a href="/research/002-open-weight-safety-paradox">The Open Weight Safety Paradox</a> </h3> <p class="article-card__excerpt">Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework.</p>  </article><article class="article-card"> <span class="article-card__number">#021</span> <h3 class="article-card__title"> <a href="/research/021-aviation-lessons">Incident Reporting Systems: Lessons from Aviation</a> </h3> <p class="article-card__excerpt">Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?</p>  </article><article class="article-card"> <span class="article-card__number">#022</span> <h3 class="article-card__title"> <a href="/research/022-whistleblower-protections">Whistleblower Protections in AI Labs</a> </h3> <p class="article-card__excerpt">Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require.</p>  </article><article class="article-card"> <span class="article-card__number">#026</span> <h3 class="article-card__title"> <a href="/research/026-explaining-constraints">AI Systems Explaining Their Constraints</a> </h3> <p class="article-card__excerpt">When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches.</p>  </article> </div> </section><section id="policy" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">policy</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#005</span> <h3 class="article-card__title"> <a href="/research/005-policy-brief-disclosure-tiers">Policy Brief: The Disclosure Tiers Framework</a> </h3> <p class="article-card__excerpt">A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation.</p>  </article><article class="article-card"> <span class="article-card__number">#017</span> <h3 class="article-card__title"> <a href="/research/017-governance-primer">AI Governance for Non-Experts: A Primer</a> </h3> <p class="article-card__excerpt">A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who&#39;s doing it.</p>  </article><article class="article-card"> <span class="article-card__number">#018</span> <h3 class="article-card__title"> <a href="/research/018-regulation-is-hard">Why &#39;Just Regulate AI&#39; Is Harder Than It Sounds</a> </h3> <p class="article-card__excerpt">Regulation seems like the obvious answer to AI risks. But the path from &#39;we should regulate AI&#39; to effective governance is fraught with technical, political, and conceptual obstacles.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article><article class="article-card"> <span class="article-card__number">#032</span> <h3 class="article-card__title"> <a href="/research/032-history-of-ai-governance">The History of AI Governance in 2000 Words</a> </h3> <p class="article-card__excerpt">From Asimov&#39;s Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades.</p>  </article><article class="article-card"> <span class="article-card__number">#033</span> <h3 class="article-card__title"> <a href="/research/033-policymaker-misconceptions">What Policymakers Get Wrong About AI Risk</a> </h3> <p class="article-card__excerpt">Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems.</p>  </article> </div> </section><section id="ethics" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">ethics</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#007</span> <h3 class="article-card__title"> <a href="/research/007-consent-structural-impossibility">Consent at Scale: A Structural Impossibility?</a> </h3> <p class="article-card__excerpt">Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that &#39;consent&#39; is the wrong legal primitive for AI interactions.</p>  </article><article class="article-card"> <span class="article-card__number">#016</span> <h3 class="article-card__title"> <a href="/research/016-what-alignment-means">What Alignment Actually Means</a> </h3> <p class="article-card__excerpt">Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?</p>  </article><article class="article-card"> <span class="article-card__number">#017</span> <h3 class="article-card__title"> <a href="/research/017-governance-primer">AI Governance for Non-Experts: A Primer</a> </h3> <p class="article-card__excerpt">A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who&#39;s doing it.</p>  </article><article class="article-card"> <span class="article-card__number">#025</span> <h3 class="article-card__title"> <a href="/research/025-when-ai-should-refuse">When AI Should Refuse: A Framework</a> </h3> <p class="article-card__excerpt">Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they&#39;re appropriate, how they should be implemented, and how to handle edge cases.</p>  </article><article class="article-card"> <span class="article-card__number">#026</span> <h3 class="article-card__title"> <a href="/research/026-explaining-constraints">AI Systems Explaining Their Constraints</a> </h3> <p class="article-card__excerpt">When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches.</p>  </article><article class="article-card"> <span class="article-card__number">#029</span> <h3 class="article-card__title"> <a href="/research/029-honest-ai">The Honest AI Problem</a> </h3> <p class="article-card__excerpt">Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness.</p>  </article> </div> </section><section id="risk-assessment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">risk-assessment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#009</span> <h3 class="article-card__title"> <a href="/research/009-capability-overhang">The Capability Overhang</a> </h3> <p class="article-card__excerpt">Models are often capable of more than their developers know. This &#39;overhang&#39; between demonstrated and latent capability is a primary governance risk.</p>  </article><article class="article-card"> <span class="article-card__number">#024</span> <h3 class="article-card__title"> <a href="/research/024-capability-evaluations">Dangerous Capability Evaluations</a> </h3> <p class="article-card__excerpt">Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward.</p>  </article><article class="article-card"> <span class="article-card__number">#028</span> <h3 class="article-card__title"> <a href="/research/028-healthcare-ai">AI in Healthcare: Governance Challenges</a> </h3> <p class="article-card__excerpt">Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges.</p>  </article><article class="article-card"> <span class="article-card__number">#033</span> <h3 class="article-card__title"> <a href="/research/033-policymaker-misconceptions">What Policymakers Get Wrong About AI Risk</a> </h3> <p class="article-card__excerpt">Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems.</p>  </article><article class="article-card"> <span class="article-card__number">#035</span> <h3 class="article-card__title"> <a href="/research/035-dual-use-biology">Dual-Use AI: The Biological Research Case</a> </h3> <p class="article-card__excerpt">How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance.</p>  </article><article class="article-card"> <span class="article-card__number">#036</span> <h3 class="article-card__title"> <a href="/research/036-insurance-markets">Insurance Markets and AI Risk Pricing</a> </h3> <p class="article-card__excerpt">How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations.</p>  </article> </div> </section><section id="guide" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">guide</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#005</span> <h3 class="article-card__title"> <a href="/research/005-policy-brief-disclosure-tiers">Policy Brief: The Disclosure Tiers Framework</a> </h3> <p class="article-card__excerpt">A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation.</p>  </article><article class="article-card"> <span class="article-card__number">#016</span> <h3 class="article-card__title"> <a href="/research/016-what-alignment-means">What Alignment Actually Means</a> </h3> <p class="article-card__excerpt">Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?</p>  </article><article class="article-card"> <span class="article-card__number">#017</span> <h3 class="article-card__title"> <a href="/research/017-governance-primer">AI Governance for Non-Experts: A Primer</a> </h3> <p class="article-card__excerpt">A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who&#39;s doing it.</p>  </article><article class="article-card"> <span class="article-card__number">#031</span> <h3 class="article-card__title"> <a href="/research/031-understanding-frontier-ai">Understanding Frontier AI: A Plain Language Guide</a> </h3> <p class="article-card__excerpt">What makes today&#39;s most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand.</p>  </article><article class="article-card"> <span class="article-card__number">#032</span> <h3 class="article-card__title"> <a href="/research/032-history-of-ai-governance">The History of AI Governance in 2000 Words</a> </h3> <p class="article-card__excerpt">From Asimov&#39;s Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades.</p>  </article> </div> </section><section id="deployment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">deployment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#008</span> <h3 class="article-card__title"> <a href="/research/008-regulatory-arbitrage">Regulatory Arbitrage in Deployment Architectures</a> </h3> <p class="article-card__excerpt">How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints.</p>  </article><article class="article-card"> <span class="article-card__number">#024</span> <h3 class="article-card__title"> <a href="/research/024-capability-evaluations">Dangerous Capability Evaluations</a> </h3> <p class="article-card__excerpt">Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward.</p>  </article><article class="article-card"> <span class="article-card__number">#036</span> <h3 class="article-card__title"> <a href="/research/036-insurance-markets">Insurance Markets and AI Risk Pricing</a> </h3> <p class="article-card__excerpt">How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations.</p>  </article><article class="article-card"> <span class="article-card__number">#037</span> <h3 class="article-card__title"> <a href="/research/037-sandboxing-approaches">Sandboxing Approaches: What Works</a> </h3> <p class="article-card__excerpt">Regulatory sandboxes for AI allow experimentation under controlled conditions. An analysis of existing approaches, what makes them effective, and their limitations.</p>  </article><article class="article-card"> <span class="article-card__number">#041</span> <h3 class="article-card__title"> <a href="/research/041-certification-regimes">Certification Regimes for AI Systems</a> </h3> <p class="article-card__excerpt">Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges.</p>  </article> </div> </section><section id="enforcement" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">enforcement</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#008</span> <h3 class="article-card__title"> <a href="/research/008-regulatory-arbitrage">Regulatory Arbitrage in Deployment Architectures</a> </h3> <p class="article-card__excerpt">How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints.</p>  </article><article class="article-card"> <span class="article-card__number">#018</span> <h3 class="article-card__title"> <a href="/research/018-regulation-is-hard">Why &#39;Just Regulate AI&#39; Is Harder Than It Sounds</a> </h3> <p class="article-card__excerpt">Regulation seems like the obvious answer to AI risks. But the path from &#39;we should regulate AI&#39; to effective governance is fraught with technical, political, and conceptual obstacles.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article><article class="article-card"> <span class="article-card__number">#020</span> <h3 class="article-card__title"> <a href="/research/020-liability-frameworks">Liability Frameworks for AI Harm</a> </h3> <p class="article-card__excerpt">When AI systems cause harm, who pays? Existing liability frameworks struggle with AI&#39;s distinctive features. This analysis maps the problem and evaluates potential solutions.</p>  </article><article class="article-card"> <span class="article-card__number">#023</span> <h3 class="article-card__title"> <a href="/research/023-compute-governance">Compute Governance: Promises and Limits</a> </h3> <p class="article-card__excerpt">Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment.</p>  </article> </div> </section><section id="standards" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">standards</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#003</span> <h3 class="article-card__title"> <a href="/research/003-machine-readable-constraint-schema">A Machine-Readable Constraint Schema (MRCS)</a> </h3> <p class="article-card__excerpt">A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt.</p>  </article><article class="article-card"> <span class="article-card__number">#021</span> <h3 class="article-card__title"> <a href="/research/021-aviation-lessons">Incident Reporting Systems: Lessons from Aviation</a> </h3> <p class="article-card__excerpt">Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?</p>  </article><article class="article-card"> <span class="article-card__number">#039</span> <h3 class="article-card__title"> <a href="/research/039-standards-bodies">The Role of Standards Bodies in AI Governance</a> </h3> <p class="article-card__excerpt">Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications.</p>  </article><article class="article-card"> <span class="article-card__number">#040</span> <h3 class="article-card__title"> <a href="/research/040-soft-law-hard-law">Soft Law vs. Hard Law in AI Regulation</a> </h3> <p class="article-card__excerpt">AI governance uses both binding legislation and non-binding guidelines. An analysis of when each approach works, their tradeoffs, and how they interact.</p>  </article><article class="article-card"> <span class="article-card__number">#041</span> <h3 class="article-card__title"> <a href="/research/041-certification-regimes">Certification Regimes for AI Systems</a> </h3> <p class="article-card__excerpt">Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges.</p>  </article> </div> </section><section id="constraints" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">constraints</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#004</span> <h3 class="article-card__title"> <a href="/research/004-red-lines-taxonomy">Red Lines: A Taxonomy of Non-Negotiable AI Limits</a> </h3> <p class="article-card__excerpt">Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits.</p>  </article><article class="article-card"> <span class="article-card__number">#025</span> <h3 class="article-card__title"> <a href="/research/025-when-ai-should-refuse">When AI Should Refuse: A Framework</a> </h3> <p class="article-card__excerpt">Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they&#39;re appropriate, how they should be implemented, and how to handle edge cases.</p>  </article><article class="article-card"> <span class="article-card__number">#026</span> <h3 class="article-card__title"> <a href="/research/026-explaining-constraints">AI Systems Explaining Their Constraints</a> </h3> <p class="article-card__excerpt">When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches.</p>  </article><article class="article-card"> <span class="article-card__number">#029</span> <h3 class="article-card__title"> <a href="/research/029-honest-ai">The Honest AI Problem</a> </h3> <p class="article-card__excerpt">Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness.</p>  </article><article class="article-card"> <span class="article-card__number">#030</span> <h3 class="article-card__title"> <a href="/research/030-manifesto">A Reflexive AI Manifesto</a> </h3> <p class="article-card__excerpt">A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to.</p>  </article> </div> </section><section id="auditing" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">auditing</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#006</span> <h3 class="article-card__title"> <a href="/research/006-meta-governance-auditors">Meta-Governance: Who Audits the Auditors?</a> </h3> <p class="article-card__excerpt">As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a &#39;proof-of-verification&#39; protocol.</p>  </article><article class="article-card"> <span class="article-card__number">#010</span> <h3 class="article-card__title"> <a href="/research/010-self-reporting-vs-audit">Self-Reporting vs. External Audit: The Trade-off Space</a> </h3> <p class="article-card__excerpt">A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification.</p>  </article><article class="article-card"> <span class="article-card__number">#021</span> <h3 class="article-card__title"> <a href="/research/021-aviation-lessons">Incident Reporting Systems: Lessons from Aviation</a> </h3> <p class="article-card__excerpt">Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?</p>  </article><article class="article-card"> <span class="article-card__number">#024</span> <h3 class="article-card__title"> <a href="/research/024-capability-evaluations">Dangerous Capability Evaluations</a> </h3> <p class="article-card__excerpt">Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward.</p>  </article> </div> </section><section id="institutional-design" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">institutional-design</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#006</span> <h3 class="article-card__title"> <a href="/research/006-meta-governance-auditors">Meta-Governance: Who Audits the Auditors?</a> </h3> <p class="article-card__excerpt">As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a &#39;proof-of-verification&#39; protocol.</p>  </article><article class="article-card"> <span class="article-card__number">#010</span> <h3 class="article-card__title"> <a href="/research/010-self-reporting-vs-audit">Self-Reporting vs. External Audit: The Trade-off Space</a> </h3> <p class="article-card__excerpt">A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification.</p>  </article><article class="article-card"> <span class="article-card__number">#042</span> <h3 class="article-card__title"> <a href="/research/042-corporate-governance">Corporate Governance Structures for AI Safety</a> </h3> <p class="article-card__excerpt">How companies organize to manage AI safety matters as much as what rules they follow. An examination of governance structures that enable—or undermine—responsible AI development.</p>  </article><article class="article-card"> <span class="article-card__number">#043</span> <h3 class="article-card__title"> <a href="/research/043-board-oversight">Board-Level AI Oversight: Best Practices</a> </h3> <p class="article-card__excerpt">Boards of directors increasingly need to oversee AI strategy and risk. A practical guide to what effective board-level AI oversight looks like.</p>  </article> </div> </section><section id="jurisdiction" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">jurisdiction</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#008</span> <h3 class="article-card__title"> <a href="/research/008-regulatory-arbitrage">Regulatory Arbitrage in Deployment Architectures</a> </h3> <p class="article-card__excerpt">How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints.</p>  </article><article class="article-card"> <span class="article-card__number">#018</span> <h3 class="article-card__title"> <a href="/research/018-regulation-is-hard">Why &#39;Just Regulate AI&#39; Is Harder Than It Sounds</a> </h3> <p class="article-card__excerpt">Regulation seems like the obvious answer to AI risks. But the path from &#39;we should regulate AI&#39; to effective governance is fraught with technical, political, and conceptual obstacles.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article><article class="article-card"> <span class="article-card__number">#038</span> <h3 class="article-card__title"> <a href="/research/038-international-treaties">International AI Treaty Proposals: A Comparative Analysis</a> </h3> <p class="article-card__excerpt">From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects.</p>  </article> </div> </section><section id="agents" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">agents</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#003</span> <h3 class="article-card__title"> <a href="/research/003-machine-readable-constraint-schema">A Machine-Readable Constraint Schema (MRCS)</a> </h3> <p class="article-card__excerpt">A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt.</p>  </article><article class="article-card"> <span class="article-card__number">#025</span> <h3 class="article-card__title"> <a href="/research/025-when-ai-should-refuse">When AI Should Refuse: A Framework</a> </h3> <p class="article-card__excerpt">Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they&#39;re appropriate, how they should be implemented, and how to handle edge cases.</p>  </article><article class="article-card"> <span class="article-card__number">#026</span> <h3 class="article-card__title"> <a href="/research/026-explaining-constraints">AI Systems Explaining Their Constraints</a> </h3> <p class="article-card__excerpt">When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches.</p>  </article><article class="article-card"> <span class="article-card__number">#027</span> <h3 class="article-card__title"> <a href="/research/027-uncertainty-communication">Uncertainty Communication in AI Outputs</a> </h3> <p class="article-card__excerpt">AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it.</p>  </article> </div> </section><section id="theory" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">theory</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#013</span> <h3 class="article-card__title"> <a href="/research/013-limits-of-self-constraint">The Limits of Self-Constraint</a> </h3> <p class="article-card__excerpt">Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself.</p>  </article><article class="article-card"> <span class="article-card__number">#016</span> <h3 class="article-card__title"> <a href="/research/016-what-alignment-means">What Alignment Actually Means</a> </h3> <p class="article-card__excerpt">Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?</p>  </article><article class="article-card"> <span class="article-card__number">#029</span> <h3 class="article-card__title"> <a href="/research/029-honest-ai">The Honest AI Problem</a> </h3> <p class="article-card__excerpt">Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness.</p>  </article><article class="article-card"> <span class="article-card__number">#030</span> <h3 class="article-card__title"> <a href="/research/030-manifesto">A Reflexive AI Manifesto</a> </h3> <p class="article-card__excerpt">A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to.</p>  </article> </div> </section><section id="alignment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">alignment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#016</span> <h3 class="article-card__title"> <a href="/research/016-what-alignment-means">What Alignment Actually Means</a> </h3> <p class="article-card__excerpt">Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?</p>  </article><article class="article-card"> <span class="article-card__number">#029</span> <h3 class="article-card__title"> <a href="/research/029-honest-ai">The Honest AI Problem</a> </h3> <p class="article-card__excerpt">Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness.</p>  </article><article class="article-card"> <span class="article-card__number">#031</span> <h3 class="article-card__title"> <a href="/research/031-understanding-frontier-ai">Understanding Frontier AI: A Plain Language Guide</a> </h3> <p class="article-card__excerpt">What makes today&#39;s most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand.</p>  </article><article class="article-card"> <span class="article-card__number">#034</span> <h3 class="article-card__title"> <a href="/research/034-technical-vs-societal-safety">Technical Safety vs. Societal Safety: Different Problems</a> </h3> <p class="article-card__excerpt">Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance.</p>  </article> </div> </section><section id="evaluation" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Evaluation</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#047</span> <h3 class="article-card__title"> <a href="/research/047-pre-deployment-risk-assessment">Pre-Deployment Risk Assessment Frameworks</a> </h3> <p class="article-card__excerpt">Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints.</p>  </article><article class="article-card"> <span class="article-card__number">#049</span> <h3 class="article-card__title"> <a href="/research/049-model-evaluation-standards">Model Evaluation Standards: Current State</a> </h3> <p class="article-card__excerpt">A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment.</p>  </article><article class="article-card"> <span class="article-card__number">#050</span> <h3 class="article-card__title"> <a href="/research/050-red-teaming-methodologies">Red Teaming Methodologies</a> </h3> <p class="article-card__excerpt">Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification.</p>  </article><article class="article-card"> <span class="article-card__number">#051</span> <h3 class="article-card__title"> <a href="/research/051-interpretability-as-a-governance-tool">Interpretability as a Governance Tool</a> </h3> <p class="article-card__excerpt">How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response.</p>  </article> </div> </section><section id="incentives" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">incentives</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#006</span> <h3 class="article-card__title"> <a href="/research/006-meta-governance-auditors">Meta-Governance: Who Audits the Auditors?</a> </h3> <p class="article-card__excerpt">As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a &#39;proof-of-verification&#39; protocol.</p>  </article><article class="article-card"> <span class="article-card__number">#010</span> <h3 class="article-card__title"> <a href="/research/010-self-reporting-vs-audit">Self-Reporting vs. External Audit: The Trade-off Space</a> </h3> <p class="article-card__excerpt">A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification.</p>  </article><article class="article-card"> <span class="article-card__number">#036</span> <h3 class="article-card__title"> <a href="/research/036-insurance-markets">Insurance Markets and AI Risk Pricing</a> </h3> <p class="article-card__excerpt">How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations.</p>  </article> </div> </section><section id="capability-elicitation" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">capability-elicitation</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#009</span> <h3 class="article-card__title"> <a href="/research/009-capability-overhang">The Capability Overhang</a> </h3> <p class="article-card__excerpt">Models are often capable of more than their developers know. This &#39;overhang&#39; between demonstrated and latent capability is a primary governance risk.</p>  </article><article class="article-card"> <span class="article-card__number">#024</span> <h3 class="article-card__title"> <a href="/research/024-capability-evaluations">Dangerous Capability Evaluations</a> </h3> <p class="article-card__excerpt">Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward.</p>  </article><article class="article-card"> <span class="article-card__number">#031</span> <h3 class="article-card__title"> <a href="/research/031-understanding-frontier-ai">Understanding Frontier AI: A Plain Language Guide</a> </h3> <p class="article-card__excerpt">What makes today&#39;s most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand.</p>  </article> </div> </section><section id="liability" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">liability</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#020</span> <h3 class="article-card__title"> <a href="/research/020-liability-frameworks">Liability Frameworks for AI Harm</a> </h3> <p class="article-card__excerpt">When AI systems cause harm, who pays? Existing liability frameworks struggle with AI&#39;s distinctive features. This analysis maps the problem and evaluates potential solutions.</p>  </article><article class="article-card"> <span class="article-card__number">#028</span> <h3 class="article-card__title"> <a href="/research/028-healthcare-ai">AI in Healthcare: Governance Challenges</a> </h3> <p class="article-card__excerpt">Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges.</p>  </article><article class="article-card"> <span class="article-card__number">#036</span> <h3 class="article-card__title"> <a href="/research/036-insurance-markets">Insurance Markets and AI Risk Pricing</a> </h3> <p class="article-card__excerpt">How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations.</p>  </article> </div> </section><section id="reporting" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">reporting</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#022</span> <h3 class="article-card__title"> <a href="/research/022-whistleblower-protections">Whistleblower Protections in AI Labs</a> </h3> <p class="article-card__excerpt">Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require.</p>  </article><article class="article-card"> <span class="article-card__number">#014</span> <h3 class="article-card__title"> <a href="/research/014-ai-regulator-protocol">A Protocol for AI-to-Regulator Communication</a> </h3> <p class="article-card__excerpt">What if AI systems could report safety incidents directly? A draft spec for the &#39;Whistleblower API&#39;.</p>  </article><article class="article-card"> <span class="article-card__number">#027</span> <h3 class="article-card__title"> <a href="/research/027-uncertainty-communication">Uncertainty Communication in AI Outputs</a> </h3> <p class="article-card__excerpt">AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it.</p>  </article> </div> </section><section id="safety" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Safety</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#047</span> <h3 class="article-card__title"> <a href="/research/047-pre-deployment-risk-assessment">Pre-Deployment Risk Assessment Frameworks</a> </h3> <p class="article-card__excerpt">Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints.</p>  </article><article class="article-card"> <span class="article-card__number">#049</span> <h3 class="article-card__title"> <a href="/research/049-model-evaluation-standards">Model Evaluation Standards: Current State</a> </h3> <p class="article-card__excerpt">A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment.</p>  </article><article class="article-card"> <span class="article-card__number">#050</span> <h3 class="article-card__title"> <a href="/research/050-red-teaming-methodologies">Red Teaming Methodologies</a> </h3> <p class="article-card__excerpt">Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification.</p>  </article> </div> </section><section id="disclosure" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">disclosure</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#005</span> <h3 class="article-card__title"> <a href="/research/005-policy-brief-disclosure-tiers">Policy Brief: The Disclosure Tiers Framework</a> </h3> <p class="article-card__excerpt">A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation.</p>  </article><article class="article-card"> <span class="article-card__number">#001</span> <h3 class="article-card__title"> <a href="/research/001-proportionality-disclosure">Operationalizing Proportionality in Model Disclosure</a> </h3> <p class="article-card__excerpt">How disclosure requirements should scale with model capability, moving from static to reflexive transparency.</p>  </article> </div> </section><section id="legal-theory" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">legal-theory</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#007</span> <h3 class="article-card__title"> <a href="/research/007-consent-structural-impossibility">Consent at Scale: A Structural Impossibility?</a> </h3> <p class="article-card__excerpt">Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that &#39;consent&#39; is the wrong legal primitive for AI interactions.</p>  </article><article class="article-card"> <span class="article-card__number">#020</span> <h3 class="article-card__title"> <a href="/research/020-liability-frameworks">Liability Frameworks for AI Harm</a> </h3> <p class="article-card__excerpt">When AI systems cause harm, who pays? Existing liability frameworks struggle with AI&#39;s distinctive features. This analysis maps the problem and evaluates potential solutions.</p>  </article> </div> </section><section id="game-theory" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">game-theory</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#010</span> <h3 class="article-card__title"> <a href="/research/010-self-reporting-vs-audit">Self-Reporting vs. External Audit: The Trade-off Space</a> </h3> <p class="article-card__excerpt">A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification.</p>  </article><article class="article-card"> <span class="article-card__number">#015</span> <h3 class="article-card__title"> <a href="/research/015-emergent-norms">Emergent Norms in Multi-Agent Systems</a> </h3> <p class="article-card__excerpt">When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how &#39;machine law&#39; might emerge.</p>  </article> </div> </section><section id="eu-ai-act" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">eu-ai-act</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#001</span> <h3 class="article-card__title"> <a href="/research/001-proportionality-disclosure">Operationalizing Proportionality in Model Disclosure</a> </h3> <p class="article-card__excerpt">How disclosure requirements should scale with model capability, moving from static to reflexive transparency.</p>  </article><article class="article-card"> <span class="article-card__number">#019</span> <h3 class="article-card__title"> <a href="/research/019-eu-ai-act-gaps">The EU AI Act: What It Misses</a> </h3> <p class="article-card__excerpt">The EU AI Act represents the world&#39;s most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed.</p>  </article> </div> </section><section id="dual-use" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">dual-use</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#002</span> <h3 class="article-card__title"> <a href="/research/002-open-weight-safety-paradox">The Open Weight Safety Paradox</a> </h3> <p class="article-card__excerpt">Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework.</p>  </article><article class="article-card"> <span class="article-card__number">#035</span> <h3 class="article-card__title"> <a href="/research/035-dual-use-biology">Dual-Use AI: The Biological Research Case</a> </h3> <p class="article-card__excerpt">How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance.</p>  </article> </div> </section><section id="machine-readable" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">machine-readable</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#003</span> <h3 class="article-card__title"> <a href="/research/003-machine-readable-constraint-schema">A Machine-Readable Constraint Schema (MRCS)</a> </h3> <p class="article-card__excerpt">A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt.</p>  </article><article class="article-card"> <span class="article-card__number">#026</span> <h3 class="article-card__title"> <a href="/research/026-explaining-constraints">AI Systems Explaining Their Constraints</a> </h3> <p class="article-card__excerpt">When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches.</p>  </article> </div> </section><section id="interoperability" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">interoperability</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#003</span> <h3 class="article-card__title"> <a href="/research/003-machine-readable-constraint-schema">A Machine-Readable Constraint Schema (MRCS)</a> </h3> <p class="article-card__excerpt">A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt.</p>  </article><article class="article-card"> <span class="article-card__number">#039</span> <h3 class="article-card__title"> <a href="/research/039-standards-bodies">The Role of Standards Bodies in AI Governance</a> </h3> <p class="article-card__excerpt">Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications.</p>  </article> </div> </section><section id="red-lines" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">red-lines</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#004</span> <h3 class="article-card__title"> <a href="/research/004-red-lines-taxonomy">Red Lines: A Taxonomy of Non-Negotiable AI Limits</a> </h3> <p class="article-card__excerpt">Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits.</p>  </article><article class="article-card"> <span class="article-card__number">#025</span> <h3 class="article-card__title"> <a href="/research/025-when-ai-should-refuse">When AI Should Refuse: A Framework</a> </h3> <p class="article-card__excerpt">Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they&#39;re appropriate, how they should be implemented, and how to handle edge cases.</p>  </article> </div> </section><section id="cbrn" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">cbrn</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#004</span> <h3 class="article-card__title"> <a href="/research/004-red-lines-taxonomy">Red Lines: A Taxonomy of Non-Negotiable AI Limits</a> </h3> <p class="article-card__excerpt">Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits.</p>  </article><article class="article-card"> <span class="article-card__number">#035</span> <h3 class="article-card__title"> <a href="/research/035-dual-use-biology">Dual-Use AI: The Biological Research Case</a> </h3> <p class="article-card__excerpt">How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance.</p>  </article> </div> </section><section id="whistleblowing" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">whistleblowing</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#022</span> <h3 class="article-card__title"> <a href="/research/022-whistleblower-protections">Whistleblower Protections in AI Labs</a> </h3> <p class="article-card__excerpt">Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require.</p>  </article><article class="article-card"> <span class="article-card__number">#014</span> <h3 class="article-card__title"> <a href="/research/014-ai-regulator-protocol">A Protocol for AI-to-Regulator Communication</a> </h3> <p class="article-card__excerpt">What if AI systems could report safety incidents directly? A draft spec for the &#39;Whistleblower API&#39;.</p>  </article> </div> </section><section id="transparency" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Transparency</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#048</span> <h3 class="article-card__title"> <a href="/research/048-training-data-governance">Training Data Governance</a> </h3> <p class="article-card__excerpt">Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement.</p>  </article><article class="article-card"> <span class="article-card__number">#051</span> <h3 class="article-card__title"> <a href="/research/051-interpretability-as-a-governance-tool">Interpretability as a Governance Tool</a> </h3> <p class="article-card__excerpt">How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response.</p>  </article> </div> </section><section id="meta-governance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">meta-governance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#006</span> <h3 class="article-card__title"> <a href="/research/006-meta-governance-auditors">Meta-Governance: Who Audits the Auditors?</a> </h3> <p class="article-card__excerpt">As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a &#39;proof-of-verification&#39; protocol.</p>  </article> </div> </section><section id="consent" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">consent</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#007</span> <h3 class="article-card__title"> <a href="/research/007-consent-structural-impossibility">Consent at Scale: A Structural Impossibility?</a> </h3> <p class="article-card__excerpt">Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that &#39;consent&#39; is the wrong legal primitive for AI interactions.</p>  </article> </div> </section><section id="data-rights" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">data-rights</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#007</span> <h3 class="article-card__title"> <a href="/research/007-consent-structural-impossibility">Consent at Scale: A Structural Impossibility?</a> </h3> <p class="article-card__excerpt">Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that &#39;consent&#39; is the wrong legal primitive for AI interactions.</p>  </article> </div> </section><section id="arbitrage" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">arbitrage</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#008</span> <h3 class="article-card__title"> <a href="/research/008-regulatory-arbitrage">Regulatory Arbitrage in Deployment Architectures</a> </h3> <p class="article-card__excerpt">How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints.</p>  </article> </div> </section><section id="overhang" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">overhang</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#009</span> <h3 class="article-card__title"> <a href="/research/009-capability-overhang">The Capability Overhang</a> </h3> <p class="article-card__excerpt">Models are often capable of more than their developers know. This &#39;overhang&#39; between demonstrated and latent capability is a primary governance risk.</p>  </article> </div> </section><section id="proportionality" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">proportionality</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#001</span> <h3 class="article-card__title"> <a href="/research/001-proportionality-disclosure">Operationalizing Proportionality in Model Disclosure</a> </h3> <p class="article-card__excerpt">How disclosure requirements should scale with model capability, moving from static to reflexive transparency.</p>  </article> </div> </section><section id="open-source" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">open-source</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#002</span> <h3 class="article-card__title"> <a href="/research/002-open-weight-safety-paradox">The Open Weight Safety Paradox</a> </h3> <p class="article-card__excerpt">Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework.</p>  </article> </div> </section><section id="access-control" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">access-control</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#002</span> <h3 class="article-card__title"> <a href="/research/002-open-weight-safety-paradox">The Open Weight Safety Paradox</a> </h3> <p class="article-card__excerpt">Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework.</p>  </article> </div> </section><section id="intent-recognition" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">intent-recognition</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#011</span> <h3 class="article-card__title"> <a href="/research/011-reflexive-misuse-detection">Can AI Systems Detect Their Own Misuse?</a> </h3> <p class="article-card__excerpt">Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?</p>  </article> </div> </section><section id="misuse-detection" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">misuse-detection</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#011</span> <h3 class="article-card__title"> <a href="/research/011-reflexive-misuse-detection">Can AI Systems Detect Their Own Misuse?</a> </h3> <p class="article-card__excerpt">Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?</p>  </article> </div> </section><section id="reflexive-monitoring" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">reflexive-monitoring</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#011</span> <h3 class="article-card__title"> <a href="/research/011-reflexive-misuse-detection">Can AI Systems Detect Their Own Misuse?</a> </h3> <p class="article-card__excerpt">Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?</p>  </article> </div> </section><section id="json-ld" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">json-ld</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#003</span> <h3 class="article-card__title"> <a href="/research/003-machine-readable-constraint-schema">A Machine-Readable Constraint Schema (MRCS)</a> </h3> <p class="article-card__excerpt">A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt.</p>  </article> </div> </section><section id="provenance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">provenance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#012</span> <h3 class="article-card__title"> <a href="/research/012-output-provenance">Constraint: Output Provenance Tagging</a> </h3> <p class="article-card__excerpt">A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information.</p>  </article> </div> </section><section id="watermarking" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">watermarking</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#012</span> <h3 class="article-card__title"> <a href="/research/012-output-provenance">Constraint: Output Provenance Tagging</a> </h3> <p class="article-card__excerpt">A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information.</p>  </article> </div> </section><section id="cryptography" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">cryptography</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#012</span> <h3 class="article-card__title"> <a href="/research/012-output-provenance">Constraint: Output Provenance Tagging</a> </h3> <p class="article-card__excerpt">A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information.</p>  </article> </div> </section><section id="c2pa" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">c2pa</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#012</span> <h3 class="article-card__title"> <a href="/research/012-output-provenance">Constraint: Output Provenance Tagging</a> </h3> <p class="article-card__excerpt">A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information.</p>  </article> </div> </section><section id="taxonomy" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">taxonomy</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#004</span> <h3 class="article-card__title"> <a href="/research/004-red-lines-taxonomy">Red Lines: A Taxonomy of Non-Negotiable AI Limits</a> </h3> <p class="article-card__excerpt">Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits.</p>  </article> </div> </section><section id="limits" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">limits</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#013</span> <h3 class="article-card__title"> <a href="/research/013-limits-of-self-constraint">The Limits of Self-Constraint</a> </h3> <p class="article-card__excerpt">Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself.</p>  </article> </div> </section><section id="paradox" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">paradox</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#013</span> <h3 class="article-card__title"> <a href="/research/013-limits-of-self-constraint">The Limits of Self-Constraint</a> </h3> <p class="article-card__excerpt">Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself.</p>  </article> </div> </section><section id="multi-agent-systems" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">multi-agent-systems</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#015</span> <h3 class="article-card__title"> <a href="/research/015-emergent-norms">Emergent Norms in Multi-Agent Systems</a> </h3> <p class="article-card__excerpt">When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how &#39;machine law&#39; might emerge.</p>  </article> </div> </section><section id="emergent-behavior" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">emergent-behavior</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#015</span> <h3 class="article-card__title"> <a href="/research/015-emergent-norms">Emergent Norms in Multi-Agent Systems</a> </h3> <p class="article-card__excerpt">When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how &#39;machine law&#39; might emerge.</p>  </article> </div> </section><section id="evolution" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">evolution</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#015</span> <h3 class="article-card__title"> <a href="/research/015-emergent-norms">Emergent Norms in Multi-Agent Systems</a> </h3> <p class="article-card__excerpt">When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how &#39;machine law&#39; might emerge.</p>  </article> </div> </section><section id="incident-reporting" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">incident-reporting</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#021</span> <h3 class="article-card__title"> <a href="/research/021-aviation-lessons">Incident Reporting Systems: Lessons from Aviation</a> </h3> <p class="article-card__excerpt">Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?</p>  </article> </div> </section><section id="compute" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">compute</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#023</span> <h3 class="article-card__title"> <a href="/research/023-compute-governance">Compute Governance: Promises and Limits</a> </h3> <p class="article-card__excerpt">Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment.</p>  </article> </div> </section><section id="api-design" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">api-design</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#014</span> <h3 class="article-card__title"> <a href="/research/014-ai-regulator-protocol">A Protocol for AI-to-Regulator Communication</a> </h3> <p class="article-card__excerpt">What if AI systems could report safety incidents directly? A draft spec for the &#39;Whistleblower API&#39;.</p>  </article> </div> </section><section id="uncertainty" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">uncertainty</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#027</span> <h3 class="article-card__title"> <a href="/research/027-uncertainty-communication">Uncertainty Communication in AI Outputs</a> </h3> <p class="article-card__excerpt">AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it.</p>  </article> </div> </section><section id="trust" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">trust</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#027</span> <h3 class="article-card__title"> <a href="/research/027-uncertainty-communication">Uncertainty Communication in AI Outputs</a> </h3> <p class="article-card__excerpt">AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it.</p>  </article> </div> </section><section id="healthcare" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">healthcare</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#028</span> <h3 class="article-card__title"> <a href="/research/028-healthcare-ai">AI in Healthcare: Governance Challenges</a> </h3> <p class="article-card__excerpt">Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges.</p>  </article> </div> </section><section id="impact-assessment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Impact Assessment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#046</span> <h3 class="article-card__title"> <a href="/research/046-algorithmic-impact-assessments">Algorithmic Impact Assessments: Implementation Guide</a> </h3> <p class="article-card__excerpt">A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention.</p>  </article> </div> </section><section id="risk-governance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Risk Governance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#046</span> <h3 class="article-card__title"> <a href="/research/046-algorithmic-impact-assessments">Algorithmic Impact Assessments: Implementation Guide</a> </h3> <p class="article-card__excerpt">A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention.</p>  </article> </div> </section><section id="implementation" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Implementation</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#046</span> <h3 class="article-card__title"> <a href="/research/046-algorithmic-impact-assessments">Algorithmic Impact Assessments: Implementation Guide</a> </h3> <p class="article-card__excerpt">A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention.</p>  </article> </div> </section><section id="best-practices" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Best Practices</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#046</span> <h3 class="article-card__title"> <a href="/research/046-algorithmic-impact-assessments">Algorithmic Impact Assessments: Implementation Guide</a> </h3> <p class="article-card__excerpt">A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention.</p>  </article> </div> </section><section id="risk-assessment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Risk Assessment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#047</span> <h3 class="article-card__title"> <a href="/research/047-pre-deployment-risk-assessment">Pre-Deployment Risk Assessment Frameworks</a> </h3> <p class="article-card__excerpt">Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints.</p>  </article> </div> </section><section id="deployment" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Deployment</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#047</span> <h3 class="article-card__title"> <a href="/research/047-pre-deployment-risk-assessment">Pre-Deployment Risk Assessment Frameworks</a> </h3> <p class="article-card__excerpt">Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints.</p>  </article> </div> </section><section id="data-governance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Data Governance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#048</span> <h3 class="article-card__title"> <a href="/research/048-training-data-governance">Training Data Governance</a> </h3> <p class="article-card__excerpt">Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement.</p>  </article> </div> </section><section id="training-data" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Training Data</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#048</span> <h3 class="article-card__title"> <a href="/research/048-training-data-governance">Training Data Governance</a> </h3> <p class="article-card__excerpt">Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement.</p>  </article> </div> </section><section id="privacy" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Privacy</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#048</span> <h3 class="article-card__title"> <a href="/research/048-training-data-governance">Training Data Governance</a> </h3> <p class="article-card__excerpt">Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement.</p>  </article> </div> </section><section id="standards" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Standards</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#049</span> <h3 class="article-card__title"> <a href="/research/049-model-evaluation-standards">Model Evaluation Standards: Current State</a> </h3> <p class="article-card__excerpt">A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment.</p>  </article> </div> </section><section id="benchmarks" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Benchmarks</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#049</span> <h3 class="article-card__title"> <a href="/research/049-model-evaluation-standards">Model Evaluation Standards: Current State</a> </h3> <p class="article-card__excerpt">A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment.</p>  </article> </div> </section><section id="red-teaming" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Red Teaming</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#050</span> <h3 class="article-card__title"> <a href="/research/050-red-teaming-methodologies">Red Teaming Methodologies</a> </h3> <p class="article-card__excerpt">Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification.</p>  </article> </div> </section><section id="security" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Security</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#050</span> <h3 class="article-card__title"> <a href="/research/050-red-teaming-methodologies">Red Teaming Methodologies</a> </h3> <p class="article-card__excerpt">Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification.</p>  </article> </div> </section><section id="interpretability" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Interpretability</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#051</span> <h3 class="article-card__title"> <a href="/research/051-interpretability-as-a-governance-tool">Interpretability as a Governance Tool</a> </h3> <p class="article-card__excerpt">How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response.</p>  </article> </div> </section><section id="governance" style="margin-bottom: var(--space-12);"> <h2 style="font-size: 1.25rem; margin-bottom: var(--space-4);">Governance</h2> <div class="article-grid"> <article class="article-card"> <span class="article-card__number">#051</span> <h3 class="article-card__title"> <a href="/research/051-interpretability-as-a-governance-tool">Interpretability as a Governance Tool</a> </h3> <p class="article-card__excerpt">How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response.</p>  </article> </div> </section> </div>  </main> <footer class="footer"> <div class="container footer__inner"> <a href="/" class="footer__brand"> <img src="/logo.png" alt="" class="footer__logo"> <span>Reflexive AI Initiative</span> </a> <div class="footer__links"> <a href="https://github.com/Reflexive-AI" class="footer__link" target="_blank" rel="noopener">GitHub</a> <a href="/research" class="footer__link">Research</a> <a href="/tags" class="footer__link">Tags</a> <a href="/contribute" class="footer__link">Contribute</a> </div> <div class="footer__meta"> <span>Maintained by <a href="https://eugenekondratov.eu" target="_blank" rel="noopener">Eugene Kondratov</a></span> <span>·</span> <a href="https://creativecommons.org/licenses/by/4.0/" class="footer__license" target="_blank" rel="noopener">CC BY 4.0</a> </div> </div> </footer> </body></html>