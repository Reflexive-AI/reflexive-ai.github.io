{
  "nodes": [
    {
      "id": "disclosure",
      "title": "disclosure",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "Operationalizing Proportionality in Model Disclosure",
          "slug": "/research/001-proportionality-disclosure",
          "excerpt": "How disclosure requirements should scale with model capability, moving from static to reflexive transparency."
        },
        {
          "title": "Policy Brief: The Disclosure Tiers Framework",
          "slug": "/research/005-policy-brief-disclosure-tiers",
          "excerpt": "A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation."
        },
        {
          "title": "The Game Theory of AI Disclosure: When Transparency is a Prisoner's Dilemma",
          "slug": "/research/067-game-theory-disclosure",
          "excerpt": "Companies face a collective action problem: all would benefit from industry-wide transparency, but unilateral disclosure may harm competitive position. How do we change the payoff structure?"
        }
      ]
    },
    {
      "id": "regulation",
      "title": "regulation",
      "group": "governance",
      "val": 30,
      "articles": [
        {
          "title": "Operationalizing Proportionality in Model Disclosure",
          "slug": "/research/001-proportionality-disclosure",
          "excerpt": "How disclosure requirements should scale with model capability, moving from static to reflexive transparency."
        },
        {
          "title": "Policy Brief: The Disclosure Tiers Framework",
          "slug": "/research/005-policy-brief-disclosure-tiers",
          "excerpt": "A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation."
        },
        {
          "title": "A Protocol for AI-to-Regulator Communication",
          "slug": "/research/014-ai-regulator-protocol",
          "excerpt": "What if AI systems could report safety incidents directly? A draft spec for the 'Whistleblower API'."
        },
        {
          "title": "AI Governance for Non-Experts: A Primer",
          "slug": "/research/017-governance-primer",
          "excerpt": "A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who's doing it."
        },
        {
          "title": "Why 'Just Regulate AI' Is Harder Than It Sounds",
          "slug": "/research/018-regulation-is-hard",
          "excerpt": "Regulation seems like the obvious answer to AI risks. But the path from 'we should regulate AI' to effective governance is fraught with technical, political, and conceptual obstacles."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        },
        {
          "title": "Liability Frameworks for AI Harm",
          "slug": "/research/020-liability-frameworks",
          "excerpt": "When AI systems cause harm, who pays? Existing liability frameworks struggle with AI's distinctive features. This analysis maps the problem and evaluates potential solutions."
        },
        {
          "title": "Compute Governance: Promises and Limits",
          "slug": "/research/023-compute-governance",
          "excerpt": "Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment."
        },
        {
          "title": "AI in Healthcare: Governance Challenges",
          "slug": "/research/028-healthcare-ai",
          "excerpt": "Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges."
        },
        {
          "title": "The History of AI Governance in 2000 Words",
          "slug": "/research/032-history-of-ai-governance",
          "excerpt": "From Asimov's Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades."
        },
        {
          "title": "What Policymakers Get Wrong About AI Risk",
          "slug": "/research/033-policymaker-misconceptions",
          "excerpt": "Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems."
        },
        {
          "title": "Sandboxing Approaches: What Works",
          "slug": "/research/037-sandboxing-approaches",
          "excerpt": "Regulatory sandboxes for AI allow experimentation under controlled conditions. An analysis of existing approaches, what makes them effective, and their limitations."
        },
        {
          "title": "International AI Treaty Proposals: A Comparative Analysis",
          "slug": "/research/038-international-treaties",
          "excerpt": "From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects."
        },
        {
          "title": "The Role of Standards Bodies in AI Governance",
          "slug": "/research/039-standards-bodies",
          "excerpt": "Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications."
        },
        {
          "title": "Soft Law vs. Hard Law in AI Regulation",
          "slug": "/research/040-soft-law-hard-law",
          "excerpt": "AI governance uses both binding legislation and non-binding guidelines. An analysis of when each approach works, their tradeoffs, and how they interact."
        },
        {
          "title": "Certification Regimes for AI Systems",
          "slug": "/research/041-certification-regimes",
          "excerpt": "Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges."
        },
        {
          "title": "API-Level Safety Controls",
          "slug": "/research/054-api-level-safety-controls",
          "excerpt": "Exploring the role of API-level safety measures in AI governance, their implementation, and their implications for mitigating malicious use and accidental harm."
        },
        {
          "title": "The False Binary: Why 'Regulate Use, Not Models' Gets AI Governance Wrong",
          "slug": "/research/062-use-vs-models-false-binary",
          "excerpt": "A recent IEEE Spectrum article argues for use-based AI regulation over model-based approaches. This framing misses what actually works: layered, reflexive governance that addresses capabilities, deployment, and systemic accountability together."
        },
        {
          "title": "Simulating Governance: Using AI to Stress-Test AI Regulations",
          "slug": "/research/072-simulating-governance",
          "excerpt": "Regulations are policies. Policies can be simulated. What if we used AI systems to model the effects of proposed regulations before implementation, identifying loopholes and unintended consequences?"
        },
        {
          "title": "The Small Actor Problem: How AI Regulation Shapes Market Structure",
          "slug": "/research/075-small-actor-problem",
          "excerpt": "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
        },
        {
          "title": "Governance for Artificial General Intelligence",
          "slug": "/research/086-governance-for-artificial-general-intelligence",
          "excerpt": "Examining the unique challenges and frameworks required to govern Artificial General Intelligence (AGI), with a focus on safety, accountability, and the role of reflexive AI in regulatory compliance."
        }
      ]
    },
    {
      "id": "eu-ai-act",
      "title": "eu-ai-act",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Operationalizing Proportionality in Model Disclosure",
          "slug": "/research/001-proportionality-disclosure",
          "excerpt": "How disclosure requirements should scale with model capability, moving from static to reflexive transparency."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        }
      ]
    },
    {
      "id": "proportionality",
      "title": "proportionality",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Operationalizing Proportionality in Model Disclosure",
          "slug": "/research/001-proportionality-disclosure",
          "excerpt": "How disclosure requirements should scale with model capability, moving from static to reflexive transparency."
        }
      ]
    },
    {
      "id": "transparency",
      "title": "transparency",
      "group": "concept",
      "val": 30,
      "articles": [
        {
          "title": "Operationalizing Proportionality in Model Disclosure",
          "slug": "/research/001-proportionality-disclosure",
          "excerpt": "How disclosure requirements should scale with model capability, moving from static to reflexive transparency."
        },
        {
          "title": "The Open Weight Safety Paradox",
          "slug": "/research/002-open-weight-safety-paradox",
          "excerpt": "Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework."
        },
        {
          "title": "Policy Brief: The Disclosure Tiers Framework",
          "slug": "/research/005-policy-brief-disclosure-tiers",
          "excerpt": "A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation."
        },
        {
          "title": "Incident Reporting Systems: Lessons from Aviation",
          "slug": "/research/021-aviation-lessons",
          "excerpt": "Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?"
        },
        {
          "title": "Whistleblower Protections in AI Labs",
          "slug": "/research/022-whistleblower-protections",
          "excerpt": "Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require."
        },
        {
          "title": "AI Systems Explaining Their Constraints",
          "slug": "/research/026-explaining-constraints",
          "excerpt": "When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches."
        },
        {
          "title": "Uncertainty Communication in AI Outputs",
          "slug": "/research/027-uncertainty-communication",
          "excerpt": "AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it."
        },
        {
          "title": "The Honest AI Problem",
          "slug": "/research/029-honest-ai",
          "excerpt": "Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness."
        },
        {
          "title": "A Reflexive AI Manifesto",
          "slug": "/research/030-manifesto",
          "excerpt": "A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to."
        },
        {
          "title": "Corporate Governance Structures for AI Safety",
          "slug": "/research/042-corporate-governance",
          "excerpt": "How companies organize to manage AI safety matters as much as what rules they follow. An examination of governance structures that enable—or undermine—responsible AI development."
        },
        {
          "title": "Board-Level AI Oversight: Best Practices",
          "slug": "/research/043-board-oversight",
          "excerpt": "Boards of directors increasingly need to oversee AI strategy and risk. A practical guide to what effective board-level AI oversight looks like."
        },
        {
          "title": "The Role of Civil Society in AI Governance",
          "slug": "/research/044-civil-society-role",
          "excerpt": "Beyond companies and regulators: how civil society organizations contribute to AI governance, and how their role could be strengthened."
        },
        {
          "title": "Public Participation in AI Policy",
          "slug": "/research/045-public-participation",
          "excerpt": "How can ordinary citizens meaningfully participate in decisions about AI that will affect their lives? An examination of mechanisms for democratic AI governance."
        },
        {
          "title": "Training Data Governance",
          "slug": "/research/048-training-data-governance",
          "excerpt": "Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement."
        },
        {
          "title": "Interpretability as a Governance Tool",
          "slug": "/research/051-interpretability-as-a-governance-tool",
          "excerpt": "How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response."
        },
        {
          "title": "The Game Theory of AI Disclosure: When Transparency is a Prisoner's Dilemma",
          "slug": "/research/067-game-theory-disclosure",
          "excerpt": "Companies face a collective action problem: all would benefit from industry-wide transparency, but unilateral disclosure may harm competitive position. How do we change the payoff structure?"
        }
      ]
    },
    {
      "id": "open-source",
      "title": "open-source",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Open Weight Safety Paradox",
          "slug": "/research/002-open-weight-safety-paradox",
          "excerpt": "Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework."
        }
      ]
    },
    {
      "id": "safety",
      "title": "safety",
      "group": "concept",
      "val": 30,
      "articles": [
        {
          "title": "The Open Weight Safety Paradox",
          "slug": "/research/002-open-weight-safety-paradox",
          "excerpt": "Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework."
        },
        {
          "title": "Red Lines: A Taxonomy of Non-Negotiable AI Limits",
          "slug": "/research/004-red-lines-taxonomy",
          "excerpt": "Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits."
        },
        {
          "title": "The Capability Overhang",
          "slug": "/research/009-capability-overhang",
          "excerpt": "Models are often capable of more than their developers know. This 'overhang' between demonstrated and latent capability is a primary governance risk."
        },
        {
          "title": "The Limits of Self-Constraint",
          "slug": "/research/013-limits-of-self-constraint",
          "excerpt": "Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself."
        },
        {
          "title": "What Alignment Actually Means",
          "slug": "/research/016-what-alignment-means",
          "excerpt": "Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?"
        },
        {
          "title": "Incident Reporting Systems: Lessons from Aviation",
          "slug": "/research/021-aviation-lessons",
          "excerpt": "Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?"
        },
        {
          "title": "Whistleblower Protections in AI Labs",
          "slug": "/research/022-whistleblower-protections",
          "excerpt": "Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require."
        },
        {
          "title": "Compute Governance: Promises and Limits",
          "slug": "/research/023-compute-governance",
          "excerpt": "Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment."
        },
        {
          "title": "Dangerous Capability Evaluations",
          "slug": "/research/024-capability-evaluations",
          "excerpt": "Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward."
        },
        {
          "title": "When AI Should Refuse: A Framework",
          "slug": "/research/025-when-ai-should-refuse",
          "excerpt": "Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they're appropriate, how they should be implemented, and how to handle edge cases."
        },
        {
          "title": "AI in Healthcare: Governance Challenges",
          "slug": "/research/028-healthcare-ai",
          "excerpt": "Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges."
        },
        {
          "title": "Understanding Frontier AI: A Plain Language Guide",
          "slug": "/research/031-understanding-frontier-ai",
          "excerpt": "What makes today's most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand."
        },
        {
          "title": "Technical Safety vs. Societal Safety: Different Problems",
          "slug": "/research/034-technical-vs-societal-safety",
          "excerpt": "Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."
        },
        {
          "title": "Dual-Use AI: The Biological Research Case",
          "slug": "/research/035-dual-use-biology",
          "excerpt": "How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance."
        },
        {
          "title": "Certification Regimes for AI Systems",
          "slug": "/research/041-certification-regimes",
          "excerpt": "Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges."
        },
        {
          "title": "Corporate Governance Structures for AI Safety",
          "slug": "/research/042-corporate-governance",
          "excerpt": "How companies organize to manage AI safety matters as much as what rules they follow. An examination of governance structures that enable—or undermine—responsible AI development."
        },
        {
          "title": "Board-Level AI Oversight: Best Practices",
          "slug": "/research/043-board-oversight",
          "excerpt": "Boards of directors increasingly need to oversee AI strategy and risk. A practical guide to what effective board-level AI oversight looks like."
        },
        {
          "title": "Pre-Deployment Risk Assessment Frameworks",
          "slug": "/research/047-pre-deployment-risk-assessment",
          "excerpt": "Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints."
        },
        {
          "title": "Model Evaluation Standards: Current State",
          "slug": "/research/049-model-evaluation-standards",
          "excerpt": "A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment."
        },
        {
          "title": "Red Teaming Methodologies",
          "slug": "/research/050-red-teaming-methodologies",
          "excerpt": "Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification."
        },
        {
          "title": "Model Versioning and Rollback Protocols",
          "slug": "/research/058-model-versioning-and-rollback-protocols",
          "excerpt": "Examining the role of versioning and rollback mechanisms in AI governance to ensure safety, accountability, and operational continuity."
        },
        {
          "title": "Hardware-Level Safety Mechanisms",
          "slug": "/research/060-hardware-level-safety-mechanisms",
          "excerpt": "Exploring how hardware design can embed safety and security features directly into AI systems, with implications for governance and risk mitigation."
        },
        {
          "title": "Self-Modifying Constraints: Technical Approaches",
          "slug": "/research/061-self-modifying-constraints-technical-approaches",
          "excerpt": "Exploring how AI systems can be governed through self-modifying constraints, bridging technical architecture with safety and oversight frameworks."
        },
        {
          "title": "The Speed-Safety Tradeoff: Making the Implicit Explicit",
          "slug": "/research/077-speed-safety-tradeoff",
          "excerpt": "Move fast and break things' vs 'go slow and be careful.' AI development constantly navigates this tension, but rarely discusses it explicitly. What does the tradeoff actually involve, and how should different actors balance it?"
        },
        {
          "title": "The Economics of AI Safety: Who Pays and Why It Matters",
          "slug": "/research/078-economics-ai-safety",
          "excerpt": "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
        },
        {
          "title": "Governance for Artificial General Intelligence",
          "slug": "/research/086-governance-for-artificial-general-intelligence",
          "excerpt": "Examining the unique challenges and frameworks required to govern Artificial General Intelligence (AGI), with a focus on safety, accountability, and the role of reflexive AI in regulatory compliance."
        },
        {
          "title": "Multi-Agent Coordination Failures",
          "slug": "/research/088-multi-agent-coordination-failures",
          "excerpt": "Exploring the dynamics, risks, and governance challenges of coordination failures among AI systems in multi-agent environments."
        }
      ]
    },
    {
      "id": "access-control",
      "title": "access-control",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Open Weight Safety Paradox",
          "slug": "/research/002-open-weight-safety-paradox",
          "excerpt": "Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework."
        }
      ]
    },
    {
      "id": "dual-use",
      "title": "dual-use",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "The Open Weight Safety Paradox",
          "slug": "/research/002-open-weight-safety-paradox",
          "excerpt": "Open-weight AI models present a governance contradiction: transparency enables both safety research and misuse. This note analyzes the structural tension and proposes a differentiated access framework."
        },
        {
          "title": "Dual-Use AI: The Biological Research Case",
          "slug": "/research/035-dual-use-biology",
          "excerpt": "How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance."
        }
      ]
    },
    {
      "id": "json-ld",
      "title": "json-ld",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "A Machine-Readable Constraint Schema (MRCS)",
          "slug": "/research/003-machine-readable-constraint-schema",
          "excerpt": "A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt."
        }
      ]
    },
    {
      "id": "machine-readable",
      "title": "machine-readable",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "A Machine-Readable Constraint Schema (MRCS)",
          "slug": "/research/003-machine-readable-constraint-schema",
          "excerpt": "A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt."
        },
        {
          "title": "AI Systems Explaining Their Constraints",
          "slug": "/research/026-explaining-constraints",
          "excerpt": "When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches."
        }
      ]
    },
    {
      "id": "standards",
      "title": "standards",
      "group": "concept",
      "val": 19,
      "articles": [
        {
          "title": "A Machine-Readable Constraint Schema (MRCS)",
          "slug": "/research/003-machine-readable-constraint-schema",
          "excerpt": "A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt."
        },
        {
          "title": "Incident Reporting Systems: Lessons from Aviation",
          "slug": "/research/021-aviation-lessons",
          "excerpt": "Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?"
        },
        {
          "title": "The Role of Standards Bodies in AI Governance",
          "slug": "/research/039-standards-bodies",
          "excerpt": "Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications."
        },
        {
          "title": "Soft Law vs. Hard Law in AI Regulation",
          "slug": "/research/040-soft-law-hard-law",
          "excerpt": "AI governance uses both binding legislation and non-binding guidelines. An analysis of when each approach works, their tradeoffs, and how they interact."
        },
        {
          "title": "Certification Regimes for AI Systems",
          "slug": "/research/041-certification-regimes",
          "excerpt": "Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges."
        },
        {
          "title": "Model Evaluation Standards: Current State",
          "slug": "/research/049-model-evaluation-standards",
          "excerpt": "A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment."
        },
        {
          "title": "AI in Climate Modeling: Validation Standards",
          "slug": "/research/084-ai-in-climate-modeling-validation-standards",
          "excerpt": "Establishing rigorous validation standards for AI-driven climate models is essential to ensure their reliability, transparency, and utility in addressing global environmental challenges."
        }
      ]
    },
    {
      "id": "interoperability",
      "title": "interoperability",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "A Machine-Readable Constraint Schema (MRCS)",
          "slug": "/research/003-machine-readable-constraint-schema",
          "excerpt": "A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt."
        },
        {
          "title": "The Role of Standards Bodies in AI Governance",
          "slug": "/research/039-standards-bodies",
          "excerpt": "Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications."
        }
      ]
    },
    {
      "id": "agents",
      "title": "agents",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "A Machine-Readable Constraint Schema (MRCS)",
          "slug": "/research/003-machine-readable-constraint-schema",
          "excerpt": "A proposed JSON-LD specification for expressing AI governance constraints in a format that agents can natively parse, validate, and adopt."
        },
        {
          "title": "When AI Should Refuse: A Framework",
          "slug": "/research/025-when-ai-should-refuse",
          "excerpt": "Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they're appropriate, how they should be implemented, and how to handle edge cases."
        },
        {
          "title": "AI Systems Explaining Their Constraints",
          "slug": "/research/026-explaining-constraints",
          "excerpt": "When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches."
        },
        {
          "title": "Uncertainty Communication in AI Outputs",
          "slug": "/research/027-uncertainty-communication",
          "excerpt": "AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it."
        }
      ]
    },
    {
      "id": "constraints",
      "title": "constraints",
      "group": "concept",
      "val": 21,
      "articles": [
        {
          "title": "Red Lines: A Taxonomy of Non-Negotiable AI Limits",
          "slug": "/research/004-red-lines-taxonomy",
          "excerpt": "Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits."
        },
        {
          "title": "When AI Should Refuse: A Framework",
          "slug": "/research/025-when-ai-should-refuse",
          "excerpt": "Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they're appropriate, how they should be implemented, and how to handle edge cases."
        },
        {
          "title": "AI Systems Explaining Their Constraints",
          "slug": "/research/026-explaining-constraints",
          "excerpt": "When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches."
        },
        {
          "title": "The Honest AI Problem",
          "slug": "/research/029-honest-ai",
          "excerpt": "Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness."
        },
        {
          "title": "A Reflexive AI Manifesto",
          "slug": "/research/030-manifesto",
          "excerpt": "A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to."
        },
        {
          "title": "Reflexive AI in Practice: A Case Study of Constraint Failures",
          "slug": "/research/068-constraint-failure-cases",
          "excerpt": "Rather than theoretical, this examines documented cases where AI systems violated their stated constraints. What went wrong? Were the constraints poorly specified, not enforced, or gamed?"
        },
        {
          "title": "The Semantic Gap Problem: Why Natural Language Constraints Fail",
          "slug": "/research/069-semantic-gap-problem",
          "excerpt": "We specify AI constraints in natural language, but models operate on statistical patterns. This disconnect means constraints may not do what we think. What are the technical approaches to bridging this gap?"
        },
        {
          "title": "Who Decides What AI Should Refuse? The Democratic Deficit in Constraint Design",
          "slug": "/research/070-democratic-deficit-constraints",
          "excerpt": "AI refusals encode value judgments. Currently, small teams at AI labs make these decisions. Is this legitimate? Exploring the democratic deficit in AI constraint design and possible alternatives."
        }
      ]
    },
    {
      "id": "red-lines",
      "title": "red-lines",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Red Lines: A Taxonomy of Non-Negotiable AI Limits",
          "slug": "/research/004-red-lines-taxonomy",
          "excerpt": "Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits."
        },
        {
          "title": "When AI Should Refuse: A Framework",
          "slug": "/research/025-when-ai-should-refuse",
          "excerpt": "Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they're appropriate, how they should be implemented, and how to handle edge cases."
        }
      ]
    },
    {
      "id": "taxonomy",
      "title": "taxonomy",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Red Lines: A Taxonomy of Non-Negotiable AI Limits",
          "slug": "/research/004-red-lines-taxonomy",
          "excerpt": "Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits."
        }
      ]
    },
    {
      "id": "cbrn",
      "title": "cbrn",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Red Lines: A Taxonomy of Non-Negotiable AI Limits",
          "slug": "/research/004-red-lines-taxonomy",
          "excerpt": "Not all constraints are created equal. This note proposes a taxonomic hierarchy for AI red lines, distinguishing between hard, soft, and contextual limits."
        },
        {
          "title": "Dual-Use AI: The Biological Research Case",
          "slug": "/research/035-dual-use-biology",
          "excerpt": "How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance."
        }
      ]
    },
    {
      "id": "policy",
      "title": "policy",
      "group": "governance",
      "val": 30,
      "articles": [
        {
          "title": "Policy Brief: The Disclosure Tiers Framework",
          "slug": "/research/005-policy-brief-disclosure-tiers",
          "excerpt": "A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation."
        },
        {
          "title": "AI Governance for Non-Experts: A Primer",
          "slug": "/research/017-governance-primer",
          "excerpt": "A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who's doing it."
        },
        {
          "title": "Why 'Just Regulate AI' Is Harder Than It Sounds",
          "slug": "/research/018-regulation-is-hard",
          "excerpt": "Regulation seems like the obvious answer to AI risks. But the path from 'we should regulate AI' to effective governance is fraught with technical, political, and conceptual obstacles."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        },
        {
          "title": "The History of AI Governance in 2000 Words",
          "slug": "/research/032-history-of-ai-governance",
          "excerpt": "From Asimov's Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades."
        },
        {
          "title": "What Policymakers Get Wrong About AI Risk",
          "slug": "/research/033-policymaker-misconceptions",
          "excerpt": "Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems."
        },
        {
          "title": "Sandboxing Approaches: What Works",
          "slug": "/research/037-sandboxing-approaches",
          "excerpt": "Regulatory sandboxes for AI allow experimentation under controlled conditions. An analysis of existing approaches, what makes them effective, and their limitations."
        },
        {
          "title": "International AI Treaty Proposals: A Comparative Analysis",
          "slug": "/research/038-international-treaties",
          "excerpt": "From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects."
        },
        {
          "title": "Soft Law vs. Hard Law in AI Regulation",
          "slug": "/research/040-soft-law-hard-law",
          "excerpt": "AI governance uses both binding legislation and non-binding guidelines. An analysis of when each approach works, their tradeoffs, and how they interact."
        },
        {
          "title": "The Role of Civil Society in AI Governance",
          "slug": "/research/044-civil-society-role",
          "excerpt": "Beyond companies and regulators: how civil society organizations contribute to AI governance, and how their role could be strengthened."
        },
        {
          "title": "Public Participation in AI Policy",
          "slug": "/research/045-public-participation",
          "excerpt": "How can ordinary citizens meaningfully participate in decisions about AI that will affect their lives? An examination of mechanisms for democratic AI governance."
        },
        {
          "title": "The False Binary: Why 'Regulate Use, Not Models' Gets AI Governance Wrong",
          "slug": "/research/062-use-vs-models-false-binary",
          "excerpt": "A recent IEEE Spectrum article argues for use-based AI regulation over model-based approaches. This framing misses what actually works: layered, reflexive governance that addresses capabilities, deployment, and systemic accountability together."
        },
        {
          "title": "AI in Agriculture: Data Governance",
          "slug": "/research/083-ai-in-agriculture-data-governance",
          "excerpt": "Exploring the governance challenges of data use in agricultural AI systems, with a focus on ethical, regulatory, and technical considerations for sustainable and equitable outcomes."
        }
      ]
    },
    {
      "id": "guide",
      "title": "guide",
      "group": "concept",
      "val": 15,
      "articles": [
        {
          "title": "Policy Brief: The Disclosure Tiers Framework",
          "slug": "/research/005-policy-brief-disclosure-tiers",
          "excerpt": "A plain-language guide for policymakers: why one-size-fits-all AI transparency fails, and how a tiered approach can balance safety with innovation."
        },
        {
          "title": "What Alignment Actually Means",
          "slug": "/research/016-what-alignment-means",
          "excerpt": "Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?"
        },
        {
          "title": "AI Governance for Non-Experts: A Primer",
          "slug": "/research/017-governance-primer",
          "excerpt": "A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who's doing it."
        },
        {
          "title": "Understanding Frontier AI: A Plain Language Guide",
          "slug": "/research/031-understanding-frontier-ai",
          "excerpt": "What makes today's most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand."
        },
        {
          "title": "The History of AI Governance in 2000 Words",
          "slug": "/research/032-history-of-ai-governance",
          "excerpt": "From Asimov's Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades."
        }
      ]
    },
    {
      "id": "auditing",
      "title": "auditing",
      "group": "governance",
      "val": 13,
      "articles": [
        {
          "title": "Meta-Governance: Who Audits the Auditors?",
          "slug": "/research/006-meta-governance-auditors",
          "excerpt": "As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a 'proof-of-verification' protocol."
        },
        {
          "title": "Self-Reporting vs. External Audit: The Trade-off Space",
          "slug": "/research/010-self-reporting-vs-audit",
          "excerpt": "A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification."
        },
        {
          "title": "Incident Reporting Systems: Lessons from Aviation",
          "slug": "/research/021-aviation-lessons",
          "excerpt": "Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?"
        },
        {
          "title": "Dangerous Capability Evaluations",
          "slug": "/research/024-capability-evaluations",
          "excerpt": "Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward."
        }
      ]
    },
    {
      "id": "meta-governance",
      "title": "meta-governance",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Meta-Governance: Who Audits the Auditors?",
          "slug": "/research/006-meta-governance-auditors",
          "excerpt": "As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a 'proof-of-verification' protocol."
        },
        {
          "title": "Governance Fragmentation: Too Many Frameworks, Not Enough Coherence",
          "slug": "/research/082-governance-fragmentation",
          "excerpt": "The AI governance landscape is crowded with frameworks: principles, guidelines, regulations, and proposals proliferate. This abundance creates incoherence. How do we synthesize without premature standardization?"
        }
      ]
    },
    {
      "id": "institutional-design",
      "title": "institutional-design",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "Meta-Governance: Who Audits the Auditors?",
          "slug": "/research/006-meta-governance-auditors",
          "excerpt": "As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a 'proof-of-verification' protocol."
        },
        {
          "title": "Self-Reporting vs. External Audit: The Trade-off Space",
          "slug": "/research/010-self-reporting-vs-audit",
          "excerpt": "A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification."
        },
        {
          "title": "Corporate Governance Structures for AI Safety",
          "slug": "/research/042-corporate-governance",
          "excerpt": "How companies organize to manage AI safety matters as much as what rules they follow. An examination of governance structures that enable—or undermine—responsible AI development."
        },
        {
          "title": "Board-Level AI Oversight: Best Practices",
          "slug": "/research/043-board-oversight",
          "excerpt": "Boards of directors increasingly need to oversee AI strategy and risk. A practical guide to what effective board-level AI oversight looks like."
        }
      ]
    },
    {
      "id": "incentives",
      "title": "incentives",
      "group": "concept",
      "val": 15,
      "articles": [
        {
          "title": "Meta-Governance: Who Audits the Auditors?",
          "slug": "/research/006-meta-governance-auditors",
          "excerpt": "As third-party auditing becomes a regulatory requirement, a new principal-agent problem emerges. This note analyzes the certification market and proposes a 'proof-of-verification' protocol."
        },
        {
          "title": "Self-Reporting vs. External Audit: The Trade-off Space",
          "slug": "/research/010-self-reporting-vs-audit",
          "excerpt": "A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification."
        },
        {
          "title": "Insurance Markets and AI Risk Pricing",
          "slug": "/research/036-insurance-markets",
          "excerpt": "How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations."
        },
        {
          "title": "The Game Theory of AI Disclosure: When Transparency is a Prisoner's Dilemma",
          "slug": "/research/067-game-theory-disclosure",
          "excerpt": "Companies face a collective action problem: all would benefit from industry-wide transparency, but unilateral disclosure may harm competitive position. How do we change the payoff structure?"
        },
        {
          "title": "The Economics of AI Safety: Who Pays and Why It Matters",
          "slug": "/research/078-economics-ai-safety",
          "excerpt": "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
        }
      ]
    },
    {
      "id": "ethics",
      "title": "ethics",
      "group": "concept",
      "val": 25,
      "articles": [
        {
          "title": "Consent at Scale: A Structural Impossibility?",
          "slug": "/research/007-consent-structural-impossibility",
          "excerpt": "Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that 'consent' is the wrong legal primitive for AI interactions."
        },
        {
          "title": "What Alignment Actually Means",
          "slug": "/research/016-what-alignment-means",
          "excerpt": "Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?"
        },
        {
          "title": "AI Governance for Non-Experts: A Primer",
          "slug": "/research/017-governance-primer",
          "excerpt": "A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who's doing it."
        },
        {
          "title": "When AI Should Refuse: A Framework",
          "slug": "/research/025-when-ai-should-refuse",
          "excerpt": "Not every request should be fulfilled. This analysis develops a principled framework for AI refusals—when they're appropriate, how they should be implemented, and how to handle edge cases."
        },
        {
          "title": "AI Systems Explaining Their Constraints",
          "slug": "/research/026-explaining-constraints",
          "excerpt": "When AI refuses or limits its behavior, can it explain why? This analysis examines constraint explainability—its value for governance, technical challenges, and implementation approaches."
        },
        {
          "title": "The Honest AI Problem",
          "slug": "/research/029-honest-ai",
          "excerpt": "Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness."
        },
        {
          "title": "A Reflexive AI Manifesto",
          "slug": "/research/030-manifesto",
          "excerpt": "A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to."
        },
        {
          "title": "Technical Safety vs. Societal Safety: Different Problems",
          "slug": "/research/034-technical-vs-societal-safety",
          "excerpt": "Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."
        },
        {
          "title": "The Role of Civil Society in AI Governance",
          "slug": "/research/044-civil-society-role",
          "excerpt": "Beyond companies and regulators: how civil society organizations contribute to AI governance, and how their role could be strengthened."
        },
        {
          "title": "Public Participation in AI Policy",
          "slug": "/research/045-public-participation",
          "excerpt": "How can ordinary citizens meaningfully participate in decisions about AI that will affect their lives? An examination of mechanisms for democratic AI governance."
        }
      ]
    },
    {
      "id": "consent",
      "title": "consent",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Consent at Scale: A Structural Impossibility?",
          "slug": "/research/007-consent-structural-impossibility",
          "excerpt": "Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that 'consent' is the wrong legal primitive for AI interactions."
        }
      ]
    },
    {
      "id": "legal-theory",
      "title": "legal-theory",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Consent at Scale: A Structural Impossibility?",
          "slug": "/research/007-consent-structural-impossibility",
          "excerpt": "Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that 'consent' is the wrong legal primitive for AI interactions."
        },
        {
          "title": "Liability Frameworks for AI Harm",
          "slug": "/research/020-liability-frameworks",
          "excerpt": "When AI systems cause harm, who pays? Existing liability frameworks struggle with AI's distinctive features. This analysis maps the problem and evaluates potential solutions."
        }
      ]
    },
    {
      "id": "data-rights",
      "title": "data-rights",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Consent at Scale: A Structural Impossibility?",
          "slug": "/research/007-consent-structural-impossibility",
          "excerpt": "Can meaningful consent exist between a human and a hyper-scale inference engine? We argue that 'consent' is the wrong legal primitive for AI interactions."
        }
      ]
    },
    {
      "id": "arbitrage",
      "title": "arbitrage",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Regulatory Arbitrage in Deployment Architectures",
          "slug": "/research/008-regulatory-arbitrage",
          "excerpt": "How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints."
        }
      ]
    },
    {
      "id": "jurisdiction",
      "title": "jurisdiction",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "Regulatory Arbitrage in Deployment Architectures",
          "slug": "/research/008-regulatory-arbitrage",
          "excerpt": "How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints."
        },
        {
          "title": "Why 'Just Regulate AI' Is Harder Than It Sounds",
          "slug": "/research/018-regulation-is-hard",
          "excerpt": "Regulation seems like the obvious answer to AI risks. But the path from 'we should regulate AI' to effective governance is fraught with technical, political, and conceptual obstacles."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        },
        {
          "title": "International AI Treaty Proposals: A Comparative Analysis",
          "slug": "/research/038-international-treaties",
          "excerpt": "From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects."
        }
      ]
    },
    {
      "id": "deployment",
      "title": "deployment",
      "group": "concept",
      "val": 17,
      "articles": [
        {
          "title": "Regulatory Arbitrage in Deployment Architectures",
          "slug": "/research/008-regulatory-arbitrage",
          "excerpt": "How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints."
        },
        {
          "title": "Dangerous Capability Evaluations",
          "slug": "/research/024-capability-evaluations",
          "excerpt": "Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward."
        },
        {
          "title": "Insurance Markets and AI Risk Pricing",
          "slug": "/research/036-insurance-markets",
          "excerpt": "How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations."
        },
        {
          "title": "Sandboxing Approaches: What Works",
          "slug": "/research/037-sandboxing-approaches",
          "excerpt": "Regulatory sandboxes for AI allow experimentation under controlled conditions. An analysis of existing approaches, what makes them effective, and their limitations."
        },
        {
          "title": "Certification Regimes for AI Systems",
          "slug": "/research/041-certification-regimes",
          "excerpt": "Could AI systems be certified for safety like aircraft or medical devices? An analysis of what AI certification might look like, its benefits, and significant challenges."
        },
        {
          "title": "Pre-Deployment Risk Assessment Frameworks",
          "slug": "/research/047-pre-deployment-risk-assessment",
          "excerpt": "Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints."
        }
      ]
    },
    {
      "id": "enforcement",
      "title": "enforcement",
      "group": "concept",
      "val": 15,
      "articles": [
        {
          "title": "Regulatory Arbitrage in Deployment Architectures",
          "slug": "/research/008-regulatory-arbitrage",
          "excerpt": "How distributed inference and model fragmentation allow actors to bypass jurisdictional constraints."
        },
        {
          "title": "Why 'Just Regulate AI' Is Harder Than It Sounds",
          "slug": "/research/018-regulation-is-hard",
          "excerpt": "Regulation seems like the obvious answer to AI risks. But the path from 'we should regulate AI' to effective governance is fraught with technical, political, and conceptual obstacles."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        },
        {
          "title": "Liability Frameworks for AI Harm",
          "slug": "/research/020-liability-frameworks",
          "excerpt": "When AI systems cause harm, who pays? Existing liability frameworks struggle with AI's distinctive features. This analysis maps the problem and evaluates potential solutions."
        },
        {
          "title": "Compute Governance: Promises and Limits",
          "slug": "/research/023-compute-governance",
          "excerpt": "Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment."
        }
      ]
    },
    {
      "id": "capability-elicitation",
      "title": "capability-elicitation",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "The Capability Overhang",
          "slug": "/research/009-capability-overhang",
          "excerpt": "Models are often capable of more than their developers know. This 'overhang' between demonstrated and latent capability is a primary governance risk."
        },
        {
          "title": "Dangerous Capability Evaluations",
          "slug": "/research/024-capability-evaluations",
          "excerpt": "Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward."
        },
        {
          "title": "Understanding Frontier AI: A Plain Language Guide",
          "slug": "/research/031-understanding-frontier-ai",
          "excerpt": "What makes today's most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand."
        }
      ]
    },
    {
      "id": "overhang",
      "title": "overhang",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Capability Overhang",
          "slug": "/research/009-capability-overhang",
          "excerpt": "Models are often capable of more than their developers know. This 'overhang' between demonstrated and latent capability is a primary governance risk."
        }
      ]
    },
    {
      "id": "risk-assessment",
      "title": "risk-assessment",
      "group": "concept",
      "val": 17,
      "articles": [
        {
          "title": "The Capability Overhang",
          "slug": "/research/009-capability-overhang",
          "excerpt": "Models are often capable of more than their developers know. This 'overhang' between demonstrated and latent capability is a primary governance risk."
        },
        {
          "title": "Dangerous Capability Evaluations",
          "slug": "/research/024-capability-evaluations",
          "excerpt": "Before deploying powerful AI, we need to know what it can do. This analysis examines the current state of capability evaluation, its limitations, and paths forward."
        },
        {
          "title": "AI in Healthcare: Governance Challenges",
          "slug": "/research/028-healthcare-ai",
          "excerpt": "Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges."
        },
        {
          "title": "What Policymakers Get Wrong About AI Risk",
          "slug": "/research/033-policymaker-misconceptions",
          "excerpt": "Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems."
        },
        {
          "title": "Dual-Use AI: The Biological Research Case",
          "slug": "/research/035-dual-use-biology",
          "excerpt": "How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance."
        },
        {
          "title": "Insurance Markets and AI Risk Pricing",
          "slug": "/research/036-insurance-markets",
          "excerpt": "How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations."
        }
      ]
    },
    {
      "id": "game-theory",
      "title": "game-theory",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "Self-Reporting vs. External Audit: The Trade-off Space",
          "slug": "/research/010-self-reporting-vs-audit",
          "excerpt": "A game-theoretic analysis of disclosure incentives. Why self-reporting fails without a credible threat of external verification."
        },
        {
          "title": "Emergent Norms in Multi-Agent Systems",
          "slug": "/research/015-emergent-norms",
          "excerpt": "When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how 'machine law' might emerge."
        },
        {
          "title": "The Game Theory of AI Disclosure: When Transparency is a Prisoner's Dilemma",
          "slug": "/research/067-game-theory-disclosure",
          "excerpt": "Companies face a collective action problem: all would benefit from industry-wide transparency, but unilateral disclosure may harm competitive position. How do we change the payoff structure?"
        }
      ]
    },
    {
      "id": "intent-recognition",
      "title": "intent-recognition",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Can AI Systems Detect Their Own Misuse?",
          "slug": "/research/011-reflexive-misuse-detection",
          "excerpt": "Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?"
        }
      ]
    },
    {
      "id": "misuse-detection",
      "title": "misuse-detection",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Can AI Systems Detect Their Own Misuse?",
          "slug": "/research/011-reflexive-misuse-detection",
          "excerpt": "Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?"
        }
      ]
    },
    {
      "id": "reflexive-monitoring",
      "title": "reflexive-monitoring",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Can AI Systems Detect Their Own Misuse?",
          "slug": "/research/011-reflexive-misuse-detection",
          "excerpt": "Moving beyond static filters to dynamic intent recognition. Can a model understand *why* a user is asking for a specific chemical precursor?"
        }
      ]
    },
    {
      "id": "provenance",
      "title": "provenance",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Constraint: Output Provenance Tagging",
          "slug": "/research/012-output-provenance",
          "excerpt": "A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information."
        },
        {
          "title": "Watermarking and Content Provenance",
          "slug": "/research/052-watermarking-and-content-provenance",
          "excerpt": "Technical and governance approaches to marking AI-generated content and establishing chains of custody for model outputs."
        }
      ]
    },
    {
      "id": "watermarking",
      "title": "watermarking",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Constraint: Output Provenance Tagging",
          "slug": "/research/012-output-provenance",
          "excerpt": "A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information."
        },
        {
          "title": "Watermarking and Content Provenance",
          "slug": "/research/052-watermarking-and-content-provenance",
          "excerpt": "Technical and governance approaches to marking AI-generated content and establishing chains of custody for model outputs."
        }
      ]
    },
    {
      "id": "cryptography",
      "title": "cryptography",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Constraint: Output Provenance Tagging",
          "slug": "/research/012-output-provenance",
          "excerpt": "A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information."
        }
      ]
    },
    {
      "id": "c2pa",
      "title": "c2pa",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Constraint: Output Provenance Tagging",
          "slug": "/research/012-output-provenance",
          "excerpt": "A cryptographic proposal for AI systems to sign their own outputs, creating a chain of custody for synthetic information."
        }
      ]
    },
    {
      "id": "theory",
      "title": "theory",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "The Limits of Self-Constraint",
          "slug": "/research/013-limits-of-self-constraint",
          "excerpt": "Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself."
        },
        {
          "title": "What Alignment Actually Means",
          "slug": "/research/016-what-alignment-means",
          "excerpt": "Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?"
        },
        {
          "title": "The Honest AI Problem",
          "slug": "/research/029-honest-ai",
          "excerpt": "Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness."
        },
        {
          "title": "A Reflexive AI Manifesto",
          "slug": "/research/030-manifesto",
          "excerpt": "A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to."
        }
      ]
    },
    {
      "id": "limits",
      "title": "limits",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Limits of Self-Constraint",
          "slug": "/research/013-limits-of-self-constraint",
          "excerpt": "Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself."
        }
      ]
    },
    {
      "id": "paradox",
      "title": "paradox",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "The Limits of Self-Constraint",
          "slug": "/research/013-limits-of-self-constraint",
          "excerpt": "Reflexive governance is not a silver bullet. This note explores the Gödelian limits of a system trying to govern itself."
        },
        {
          "title": "The Governance Paradox: When AI Systems Are Better Regulators Than Humans",
          "slug": "/research/063-governance-paradox",
          "excerpt": "AI systems may detect regulatory violations more reliably than human auditors. This creates tension with the principle of human oversight. What does meaningful oversight mean when humans are the bottleneck?"
        }
      ]
    },
    {
      "id": "whistleblowing",
      "title": "whistleblowing",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "A Protocol for AI-to-Regulator Communication",
          "slug": "/research/014-ai-regulator-protocol",
          "excerpt": "What if AI systems could report safety incidents directly? A draft spec for the 'Whistleblower API'."
        },
        {
          "title": "Whistleblower Protections in AI Labs",
          "slug": "/research/022-whistleblower-protections",
          "excerpt": "Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require."
        }
      ]
    },
    {
      "id": "reporting",
      "title": "reporting",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "A Protocol for AI-to-Regulator Communication",
          "slug": "/research/014-ai-regulator-protocol",
          "excerpt": "What if AI systems could report safety incidents directly? A draft spec for the 'Whistleblower API'."
        },
        {
          "title": "Whistleblower Protections in AI Labs",
          "slug": "/research/022-whistleblower-protections",
          "excerpt": "Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require."
        },
        {
          "title": "Uncertainty Communication in AI Outputs",
          "slug": "/research/027-uncertainty-communication",
          "excerpt": "AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it."
        }
      ]
    },
    {
      "id": "api-design",
      "title": "api-design",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "A Protocol for AI-to-Regulator Communication",
          "slug": "/research/014-ai-regulator-protocol",
          "excerpt": "What if AI systems could report safety incidents directly? A draft spec for the 'Whistleblower API'."
        }
      ]
    },
    {
      "id": "multi-agent-systems",
      "title": "multi-agent-systems",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Emergent Norms in Multi-Agent Systems",
          "slug": "/research/015-emergent-norms",
          "excerpt": "When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how 'machine law' might emerge."
        }
      ]
    },
    {
      "id": "emergent-behavior",
      "title": "emergent-behavior",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Emergent Norms in Multi-Agent Systems",
          "slug": "/research/015-emergent-norms",
          "excerpt": "When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how 'machine law' might emerge."
        }
      ]
    },
    {
      "id": "evolution",
      "title": "evolution",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Emergent Norms in Multi-Agent Systems",
          "slug": "/research/015-emergent-norms",
          "excerpt": "When agents interact at speed and scale, human law is too slow. We look to game theory and evolution for how 'machine law' might emerge."
        }
      ]
    },
    {
      "id": "alignment",
      "title": "alignment",
      "group": "concept",
      "val": 15,
      "articles": [
        {
          "title": "What Alignment Actually Means",
          "slug": "/research/016-what-alignment-means",
          "excerpt": "Demystifying AI alignment for non-technical audiences. What are we trying to align, to what, and why is it so hard?"
        },
        {
          "title": "The Honest AI Problem",
          "slug": "/research/029-honest-ai",
          "excerpt": "Should AI systems tell the truth? The question sounds simple but reveals deep tensions between honesty, helpfulness, and harm. A conceptual analysis of AI truthfulness."
        },
        {
          "title": "Understanding Frontier AI: A Plain Language Guide",
          "slug": "/research/031-understanding-frontier-ai",
          "excerpt": "What makes today's most advanced AI systems different, why they matter for governance, and what non-technical readers need to understand."
        },
        {
          "title": "Technical Safety vs. Societal Safety: Different Problems",
          "slug": "/research/034-technical-vs-societal-safety",
          "excerpt": "Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."
        },
        {
          "title": "Why AI Safety Researchers Disagree: A Taxonomy of Worldviews",
          "slug": "/research/064-ai-safety-worldviews",
          "excerpt": "The AI safety field appears fractured. Some focus on alignment, others on governance, others on misuse. This article maps the underlying worldview differences that produce divergent research agendas."
        }
      ]
    },
    {
      "id": "governance",
      "title": "governance",
      "group": "concept",
      "val": 30,
      "articles": [
        {
          "title": "AI Governance for Non-Experts: A Primer",
          "slug": "/research/017-governance-primer",
          "excerpt": "A five-minute introduction to AI governance. No technical background required. What it is, why it matters, and who's doing it."
        },
        {
          "title": "Why 'Just Regulate AI' Is Harder Than It Sounds",
          "slug": "/research/018-regulation-is-hard",
          "excerpt": "Regulation seems like the obvious answer to AI risks. But the path from 'we should regulate AI' to effective governance is fraught with technical, political, and conceptual obstacles."
        },
        {
          "title": "The EU AI Act: What It Misses",
          "slug": "/research/019-eu-ai-act-gaps",
          "excerpt": "The EU AI Act represents the world's most comprehensive AI legislation. But even well-designed regulation has blind spots. A constructive critique of what the Act leaves unaddressed."
        },
        {
          "title": "Liability Frameworks for AI Harm",
          "slug": "/research/020-liability-frameworks",
          "excerpt": "When AI systems cause harm, who pays? Existing liability frameworks struggle with AI's distinctive features. This analysis maps the problem and evaluates potential solutions."
        },
        {
          "title": "Whistleblower Protections in AI Labs",
          "slug": "/research/022-whistleblower-protections",
          "excerpt": "Employees at AI companies often have unique insight into risks. Current protections are inadequate. This analysis examines what meaningful whistleblower frameworks for AI would require."
        },
        {
          "title": "Compute Governance: Promises and Limits",
          "slug": "/research/023-compute-governance",
          "excerpt": "Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment."
        },
        {
          "title": "A Reflexive AI Manifesto",
          "slug": "/research/030-manifesto",
          "excerpt": "A statement of principles for AI that participates in its own governance. What reflexive AI means, why it matters, and what it commits to."
        },
        {
          "title": "The History of AI Governance in 2000 Words",
          "slug": "/research/032-history-of-ai-governance",
          "excerpt": "From Asimov's Laws to the EU AI Act: how thinking about governing artificial intelligence has evolved over eight decades."
        },
        {
          "title": "What Policymakers Get Wrong About AI Risk",
          "slug": "/research/033-policymaker-misconceptions",
          "excerpt": "Common misconceptions that lead to ineffective AI policy, and how to think more clearly about the actual risks posed by advanced AI systems."
        },
        {
          "title": "Technical Safety vs. Societal Safety: Different Problems",
          "slug": "/research/034-technical-vs-societal-safety",
          "excerpt": "Why making AI systems work as intended is a different challenge from making AI development good for society—and why confusing them leads to poor governance."
        },
        {
          "title": "Dual-Use AI: The Biological Research Case",
          "slug": "/research/035-dual-use-biology",
          "excerpt": "How AI is transforming biological research—and why the same capabilities that could cure diseases could enable bioweapons. A case study in dual-use governance."
        },
        {
          "title": "Insurance Markets and AI Risk Pricing",
          "slug": "/research/036-insurance-markets",
          "excerpt": "How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations."
        },
        {
          "title": "Sandboxing Approaches: What Works",
          "slug": "/research/037-sandboxing-approaches",
          "excerpt": "Regulatory sandboxes for AI allow experimentation under controlled conditions. An analysis of existing approaches, what makes them effective, and their limitations."
        },
        {
          "title": "International AI Treaty Proposals: A Comparative Analysis",
          "slug": "/research/038-international-treaties",
          "excerpt": "From the Bletchley Declaration to proposed AI treaties: analyzing what international agreements on AI governance have been proposed, what they would achieve, and their prospects."
        },
        {
          "title": "The Role of Standards Bodies in AI Governance",
          "slug": "/research/039-standards-bodies",
          "excerpt": "Technical standards organizations may shape AI governance as much as legislation. An examination of who sets AI standards, how standards work, and their governance implications."
        },
        {
          "title": "Soft Law vs. Hard Law in AI Regulation",
          "slug": "/research/040-soft-law-hard-law",
          "excerpt": "AI governance uses both binding legislation and non-binding guidelines. An analysis of when each approach works, their tradeoffs, and how they interact."
        },
        {
          "title": "Corporate Governance Structures for AI Safety",
          "slug": "/research/042-corporate-governance",
          "excerpt": "How companies organize to manage AI safety matters as much as what rules they follow. An examination of governance structures that enable—or undermine—responsible AI development."
        },
        {
          "title": "Board-Level AI Oversight: Best Practices",
          "slug": "/research/043-board-oversight",
          "excerpt": "Boards of directors increasingly need to oversee AI strategy and risk. A practical guide to what effective board-level AI oversight looks like."
        },
        {
          "title": "The Role of Civil Society in AI Governance",
          "slug": "/research/044-civil-society-role",
          "excerpt": "Beyond companies and regulators: how civil society organizations contribute to AI governance, and how their role could be strengthened."
        },
        {
          "title": "Public Participation in AI Policy",
          "slug": "/research/045-public-participation",
          "excerpt": "How can ordinary citizens meaningfully participate in decisions about AI that will affect their lives? An examination of mechanisms for democratic AI governance."
        },
        {
          "title": "Interpretability as a Governance Tool",
          "slug": "/research/051-interpretability-as-a-governance-tool",
          "excerpt": "How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response."
        },
        {
          "title": "Watermarking and Content Provenance",
          "slug": "/research/052-watermarking-and-content-provenance",
          "excerpt": "Technical and governance approaches to marking AI-generated content and establishing chains of custody for model outputs."
        },
        {
          "title": "Secure Model Weights: Physical and Digital",
          "slug": "/research/053-secure-model-weights-physical-and-digital",
          "excerpt": "Security measures for protecting AI model weights from theft, tampering, and unauthorized access across physical and digital domains."
        },
        {
          "title": "Monitoring Deployed Models",
          "slug": "/research/056-monitoring-deployed-models",
          "excerpt": "A comprehensive framework for ensuring the safety, reliability, and accountability of AI models post-deployment."
        },
        {
          "title": "Post-Deployment Capability Discovery",
          "slug": "/research/057-post-deployment-capability-discovery",
          "excerpt": "Examining the phenomenon of emergent capabilities in deployed AI systems and its implications for safety, governance, and accountability."
        },
        {
          "title": "Differential Privacy in AI Systems",
          "slug": "/research/059-differential-privacy-in-ai-systems",
          "excerpt": "An exploration of differential privacy as a critical tool for AI governance, its practical applications, limitations, and its role in ensuring both technical and societal safety."
        },
        {
          "title": "Hardware-Level Safety Mechanisms",
          "slug": "/research/060-hardware-level-safety-mechanisms",
          "excerpt": "Exploring how hardware design can embed safety and security features directly into AI systems, with implications for governance and risk mitigation."
        },
        {
          "title": "Self-Modifying Constraints: Technical Approaches",
          "slug": "/research/061-self-modifying-constraints-technical-approaches",
          "excerpt": "Exploring how AI systems can be governed through self-modifying constraints, bridging technical architecture with safety and oversight frameworks."
        },
        {
          "title": "The False Binary: Why 'Regulate Use, Not Models' Gets AI Governance Wrong",
          "slug": "/research/062-use-vs-models-false-binary",
          "excerpt": "A recent IEEE Spectrum article argues for use-based AI regulation over model-based approaches. This framing misses what actually works: layered, reflexive governance that addresses capabilities, deployment, and systemic accountability together."
        },
        {
          "title": "The Governance Paradox: When AI Systems Are Better Regulators Than Humans",
          "slug": "/research/063-governance-paradox",
          "excerpt": "AI systems may detect regulatory violations more reliably than human auditors. This creates tension with the principle of human oversight. What does meaningful oversight mean when humans are the bottleneck?"
        },
        {
          "title": "Why AI Safety Researchers Disagree: A Taxonomy of Worldviews",
          "slug": "/research/064-ai-safety-worldviews",
          "excerpt": "The AI safety field appears fractured. Some focus on alignment, others on governance, others on misuse. This article maps the underlying worldview differences that produce divergent research agendas."
        },
        {
          "title": "The Attention Economy Meets AI Governance: Designing for Distraction-Resistant Oversight",
          "slug": "/research/065-attention-economy-governance",
          "excerpt": "Human oversight assumes humans are paying attention. But modern interfaces are designed to maximize engagement, not careful evaluation. How do attention-economy dynamics undermine governance?"
        },
        {
          "title": "AI in Education: Personalization vs. Privacy",
          "slug": "/research/085-ai-in-education-personalization-vs-privacy",
          "excerpt": "This article examines the tension between personalization and privacy in AI-driven educational tools, exploring governance frameworks, technological solutions, and ethical trade-offs."
        }
      ]
    },
    {
      "id": "liability",
      "title": "liability",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "Liability Frameworks for AI Harm",
          "slug": "/research/020-liability-frameworks",
          "excerpt": "When AI systems cause harm, who pays? Existing liability frameworks struggle with AI's distinctive features. This analysis maps the problem and evaluates potential solutions."
        },
        {
          "title": "AI in Healthcare: Governance Challenges",
          "slug": "/research/028-healthcare-ai",
          "excerpt": "Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges."
        },
        {
          "title": "Insurance Markets and AI Risk Pricing",
          "slug": "/research/036-insurance-markets",
          "excerpt": "How insurance markets could help govern AI by pricing risk, incentivizing safety, and providing accountability. An analysis of possibilities and limitations."
        },
        {
          "title": "The Liability Vacuum: When AI Harms Fall Between Legal Categories",
          "slug": "/research/071-liability-vacuum",
          "excerpt": "AI harms often don't fit existing legal frameworks: not quite product liability, not quite professional malpractice, not quite negligence. This article maps the specific gaps and proposes targeted fixes."
        }
      ]
    },
    {
      "id": "incident-reporting",
      "title": "incident-reporting",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Incident Reporting Systems: Lessons from Aviation",
          "slug": "/research/021-aviation-lessons",
          "excerpt": "Aviation has developed sophisticated systems for reporting and learning from incidents. What can AI governance learn from this decades-long experiment in safety culture?"
        }
      ]
    },
    {
      "id": "compute",
      "title": "compute",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Compute Governance: Promises and Limits",
          "slug": "/research/023-compute-governance",
          "excerpt": "Compute is one of the few measurable inputs to AI development. Governing at the compute layer is appealing but faces significant challenges. An honest assessment."
        }
      ]
    },
    {
      "id": "uncertainty",
      "title": "uncertainty",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Uncertainty Communication in AI Outputs",
          "slug": "/research/027-uncertainty-communication",
          "excerpt": "AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it."
        },
        {
          "title": "When Experts Were Wrong: Epistemic Humility in AI Predictions",
          "slug": "/research/074-when-experts-wrong",
          "excerpt": "AI experts have a poor track record of prediction. Timelines, capabilities, and societal impacts have been consistently misjudged. What should this teach us about confidence in current claims?"
        }
      ]
    },
    {
      "id": "trust",
      "title": "trust",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Uncertainty Communication in AI Outputs",
          "slug": "/research/027-uncertainty-communication",
          "excerpt": "AI systems often present confident outputs when genuine uncertainty exists. This analysis examines how AI can better communicate its uncertainty—and why governance requires it."
        },
        {
          "title": "Trust Calibration: Teaching Users When to Believe AI",
          "slug": "/research/079-trust-calibration",
          "excerpt": "Most AI governance focuses on developers and deployers. But users make trust decisions constantly: should I believe this output? Follow this recommendation? This article explores user-facing trust calibration."
        }
      ]
    },
    {
      "id": "healthcare",
      "title": "healthcare",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI in Healthcare: Governance Challenges",
          "slug": "/research/028-healthcare-ai",
          "excerpt": "Healthcare AI promises better diagnoses, treatments, and outcomes. But it also concentrates critical decisions in opaque systems. A domain-specific analysis of governance challenges."
        }
      ]
    },
    {
      "id": "impact assessment",
      "title": "Impact Assessment",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Algorithmic Impact Assessments: Implementation Guide",
          "slug": "/research/046-algorithmic-impact-assessments",
          "excerpt": "A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention."
        }
      ]
    },
    {
      "id": "risk governance",
      "title": "Risk Governance",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Algorithmic Impact Assessments: Implementation Guide",
          "slug": "/research/046-algorithmic-impact-assessments",
          "excerpt": "A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention."
        }
      ]
    },
    {
      "id": "implementation",
      "title": "Implementation",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "Algorithmic Impact Assessments: Implementation Guide",
          "slug": "/research/046-algorithmic-impact-assessments",
          "excerpt": "A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention."
        },
        {
          "title": "Reflexive AI in Practice: A Case Study of Constraint Failures",
          "slug": "/research/068-constraint-failure-cases",
          "excerpt": "Rather than theoretical, this examines documented cases where AI systems violated their stated constraints. What went wrong? Were the constraints poorly specified, not enforced, or gamed?"
        },
        {
          "title": "The Semantic Gap Problem: Why Natural Language Constraints Fail",
          "slug": "/research/069-semantic-gap-problem",
          "excerpt": "We specify AI constraints in natural language, but models operate on statistical patterns. This disconnect means constraints may not do what we think. What are the technical approaches to bridging this gap?"
        }
      ]
    },
    {
      "id": "best practices",
      "title": "Best Practices",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Algorithmic Impact Assessments: Implementation Guide",
          "slug": "/research/046-algorithmic-impact-assessments",
          "excerpt": "A practical framework for conducting meaningful algorithmic impact assessments that move beyond checkbox compliance to genuine harm prevention."
        }
      ]
    },
    {
      "id": "risk assessment",
      "title": "Risk Assessment",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Pre-Deployment Risk Assessment Frameworks",
          "slug": "/research/047-pre-deployment-risk-assessment",
          "excerpt": "Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints."
        }
      ]
    },
    {
      "id": "evaluation",
      "title": "Evaluation",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "Pre-Deployment Risk Assessment Frameworks",
          "slug": "/research/047-pre-deployment-risk-assessment",
          "excerpt": "Structured approaches to evaluating AI system risks before release, balancing comprehensiveness with practical constraints."
        },
        {
          "title": "Model Evaluation Standards: Current State",
          "slug": "/research/049-model-evaluation-standards",
          "excerpt": "A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment."
        },
        {
          "title": "Red Teaming Methodologies",
          "slug": "/research/050-red-teaming-methodologies",
          "excerpt": "Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification."
        },
        {
          "title": "Interpretability as a Governance Tool",
          "slug": "/research/051-interpretability-as-a-governance-tool",
          "excerpt": "How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response."
        }
      ]
    },
    {
      "id": "data governance",
      "title": "Data Governance",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Training Data Governance",
          "slug": "/research/048-training-data-governance",
          "excerpt": "Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement."
        }
      ]
    },
    {
      "id": "training data",
      "title": "Training Data",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Training Data Governance",
          "slug": "/research/048-training-data-governance",
          "excerpt": "Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement."
        }
      ]
    },
    {
      "id": "privacy",
      "title": "Privacy",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "Training Data Governance",
          "slug": "/research/048-training-data-governance",
          "excerpt": "Comprehensive frameworks for managing the data that shapes AI systems, from collection through curation to retirement."
        },
        {
          "title": "Differential Privacy in AI Systems",
          "slug": "/research/059-differential-privacy-in-ai-systems",
          "excerpt": "An exploration of differential privacy as a critical tool for AI governance, its practical applications, limitations, and its role in ensuring both technical and societal safety."
        },
        {
          "title": "AI in Education: Personalization vs. Privacy",
          "slug": "/research/085-ai-in-education-personalization-vs-privacy",
          "excerpt": "This article examines the tension between personalization and privacy in AI-driven educational tools, exploring governance frameworks, technological solutions, and ethical trade-offs."
        }
      ]
    },
    {
      "id": "benchmarks",
      "title": "Benchmarks",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Model Evaluation Standards: Current State",
          "slug": "/research/049-model-evaluation-standards",
          "excerpt": "A survey of existing standards and practices for evaluating AI model performance, safety, and fitness for deployment."
        }
      ]
    },
    {
      "id": "red teaming",
      "title": "Red Teaming",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Red Teaming Methodologies",
          "slug": "/research/050-red-teaming-methodologies",
          "excerpt": "Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification."
        }
      ]
    },
    {
      "id": "security",
      "title": "Security",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Red Teaming Methodologies",
          "slug": "/research/050-red-teaming-methodologies",
          "excerpt": "Structured approaches to adversarial testing of AI systems, from scope definition through remediation verification."
        },
        {
          "title": "Secure Model Weights: Physical and Digital",
          "slug": "/research/053-secure-model-weights-physical-and-digital",
          "excerpt": "Security measures for protecting AI model weights from theft, tampering, and unauthorized access across physical and digital domains."
        }
      ]
    },
    {
      "id": "interpretability",
      "title": "Interpretability",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Interpretability as a Governance Tool",
          "slug": "/research/051-interpretability-as-a-governance-tool",
          "excerpt": "How interpretability methods supply evidence for AI oversight: approvals, audits, monitoring, and incident response."
        }
      ]
    },
    {
      "id": "content authentication",
      "title": "Content Authentication",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Watermarking and Content Provenance",
          "slug": "/research/052-watermarking-and-content-provenance",
          "excerpt": "Technical and governance approaches to marking AI-generated content and establishing chains of custody for model outputs."
        }
      ]
    },
    {
      "id": "model weights",
      "title": "Model Weights",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Secure Model Weights: Physical and Digital",
          "slug": "/research/053-secure-model-weights-physical-and-digital",
          "excerpt": "Security measures for protecting AI model weights from theft, tampering, and unauthorized access across physical and digital domains."
        }
      ]
    },
    {
      "id": "infrastructure",
      "title": "Infrastructure",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Secure Model Weights: Physical and Digital",
          "slug": "/research/053-secure-model-weights-physical-and-digital",
          "excerpt": "Security measures for protecting AI model weights from theft, tampering, and unauthorized access across physical and digital domains."
        }
      ]
    },
    {
      "id": "api-controls",
      "title": "api-controls",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "API-Level Safety Controls",
          "slug": "/research/054-api-level-safety-controls",
          "excerpt": "Exploring the role of API-level safety measures in AI governance, their implementation, and their implications for mitigating malicious use and accidental harm."
        }
      ]
    },
    {
      "id": "safety-mechanisms",
      "title": "safety-mechanisms",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "API-Level Safety Controls",
          "slug": "/research/054-api-level-safety-controls",
          "excerpt": "Exploring the role of API-level safety measures in AI governance, their implementation, and their implications for mitigating malicious use and accidental harm."
        }
      ]
    },
    {
      "id": "access-management",
      "title": "access-management",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "API-Level Safety Controls",
          "slug": "/research/054-api-level-safety-controls",
          "excerpt": "Exploring the role of API-level safety measures in AI governance, their implementation, and their implications for mitigating malicious use and accidental harm."
        }
      ]
    },
    {
      "id": "ai-governance",
      "title": "ai-governance",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "API-Level Safety Controls",
          "slug": "/research/054-api-level-safety-controls",
          "excerpt": "Exploring the role of API-level safety measures in AI governance, their implementation, and their implications for mitigating malicious use and accidental harm."
        },
        {
          "title": "Model Versioning and Rollback Protocols",
          "slug": "/research/058-model-versioning-and-rollback-protocols",
          "excerpt": "Examining the role of versioning and rollback mechanisms in AI governance to ensure safety, accountability, and operational continuity."
        }
      ]
    },
    {
      "id": "rate limiting",
      "title": "rate limiting",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Rate Limiting and Abuse Detection",
          "slug": "/research/055-rate-limiting-and-abuse-detection",
          "excerpt": "A comprehensive exploration of how rate limiting and abuse detection mechanisms can be employed to improve AI system safety and governance."
        }
      ]
    },
    {
      "id": "abuse detection",
      "title": "abuse detection",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Rate Limiting and Abuse Detection",
          "slug": "/research/055-rate-limiting-and-abuse-detection",
          "excerpt": "A comprehensive exploration of how rate limiting and abuse detection mechanisms can be employed to improve AI system safety and governance."
        }
      ]
    },
    {
      "id": "ai governance",
      "title": "ai governance",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Rate Limiting and Abuse Detection",
          "slug": "/research/055-rate-limiting-and-abuse-detection",
          "excerpt": "A comprehensive exploration of how rate limiting and abuse detection mechanisms can be employed to improve AI system safety and governance."
        },
        {
          "title": "Governance for Artificial General Intelligence",
          "slug": "/research/086-governance-for-artificial-general-intelligence",
          "excerpt": "Examining the unique challenges and frameworks required to govern Artificial General Intelligence (AGI), with a focus on safety, accountability, and the role of reflexive AI in regulatory compliance."
        }
      ]
    },
    {
      "id": "safety mechanisms",
      "title": "safety mechanisms",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Rate Limiting and Abuse Detection",
          "slug": "/research/055-rate-limiting-and-abuse-detection",
          "excerpt": "A comprehensive exploration of how rate limiting and abuse detection mechanisms can be employed to improve AI system safety and governance."
        }
      ]
    },
    {
      "id": "trust and safety",
      "title": "trust and safety",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Rate Limiting and Abuse Detection",
          "slug": "/research/055-rate-limiting-and-abuse-detection",
          "excerpt": "A comprehensive exploration of how rate limiting and abuse detection mechanisms can be employed to improve AI system safety and governance."
        }
      ]
    },
    {
      "id": "monitoring",
      "title": "monitoring",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Monitoring Deployed Models",
          "slug": "/research/056-monitoring-deployed-models",
          "excerpt": "A comprehensive framework for ensuring the safety, reliability, and accountability of AI models post-deployment."
        },
        {
          "title": "Post-Deployment Capability Discovery",
          "slug": "/research/057-post-deployment-capability-discovery",
          "excerpt": "Examining the phenomenon of emergent capabilities in deployed AI systems and its implications for safety, governance, and accountability."
        }
      ]
    },
    {
      "id": "ai-safety",
      "title": "ai-safety",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Monitoring Deployed Models",
          "slug": "/research/056-monitoring-deployed-models",
          "excerpt": "A comprehensive framework for ensuring the safety, reliability, and accountability of AI models post-deployment."
        },
        {
          "title": "Why AI Safety Researchers Disagree: A Taxonomy of Worldviews",
          "slug": "/research/064-ai-safety-worldviews",
          "excerpt": "The AI safety field appears fractured. Some focus on alignment, others on governance, others on misuse. This article maps the underlying worldview differences that produce divergent research agendas."
        }
      ]
    },
    {
      "id": "accountability",
      "title": "accountability",
      "group": "concept",
      "val": 13,
      "articles": [
        {
          "title": "Monitoring Deployed Models",
          "slug": "/research/056-monitoring-deployed-models",
          "excerpt": "A comprehensive framework for ensuring the safety, reliability, and accountability of AI models post-deployment."
        },
        {
          "title": "Model Versioning and Rollback Protocols",
          "slug": "/research/058-model-versioning-and-rollback-protocols",
          "excerpt": "Examining the role of versioning and rollback mechanisms in AI governance to ensure safety, accountability, and operational continuity."
        },
        {
          "title": "The Liability Vacuum: When AI Harms Fall Between Legal Categories",
          "slug": "/research/071-liability-vacuum",
          "excerpt": "AI harms often don't fit existing legal frameworks: not quite product liability, not quite professional malpractice, not quite negligence. This article maps the specific gaps and proposes targeted fixes."
        },
        {
          "title": "AI in Climate Modeling: Validation Standards",
          "slug": "/research/084-ai-in-climate-modeling-validation-standards",
          "excerpt": "Establishing rigorous validation standards for AI-driven climate models is essential to ensure their reliability, transparency, and utility in addressing global environmental challenges."
        }
      ]
    },
    {
      "id": "emergent behavior",
      "title": "emergent behavior",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Post-Deployment Capability Discovery",
          "slug": "/research/057-post-deployment-capability-discovery",
          "excerpt": "Examining the phenomenon of emergent capabilities in deployed AI systems and its implications for safety, governance, and accountability."
        }
      ]
    },
    {
      "id": "post-deployment risks",
      "title": "post-deployment risks",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Post-Deployment Capability Discovery",
          "slug": "/research/057-post-deployment-capability-discovery",
          "excerpt": "Examining the phenomenon of emergent capabilities in deployed AI systems and its implications for safety, governance, and accountability."
        }
      ]
    },
    {
      "id": "reflexive ai",
      "title": "reflexive AI",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Post-Deployment Capability Discovery",
          "slug": "/research/057-post-deployment-capability-discovery",
          "excerpt": "Examining the phenomenon of emergent capabilities in deployed AI systems and its implications for safety, governance, and accountability."
        },
        {
          "title": "Governance for Artificial General Intelligence",
          "slug": "/research/086-governance-for-artificial-general-intelligence",
          "excerpt": "Examining the unique challenges and frameworks required to govern Artificial General Intelligence (AGI), with a focus on safety, accountability, and the role of reflexive AI in regulatory compliance."
        }
      ]
    },
    {
      "id": "versioning",
      "title": "versioning",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Model Versioning and Rollback Protocols",
          "slug": "/research/058-model-versioning-and-rollback-protocols",
          "excerpt": "Examining the role of versioning and rollback mechanisms in AI governance to ensure safety, accountability, and operational continuity."
        }
      ]
    },
    {
      "id": "rollback",
      "title": "rollback",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Model Versioning and Rollback Protocols",
          "slug": "/research/058-model-versioning-and-rollback-protocols",
          "excerpt": "Examining the role of versioning and rollback mechanisms in AI governance to ensure safety, accountability, and operational continuity."
        }
      ]
    },
    {
      "id": "ai safety",
      "title": "AI safety",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "Differential Privacy in AI Systems",
          "slug": "/research/059-differential-privacy-in-ai-systems",
          "excerpt": "An exploration of differential privacy as a critical tool for AI governance, its practical applications, limitations, and its role in ensuring both technical and societal safety."
        },
        {
          "title": "AI in Climate Modeling: Validation Standards",
          "slug": "/research/084-ai-in-climate-modeling-validation-standards",
          "excerpt": "Establishing rigorous validation standards for AI-driven climate models is essential to ensure their reliability, transparency, and utility in addressing global environmental challenges."
        }
      ]
    },
    {
      "id": "technical safeguards",
      "title": "technical safeguards",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Differential Privacy in AI Systems",
          "slug": "/research/059-differential-privacy-in-ai-systems",
          "excerpt": "An exploration of differential privacy as a critical tool for AI governance, its practical applications, limitations, and its role in ensuring both technical and societal safety."
        }
      ]
    },
    {
      "id": "data security",
      "title": "data security",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Differential Privacy in AI Systems",
          "slug": "/research/059-differential-privacy-in-ai-systems",
          "excerpt": "An exploration of differential privacy as a critical tool for AI governance, its practical applications, limitations, and its role in ensuring both technical and societal safety."
        }
      ]
    },
    {
      "id": "hardware",
      "title": "hardware",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Hardware-Level Safety Mechanisms",
          "slug": "/research/060-hardware-level-safety-mechanisms",
          "excerpt": "Exploring how hardware design can embed safety and security features directly into AI systems, with implications for governance and risk mitigation."
        }
      ]
    },
    {
      "id": "risk-mitigation",
      "title": "risk-mitigation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Hardware-Level Safety Mechanisms",
          "slug": "/research/060-hardware-level-safety-mechanisms",
          "excerpt": "Exploring how hardware design can embed safety and security features directly into AI systems, with implications for governance and risk mitigation."
        }
      ]
    },
    {
      "id": "self-modification",
      "title": "self-modification",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Self-Modifying Constraints: Technical Approaches",
          "slug": "/research/061-self-modifying-constraints-technical-approaches",
          "excerpt": "Exploring how AI systems can be governed through self-modifying constraints, bridging technical architecture with safety and oversight frameworks."
        }
      ]
    },
    {
      "id": "ai constraints",
      "title": "AI constraints",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Self-Modifying Constraints: Technical Approaches",
          "slug": "/research/061-self-modifying-constraints-technical-approaches",
          "excerpt": "Exploring how AI systems can be governed through self-modifying constraints, bridging technical architecture with safety and oversight frameworks."
        }
      ]
    },
    {
      "id": "ai-models",
      "title": "ai-models",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "The False Binary: Why 'Regulate Use, Not Models' Gets AI Governance Wrong",
          "slug": "/research/062-use-vs-models-false-binary",
          "excerpt": "A recent IEEE Spectrum article argues for use-based AI regulation over model-based approaches. This framing misses what actually works: layered, reflexive governance that addresses capabilities, deployment, and systemic accountability together."
        }
      ]
    },
    {
      "id": "use-based-regulation",
      "title": "use-based-regulation",
      "group": "governance",
      "val": 7,
      "articles": [
        {
          "title": "The False Binary: Why 'Regulate Use, Not Models' Gets AI Governance Wrong",
          "slug": "/research/062-use-vs-models-false-binary",
          "excerpt": "A recent IEEE Spectrum article argues for use-based AI regulation over model-based approaches. This framing misses what actually works: layered, reflexive governance that addresses capabilities, deployment, and systemic accountability together."
        }
      ]
    },
    {
      "id": "oversight",
      "title": "oversight",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "The Governance Paradox: When AI Systems Are Better Regulators Than Humans",
          "slug": "/research/063-governance-paradox",
          "excerpt": "AI systems may detect regulatory violations more reliably than human auditors. This creates tension with the principle of human oversight. What does meaningful oversight mean when humans are the bottleneck?"
        },
        {
          "title": "The Attention Economy Meets AI Governance: Designing for Distraction-Resistant Oversight",
          "slug": "/research/065-attention-economy-governance",
          "excerpt": "Human oversight assumes humans are paying attention. But modern interfaces are designed to maximize engagement, not careful evaluation. How do attention-economy dynamics undermine governance?"
        }
      ]
    },
    {
      "id": "human-in-the-loop",
      "title": "human-in-the-loop",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Governance Paradox: When AI Systems Are Better Regulators Than Humans",
          "slug": "/research/063-governance-paradox",
          "excerpt": "AI systems may detect regulatory violations more reliably than human auditors. This creates tension with the principle of human oversight. What does meaningful oversight mean when humans are the bottleneck?"
        }
      ]
    },
    {
      "id": "automation",
      "title": "automation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Governance Paradox: When AI Systems Are Better Regulators Than Humans",
          "slug": "/research/063-governance-paradox",
          "excerpt": "AI systems may detect regulatory violations more reliably than human auditors. This creates tension with the principle of human oversight. What does meaningful oversight mean when humans are the bottleneck?"
        }
      ]
    },
    {
      "id": "worldviews",
      "title": "worldviews",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Why AI Safety Researchers Disagree: A Taxonomy of Worldviews",
          "slug": "/research/064-ai-safety-worldviews",
          "excerpt": "The AI safety field appears fractured. Some focus on alignment, others on governance, others on misuse. This article maps the underlying worldview differences that produce divergent research agendas."
        }
      ]
    },
    {
      "id": "methodology",
      "title": "methodology",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Why AI Safety Researchers Disagree: A Taxonomy of Worldviews",
          "slug": "/research/064-ai-safety-worldviews",
          "excerpt": "The AI safety field appears fractured. Some focus on alignment, others on governance, others on misuse. This article maps the underlying worldview differences that produce divergent research agendas."
        }
      ]
    },
    {
      "id": "attention",
      "title": "attention",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Attention Economy Meets AI Governance: Designing for Distraction-Resistant Oversight",
          "slug": "/research/065-attention-economy-governance",
          "excerpt": "Human oversight assumes humans are paying attention. But modern interfaces are designed to maximize engagement, not careful evaluation. How do attention-economy dynamics undermine governance?"
        }
      ]
    },
    {
      "id": "interface-design",
      "title": "interface-design",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Attention Economy Meets AI Governance: Designing for Distraction-Resistant Oversight",
          "slug": "/research/065-attention-economy-governance",
          "excerpt": "Human oversight assumes humans are paying attention. But modern interfaces are designed to maximize engagement, not careful evaluation. How do attention-economy dynamics undermine governance?"
        }
      ]
    },
    {
      "id": "human-factors",
      "title": "human-factors",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Attention Economy Meets AI Governance: Designing for Distraction-Resistant Oversight",
          "slug": "/research/065-attention-economy-governance",
          "excerpt": "Human oversight assumes humans are paying attention. But modern interfaces are designed to maximize engagement, not careful evaluation. How do attention-economy dynamics undermine governance?"
        }
      ]
    },
    {
      "id": "internet-governance",
      "title": "internet-governance",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI Governance Without Borders: Lessons from Internet Governance History",
          "slug": "/research/066-internet-governance-lessons",
          "excerpt": "The internet faced similar challenges: global technology, national regulation, coordination problems. What worked? What failed? Lessons from ICANN, IETF, and content moderation for AI governance."
        }
      ]
    },
    {
      "id": "international",
      "title": "international",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "AI Governance Without Borders: Lessons from Internet Governance History",
          "slug": "/research/066-internet-governance-lessons",
          "excerpt": "The internet faced similar challenges: global technology, national regulation, coordination problems. What worked? What failed? Lessons from ICANN, IETF, and content moderation for AI governance."
        },
        {
          "title": "AI Governance in the Global South: Different Contexts, Different Priorities",
          "slug": "/research/076-global-south-governance",
          "excerpt": "Most AI governance discourse centers the US, EU, and China. But AI is a global technology. What does governance look like from Africa, Southeast Asia, or Latin America? Different contexts demand different priorities."
        }
      ]
    },
    {
      "id": "coordination",
      "title": "coordination",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "AI Governance Without Borders: Lessons from Internet Governance History",
          "slug": "/research/066-internet-governance-lessons",
          "excerpt": "The internet faced similar challenges: global technology, national regulation, coordination problems. What worked? What failed? Lessons from ICANN, IETF, and content moderation for AI governance."
        },
        {
          "title": "Governance Fragmentation: Too Many Frameworks, Not Enough Coherence",
          "slug": "/research/082-governance-fragmentation",
          "excerpt": "The AI governance landscape is crowded with frameworks: principles, guidelines, regulations, and proposals proliferate. This abundance creates incoherence. How do we synthesize without premature standardization?"
        }
      ]
    },
    {
      "id": "multistakeholder",
      "title": "multistakeholder",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI Governance Without Borders: Lessons from Internet Governance History",
          "slug": "/research/066-internet-governance-lessons",
          "excerpt": "The internet faced similar challenges: global technology, national regulation, coordination problems. What worked? What failed? Lessons from ICANN, IETF, and content moderation for AI governance."
        }
      ]
    },
    {
      "id": "history",
      "title": "history",
      "group": "concept",
      "val": 9,
      "articles": [
        {
          "title": "AI Governance Without Borders: Lessons from Internet Governance History",
          "slug": "/research/066-internet-governance-lessons",
          "excerpt": "The internet faced similar challenges: global technology, national regulation, coordination problems. What worked? What failed? Lessons from ICANN, IETF, and content moderation for AI governance."
        },
        {
          "title": "When Experts Were Wrong: Epistemic Humility in AI Predictions",
          "slug": "/research/074-when-experts-wrong",
          "excerpt": "AI experts have a poor track record of prediction. Timelines, capabilities, and societal impacts have been consistently misjudged. What should this teach us about confidence in current claims?"
        }
      ]
    },
    {
      "id": "collective-action",
      "title": "collective-action",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Game Theory of AI Disclosure: When Transparency is a Prisoner's Dilemma",
          "slug": "/research/067-game-theory-disclosure",
          "excerpt": "Companies face a collective action problem: all would benefit from industry-wide transparency, but unilateral disclosure may harm competitive position. How do we change the payoff structure?"
        }
      ]
    },
    {
      "id": "failures",
      "title": "failures",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Reflexive AI in Practice: A Case Study of Constraint Failures",
          "slug": "/research/068-constraint-failure-cases",
          "excerpt": "Rather than theoretical, this examines documented cases where AI systems violated their stated constraints. What went wrong? Were the constraints poorly specified, not enforced, or gamed?"
        }
      ]
    },
    {
      "id": "case-studies",
      "title": "case-studies",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Reflexive AI in Practice: A Case Study of Constraint Failures",
          "slug": "/research/068-constraint-failure-cases",
          "excerpt": "Rather than theoretical, this examines documented cases where AI systems violated their stated constraints. What went wrong? Were the constraints poorly specified, not enforced, or gamed?"
        }
      ]
    },
    {
      "id": "lessons",
      "title": "lessons",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Reflexive AI in Practice: A Case Study of Constraint Failures",
          "slug": "/research/068-constraint-failure-cases",
          "excerpt": "Rather than theoretical, this examines documented cases where AI systems violated their stated constraints. What went wrong? Were the constraints poorly specified, not enforced, or gamed?"
        }
      ]
    },
    {
      "id": "semantics",
      "title": "semantics",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Semantic Gap Problem: Why Natural Language Constraints Fail",
          "slug": "/research/069-semantic-gap-problem",
          "excerpt": "We specify AI constraints in natural language, but models operate on statistical patterns. This disconnect means constraints may not do what we think. What are the technical approaches to bridging this gap?"
        }
      ]
    },
    {
      "id": "natural-language",
      "title": "natural-language",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Semantic Gap Problem: Why Natural Language Constraints Fail",
          "slug": "/research/069-semantic-gap-problem",
          "excerpt": "We specify AI constraints in natural language, but models operate on statistical patterns. This disconnect means constraints may not do what we think. What are the technical approaches to bridging this gap?"
        }
      ]
    },
    {
      "id": "formal-verification",
      "title": "formal-verification",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Semantic Gap Problem: Why Natural Language Constraints Fail",
          "slug": "/research/069-semantic-gap-problem",
          "excerpt": "We specify AI constraints in natural language, but models operate on statistical patterns. This disconnect means constraints may not do what we think. What are the technical approaches to bridging this gap?"
        }
      ]
    },
    {
      "id": "democracy",
      "title": "democracy",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Who Decides What AI Should Refuse? The Democratic Deficit in Constraint Design",
          "slug": "/research/070-democratic-deficit-constraints",
          "excerpt": "AI refusals encode value judgments. Currently, small teams at AI labs make these decisions. Is this legitimate? Exploring the democratic deficit in AI constraint design and possible alternatives."
        }
      ]
    },
    {
      "id": "legitimacy",
      "title": "legitimacy",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Who Decides What AI Should Refuse? The Democratic Deficit in Constraint Design",
          "slug": "/research/070-democratic-deficit-constraints",
          "excerpt": "AI refusals encode value judgments. Currently, small teams at AI labs make these decisions. Is this legitimate? Exploring the democratic deficit in AI constraint design and possible alternatives."
        }
      ]
    },
    {
      "id": "refusals",
      "title": "refusals",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Who Decides What AI Should Refuse? The Democratic Deficit in Constraint Design",
          "slug": "/research/070-democratic-deficit-constraints",
          "excerpt": "AI refusals encode value judgments. Currently, small teams at AI labs make these decisions. Is this legitimate? Exploring the democratic deficit in AI constraint design and possible alternatives."
        }
      ]
    },
    {
      "id": "participation",
      "title": "participation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Who Decides What AI Should Refuse? The Democratic Deficit in Constraint Design",
          "slug": "/research/070-democratic-deficit-constraints",
          "excerpt": "AI refusals encode value judgments. Currently, small teams at AI labs make these decisions. Is this legitimate? Exploring the democratic deficit in AI constraint design and possible alternatives."
        }
      ]
    },
    {
      "id": "law",
      "title": "law",
      "group": "governance",
      "val": 7,
      "articles": [
        {
          "title": "The Liability Vacuum: When AI Harms Fall Between Legal Categories",
          "slug": "/research/071-liability-vacuum",
          "excerpt": "AI harms often don't fit existing legal frameworks: not quite product liability, not quite professional malpractice, not quite negligence. This article maps the specific gaps and proposes targeted fixes."
        }
      ]
    },
    {
      "id": "harms",
      "title": "harms",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Liability Vacuum: When AI Harms Fall Between Legal Categories",
          "slug": "/research/071-liability-vacuum",
          "excerpt": "AI harms often don't fit existing legal frameworks: not quite product liability, not quite professional malpractice, not quite negligence. This article maps the specific gaps and proposes targeted fixes."
        }
      ]
    },
    {
      "id": "legal-frameworks",
      "title": "legal-frameworks",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Liability Vacuum: When AI Harms Fall Between Legal Categories",
          "slug": "/research/071-liability-vacuum",
          "excerpt": "AI harms often don't fit existing legal frameworks: not quite product liability, not quite professional malpractice, not quite negligence. This article maps the specific gaps and proposes targeted fixes."
        }
      ]
    },
    {
      "id": "simulation",
      "title": "simulation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Simulating Governance: Using AI to Stress-Test AI Regulations",
          "slug": "/research/072-simulating-governance",
          "excerpt": "Regulations are policies. Policies can be simulated. What if we used AI systems to model the effects of proposed regulations before implementation, identifying loopholes and unintended consequences?"
        }
      ]
    },
    {
      "id": "policy-testing",
      "title": "policy-testing",
      "group": "governance",
      "val": 7,
      "articles": [
        {
          "title": "Simulating Governance: Using AI to Stress-Test AI Regulations",
          "slug": "/research/072-simulating-governance",
          "excerpt": "Regulations are policies. Policies can be simulated. What if we used AI systems to model the effects of proposed regulations before implementation, identifying loopholes and unintended consequences?"
        }
      ]
    },
    {
      "id": "modeling",
      "title": "modeling",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "Simulating Governance: Using AI to Stress-Test AI Regulations",
          "slug": "/research/072-simulating-governance",
          "excerpt": "Regulations are policies. Policies can be simulated. What if we used AI systems to model the effects of proposed regulations before implementation, identifying loopholes and unintended consequences?"
        }
      ]
    },
    {
      "id": "unintended-consequences",
      "title": "unintended-consequences",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Simulating Governance: Using AI to Stress-Test AI Regulations",
          "slug": "/research/072-simulating-governance",
          "excerpt": "Regulations are policies. Policies can be simulated. What if we used AI systems to model the effects of proposed regulations before implementation, identifying loopholes and unintended consequences?"
        }
      ]
    },
    {
      "id": "researcher-wellbeing",
      "title": "researcher-wellbeing",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "The Burnout Problem: Sustainability in AI Safety Research",
          "slug": "/research/073-burnout-problem",
          "excerpt": "AI safety research operates under a 'race against catastrophe' framing. This urgency culture may undermine the very work it motivates. What does sustainable safety research look like?"
        }
      ]
    },
    {
      "id": "sustainability",
      "title": "sustainability",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Burnout Problem: Sustainability in AI Safety Research",
          "slug": "/research/073-burnout-problem",
          "excerpt": "AI safety research operates under a 'race against catastrophe' framing. This urgency culture may undermine the very work it motivates. What does sustainable safety research look like?"
        }
      ]
    },
    {
      "id": "culture",
      "title": "culture",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Burnout Problem: Sustainability in AI Safety Research",
          "slug": "/research/073-burnout-problem",
          "excerpt": "AI safety research operates under a 'race against catastrophe' framing. This urgency culture may undermine the very work it motivates. What does sustainable safety research look like?"
        }
      ]
    },
    {
      "id": "institutions",
      "title": "institutions",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Burnout Problem: Sustainability in AI Safety Research",
          "slug": "/research/073-burnout-problem",
          "excerpt": "AI safety research operates under a 'race against catastrophe' framing. This urgency culture may undermine the very work it motivates. What does sustainable safety research look like?"
        }
      ]
    },
    {
      "id": "meta",
      "title": "meta",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Burnout Problem: Sustainability in AI Safety Research",
          "slug": "/research/073-burnout-problem",
          "excerpt": "AI safety research operates under a 'race against catastrophe' framing. This urgency culture may undermine the very work it motivates. What does sustainable safety research look like?"
        }
      ]
    },
    {
      "id": "predictions",
      "title": "predictions",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "When Experts Were Wrong: Epistemic Humility in AI Predictions",
          "slug": "/research/074-when-experts-wrong",
          "excerpt": "AI experts have a poor track record of prediction. Timelines, capabilities, and societal impacts have been consistently misjudged. What should this teach us about confidence in current claims?"
        }
      ]
    },
    {
      "id": "epistemic-humility",
      "title": "epistemic-humility",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "When Experts Were Wrong: Epistemic Humility in AI Predictions",
          "slug": "/research/074-when-experts-wrong",
          "excerpt": "AI experts have a poor track record of prediction. Timelines, capabilities, and societal impacts have been consistently misjudged. What should this teach us about confidence in current claims?"
        }
      ]
    },
    {
      "id": "expert-judgment",
      "title": "expert-judgment",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "When Experts Were Wrong: Epistemic Humility in AI Predictions",
          "slug": "/research/074-when-experts-wrong",
          "excerpt": "AI experts have a poor track record of prediction. Timelines, capabilities, and societal impacts have been consistently misjudged. What should this teach us about confidence in current claims?"
        }
      ]
    },
    {
      "id": "market-structure",
      "title": "market-structure",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Small Actor Problem: How AI Regulation Shapes Market Structure",
          "slug": "/research/075-small-actor-problem",
          "excerpt": "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
        }
      ]
    },
    {
      "id": "small-actors",
      "title": "small-actors",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Small Actor Problem: How AI Regulation Shapes Market Structure",
          "slug": "/research/075-small-actor-problem",
          "excerpt": "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
        }
      ]
    },
    {
      "id": "competition",
      "title": "competition",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Small Actor Problem: How AI Regulation Shapes Market Structure",
          "slug": "/research/075-small-actor-problem",
          "excerpt": "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
        }
      ]
    },
    {
      "id": "power-concentration",
      "title": "power-concentration",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Small Actor Problem: How AI Regulation Shapes Market Structure",
          "slug": "/research/075-small-actor-problem",
          "excerpt": "AI regulations often benefit large incumbents who can afford compliance. How can governance avoid creating barriers that entrench power and lock out smaller innovators?"
        }
      ]
    },
    {
      "id": "global-south",
      "title": "global-south",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI Governance in the Global South: Different Contexts, Different Priorities",
          "slug": "/research/076-global-south-governance",
          "excerpt": "Most AI governance discourse centers the US, EU, and China. But AI is a global technology. What does governance look like from Africa, Southeast Asia, or Latin America? Different contexts demand different priorities."
        }
      ]
    },
    {
      "id": "development",
      "title": "development",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "AI Governance in the Global South: Different Contexts, Different Priorities",
          "slug": "/research/076-global-south-governance",
          "excerpt": "Most AI governance discourse centers the US, EU, and China. But AI is a global technology. What does governance look like from Africa, Southeast Asia, or Latin America? Different contexts demand different priorities."
        },
        {
          "title": "The Speed-Safety Tradeoff: Making the Implicit Explicit",
          "slug": "/research/077-speed-safety-tradeoff",
          "excerpt": "Move fast and break things' vs 'go slow and be careful.' AI development constantly navigates this tension, but rarely discusses it explicitly. What does the tradeoff actually involve, and how should different actors balance it?"
        },
        {
          "title": "AI and Children: Distinct Moral and Governance Considerations",
          "slug": "/research/080-ai-and-children",
          "excerpt": "Children are not small adults. AI systems designed for adult contexts may harm children in specific ways. What governance considerations are distinct when AI systems interact with minors?"
        }
      ]
    },
    {
      "id": "context",
      "title": "context",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI Governance in the Global South: Different Contexts, Different Priorities",
          "slug": "/research/076-global-south-governance",
          "excerpt": "Most AI governance discourse centers the US, EU, and China. But AI is a global technology. What does governance look like from Africa, Southeast Asia, or Latin America? Different contexts demand different priorities."
        }
      ]
    },
    {
      "id": "priorities",
      "title": "priorities",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI Governance in the Global South: Different Contexts, Different Priorities",
          "slug": "/research/076-global-south-governance",
          "excerpt": "Most AI governance discourse centers the US, EU, and China. But AI is a global technology. What does governance look like from Africa, Southeast Asia, or Latin America? Different contexts demand different priorities."
        }
      ]
    },
    {
      "id": "speed",
      "title": "speed",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Speed-Safety Tradeoff: Making the Implicit Explicit",
          "slug": "/research/077-speed-safety-tradeoff",
          "excerpt": "Move fast and break things' vs 'go slow and be careful.' AI development constantly navigates this tension, but rarely discusses it explicitly. What does the tradeoff actually involve, and how should different actors balance it?"
        }
      ]
    },
    {
      "id": "tradeoffs",
      "title": "tradeoffs",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Speed-Safety Tradeoff: Making the Implicit Explicit",
          "slug": "/research/077-speed-safety-tradeoff",
          "excerpt": "Move fast and break things' vs 'go slow and be careful.' AI development constantly navigates this tension, but rarely discusses it explicitly. What does the tradeoff actually involve, and how should different actors balance it?"
        }
      ]
    },
    {
      "id": "decision-making",
      "title": "decision-making",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Speed-Safety Tradeoff: Making the Implicit Explicit",
          "slug": "/research/077-speed-safety-tradeoff",
          "excerpt": "Move fast and break things' vs 'go slow and be careful.' AI development constantly navigates this tension, but rarely discusses it explicitly. What does the tradeoff actually involve, and how should different actors balance it?"
        }
      ]
    },
    {
      "id": "economics",
      "title": "economics",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Economics of AI Safety: Who Pays and Why It Matters",
          "slug": "/research/078-economics-ai-safety",
          "excerpt": "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
        }
      ]
    },
    {
      "id": "funding",
      "title": "funding",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Economics of AI Safety: Who Pays and Why It Matters",
          "slug": "/research/078-economics-ai-safety",
          "excerpt": "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
        }
      ]
    },
    {
      "id": "investment",
      "title": "investment",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Economics of AI Safety: Who Pays and Why It Matters",
          "slug": "/research/078-economics-ai-safety",
          "excerpt": "Safety costs money. Who bears those costs shapes what safety work gets done. This article examines the economics of AI safety: funding structures, incentive misalignments, and what economic systems would adequately support safety."
        }
      ]
    },
    {
      "id": "users",
      "title": "users",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Trust Calibration: Teaching Users When to Believe AI",
          "slug": "/research/079-trust-calibration",
          "excerpt": "Most AI governance focuses on developers and deployers. But users make trust decisions constantly: should I believe this output? Follow this recommendation? This article explores user-facing trust calibration."
        }
      ]
    },
    {
      "id": "calibration",
      "title": "calibration",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Trust Calibration: Teaching Users When to Believe AI",
          "slug": "/research/079-trust-calibration",
          "excerpt": "Most AI governance focuses on developers and deployers. But users make trust decisions constantly: should I believe this output? Follow this recommendation? This article explores user-facing trust calibration."
        }
      ]
    },
    {
      "id": "reliability",
      "title": "reliability",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Trust Calibration: Teaching Users When to Believe AI",
          "slug": "/research/079-trust-calibration",
          "excerpt": "Most AI governance focuses on developers and deployers. But users make trust decisions constantly: should I believe this output? Follow this recommendation? This article explores user-facing trust calibration."
        }
      ]
    },
    {
      "id": "education",
      "title": "education",
      "group": "concept",
      "val": 11,
      "articles": [
        {
          "title": "Trust Calibration: Teaching Users When to Believe AI",
          "slug": "/research/079-trust-calibration",
          "excerpt": "Most AI governance focuses on developers and deployers. But users make trust decisions constantly: should I believe this output? Follow this recommendation? This article explores user-facing trust calibration."
        },
        {
          "title": "AI and Children: Distinct Moral and Governance Considerations",
          "slug": "/research/080-ai-and-children",
          "excerpt": "Children are not small adults. AI systems designed for adult contexts may harm children in specific ways. What governance considerations are distinct when AI systems interact with minors?"
        },
        {
          "title": "AI in Education: Personalization vs. Privacy",
          "slug": "/research/085-ai-in-education-personalization-vs-privacy",
          "excerpt": "This article examines the tension between personalization and privacy in AI-driven educational tools, exploring governance frameworks, technological solutions, and ethical trade-offs."
        }
      ]
    },
    {
      "id": "children",
      "title": "children",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI and Children: Distinct Moral and Governance Considerations",
          "slug": "/research/080-ai-and-children",
          "excerpt": "Children are not small adults. AI systems designed for adult contexts may harm children in specific ways. What governance considerations are distinct when AI systems interact with minors?"
        }
      ]
    },
    {
      "id": "minors",
      "title": "minors",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI and Children: Distinct Moral and Governance Considerations",
          "slug": "/research/080-ai-and-children",
          "excerpt": "Children are not small adults. AI systems designed for adult contexts may harm children in specific ways. What governance considerations are distinct when AI systems interact with minors?"
        }
      ]
    },
    {
      "id": "protection",
      "title": "protection",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI and Children: Distinct Moral and Governance Considerations",
          "slug": "/research/080-ai-and-children",
          "excerpt": "Children are not small adults. AI systems designed for adult contexts may harm children in specific ways. What governance considerations are distinct when AI systems interact with minors?"
        }
      ]
    },
    {
      "id": "psychology",
      "title": "psychology",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Emotional Labor of AI: Psychological Impacts at Scale",
          "slug": "/research/081-emotional-labor-ai",
          "excerpt": "Millions form emotional connections with AI systems: companions, assistants, therapeutic tools. What are the psychological effects? What responsibilities do developers have for emotional wellbeing?"
        }
      ]
    },
    {
      "id": "emotions",
      "title": "emotions",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Emotional Labor of AI: Psychological Impacts at Scale",
          "slug": "/research/081-emotional-labor-ai",
          "excerpt": "Millions form emotional connections with AI systems: companions, assistants, therapeutic tools. What are the psychological effects? What responsibilities do developers have for emotional wellbeing?"
        }
      ]
    },
    {
      "id": "relationships",
      "title": "relationships",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Emotional Labor of AI: Psychological Impacts at Scale",
          "slug": "/research/081-emotional-labor-ai",
          "excerpt": "Millions form emotional connections with AI systems: companions, assistants, therapeutic tools. What are the psychological effects? What responsibilities do developers have for emotional wellbeing?"
        }
      ]
    },
    {
      "id": "wellbeing",
      "title": "wellbeing",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Emotional Labor of AI: Psychological Impacts at Scale",
          "slug": "/research/081-emotional-labor-ai",
          "excerpt": "Millions form emotional connections with AI systems: companions, assistants, therapeutic tools. What are the psychological effects? What responsibilities do developers have for emotional wellbeing?"
        }
      ]
    },
    {
      "id": "companionship",
      "title": "companionship",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "The Emotional Labor of AI: Psychological Impacts at Scale",
          "slug": "/research/081-emotional-labor-ai",
          "excerpt": "Millions form emotional connections with AI systems: companions, assistants, therapeutic tools. What are the psychological effects? What responsibilities do developers have for emotional wellbeing?"
        }
      ]
    },
    {
      "id": "fragmentation",
      "title": "fragmentation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Governance Fragmentation: Too Many Frameworks, Not Enough Coherence",
          "slug": "/research/082-governance-fragmentation",
          "excerpt": "The AI governance landscape is crowded with frameworks: principles, guidelines, regulations, and proposals proliferate. This abundance creates incoherence. How do we synthesize without premature standardization?"
        }
      ]
    },
    {
      "id": "frameworks",
      "title": "frameworks",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Governance Fragmentation: Too Many Frameworks, Not Enough Coherence",
          "slug": "/research/082-governance-fragmentation",
          "excerpt": "The AI governance landscape is crowded with frameworks: principles, guidelines, regulations, and proposals proliferate. This abundance creates incoherence. How do we synthesize without premature standardization?"
        }
      ]
    },
    {
      "id": "synthesis",
      "title": "synthesis",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Governance Fragmentation: Too Many Frameworks, Not Enough Coherence",
          "slug": "/research/082-governance-fragmentation",
          "excerpt": "The AI governance landscape is crowded with frameworks: principles, guidelines, regulations, and proposals proliferate. This abundance creates incoherence. How do we synthesize without premature standardization?"
        }
      ]
    },
    {
      "id": "research",
      "title": "research",
      "group": "technical",
      "val": 9,
      "articles": [
        {
          "title": "AI in Agriculture: Data Governance",
          "slug": "/research/083-ai-in-agriculture-data-governance",
          "excerpt": "Exploring the governance challenges of data use in agricultural AI systems, with a focus on ethical, regulatory, and technical considerations for sustainable and equitable outcomes."
        },
        {
          "title": "Multi-Agent Coordination Failures",
          "slug": "/research/088-multi-agent-coordination-failures",
          "excerpt": "Exploring the dynamics, risks, and governance challenges of coordination failures among AI systems in multi-agent environments."
        }
      ]
    },
    {
      "id": "ai-focused",
      "title": "ai-focused",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI in Agriculture: Data Governance",
          "slug": "/research/083-ai-in-agriculture-data-governance",
          "excerpt": "Exploring the governance challenges of data use in agricultural AI systems, with a focus on ethical, regulatory, and technical considerations for sustainable and equitable outcomes."
        }
      ]
    },
    {
      "id": "validation",
      "title": "validation",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI in Climate Modeling: Validation Standards",
          "slug": "/research/084-ai-in-climate-modeling-validation-standards",
          "excerpt": "Establishing rigorous validation standards for AI-driven climate models is essential to ensure their reliability, transparency, and utility in addressing global environmental challenges."
        }
      ]
    },
    {
      "id": "climate modeling",
      "title": "climate modeling",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "AI in Climate Modeling: Validation Standards",
          "slug": "/research/084-ai-in-climate-modeling-validation-standards",
          "excerpt": "Establishing rigorous validation standards for AI-driven climate models is essential to ensure their reliability, transparency, and utility in addressing global environmental challenges."
        }
      ]
    },
    {
      "id": "personalization",
      "title": "personalization",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "AI in Education: Personalization vs. Privacy",
          "slug": "/research/085-ai-in-education-personalization-vs-privacy",
          "excerpt": "This article examines the tension between personalization and privacy in AI-driven educational tools, exploring governance frameworks, technological solutions, and ethical trade-offs."
        }
      ]
    },
    {
      "id": "data-protection",
      "title": "data-protection",
      "group": "technical",
      "val": 7,
      "articles": [
        {
          "title": "AI in Education: Personalization vs. Privacy",
          "slug": "/research/085-ai-in-education-personalization-vs-privacy",
          "excerpt": "This article examines the tension between personalization and privacy in AI-driven educational tools, exploring governance frameworks, technological solutions, and ethical trade-offs."
        }
      ]
    },
    {
      "id": "agi",
      "title": "AGI",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Governance for Artificial General Intelligence",
          "slug": "/research/086-governance-for-artificial-general-intelligence",
          "excerpt": "Examining the unique challenges and frameworks required to govern Artificial General Intelligence (AGI), with a focus on safety, accountability, and the role of reflexive AI in regulatory compliance."
        }
      ]
    },
    {
      "id": "multi-agent systems",
      "title": "multi-agent systems",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Multi-Agent Coordination Failures",
          "slug": "/research/088-multi-agent-coordination-failures",
          "excerpt": "Exploring the dynamics, risks, and governance challenges of coordination failures among AI systems in multi-agent environments."
        }
      ]
    },
    {
      "id": "ai coordination",
      "title": "AI coordination",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Multi-Agent Coordination Failures",
          "slug": "/research/088-multi-agent-coordination-failures",
          "excerpt": "Exploring the dynamics, risks, and governance challenges of coordination failures among AI systems in multi-agent environments."
        }
      ]
    },
    {
      "id": "failure modes",
      "title": "failure modes",
      "group": "concept",
      "val": 7,
      "articles": [
        {
          "title": "Multi-Agent Coordination Failures",
          "slug": "/research/088-multi-agent-coordination-failures",
          "excerpt": "Exploring the dynamics, risks, and governance challenges of coordination failures among AI systems in multi-agent environments."
        }
      ]
    }
  ],
  "links": [
    {
      "source": "disclosure",
      "target": "regulation",
      "value": 2
    },
    {
      "source": "disclosure",
      "target": "eu-ai-act",
      "value": 1
    },
    {
      "source": "disclosure",
      "target": "proportionality",
      "value": 1
    },
    {
      "source": "disclosure",
      "target": "transparency",
      "value": 3
    },
    {
      "source": "eu-ai-act",
      "target": "regulation",
      "value": 2
    },
    {
      "source": "proportionality",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "transparency",
      "value": 2
    },
    {
      "source": "eu-ai-act",
      "target": "proportionality",
      "value": 1
    },
    {
      "source": "eu-ai-act",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "proportionality",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "open-source",
      "target": "safety",
      "value": 1
    },
    {
      "source": "open-source",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "access-control",
      "target": "open-source",
      "value": 1
    },
    {
      "source": "dual-use",
      "target": "open-source",
      "value": 1
    },
    {
      "source": "safety",
      "target": "transparency",
      "value": 5
    },
    {
      "source": "access-control",
      "target": "safety",
      "value": 1
    },
    {
      "source": "dual-use",
      "target": "safety",
      "value": 2
    },
    {
      "source": "access-control",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "dual-use",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "access-control",
      "target": "dual-use",
      "value": 1
    },
    {
      "source": "json-ld",
      "target": "machine-readable",
      "value": 1
    },
    {
      "source": "json-ld",
      "target": "standards",
      "value": 1
    },
    {
      "source": "interoperability",
      "target": "json-ld",
      "value": 1
    },
    {
      "source": "agents",
      "target": "json-ld",
      "value": 1
    },
    {
      "source": "machine-readable",
      "target": "standards",
      "value": 1
    },
    {
      "source": "interoperability",
      "target": "machine-readable",
      "value": 1
    },
    {
      "source": "agents",
      "target": "machine-readable",
      "value": 2
    },
    {
      "source": "interoperability",
      "target": "standards",
      "value": 2
    },
    {
      "source": "agents",
      "target": "standards",
      "value": 1
    },
    {
      "source": "agents",
      "target": "interoperability",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "safety",
      "value": 2
    },
    {
      "source": "red-lines",
      "target": "safety",
      "value": 2
    },
    {
      "source": "safety",
      "target": "taxonomy",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "safety",
      "value": 2
    },
    {
      "source": "constraints",
      "target": "red-lines",
      "value": 2
    },
    {
      "source": "constraints",
      "target": "taxonomy",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "constraints",
      "value": 1
    },
    {
      "source": "red-lines",
      "target": "taxonomy",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "red-lines",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "taxonomy",
      "value": 1
    },
    {
      "source": "policy",
      "target": "regulation",
      "value": 5
    },
    {
      "source": "guide",
      "target": "policy",
      "value": 3
    },
    {
      "source": "disclosure",
      "target": "policy",
      "value": 1
    },
    {
      "source": "policy",
      "target": "transparency",
      "value": 3
    },
    {
      "source": "guide",
      "target": "regulation",
      "value": 3
    },
    {
      "source": "disclosure",
      "target": "guide",
      "value": 1
    },
    {
      "source": "guide",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "institutional-design",
      "value": 2
    },
    {
      "source": "auditing",
      "target": "incentives",
      "value": 2
    },
    {
      "source": "institutional-design",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "institutional-design",
      "value": 2
    },
    {
      "source": "consent",
      "target": "ethics",
      "value": 1
    },
    {
      "source": "ethics",
      "target": "legal-theory",
      "value": 1
    },
    {
      "source": "data-rights",
      "target": "ethics",
      "value": 1
    },
    {
      "source": "consent",
      "target": "legal-theory",
      "value": 1
    },
    {
      "source": "consent",
      "target": "data-rights",
      "value": 1
    },
    {
      "source": "data-rights",
      "target": "legal-theory",
      "value": 1
    },
    {
      "source": "arbitrage",
      "target": "jurisdiction",
      "value": 1
    },
    {
      "source": "arbitrage",
      "target": "deployment",
      "value": 1
    },
    {
      "source": "arbitrage",
      "target": "enforcement",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "jurisdiction",
      "value": 1
    },
    {
      "source": "enforcement",
      "target": "jurisdiction",
      "value": 3
    },
    {
      "source": "deployment",
      "target": "enforcement",
      "value": 1
    },
    {
      "source": "capability-elicitation",
      "target": "safety",
      "value": 3
    },
    {
      "source": "capability-elicitation",
      "target": "overhang",
      "value": 1
    },
    {
      "source": "capability-elicitation",
      "target": "risk-assessment",
      "value": 2
    },
    {
      "source": "overhang",
      "target": "safety",
      "value": 1
    },
    {
      "source": "risk-assessment",
      "target": "safety",
      "value": 4
    },
    {
      "source": "overhang",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "game-theory",
      "value": 1
    },
    {
      "source": "game-theory",
      "target": "incentives",
      "value": 2
    },
    {
      "source": "game-theory",
      "target": "institutional-design",
      "value": 1
    },
    {
      "source": "intent-recognition",
      "target": "misuse-detection",
      "value": 1
    },
    {
      "source": "intent-recognition",
      "target": "reflexive-monitoring",
      "value": 1
    },
    {
      "source": "misuse-detection",
      "target": "reflexive-monitoring",
      "value": 1
    },
    {
      "source": "provenance",
      "target": "watermarking",
      "value": 2
    },
    {
      "source": "cryptography",
      "target": "provenance",
      "value": 1
    },
    {
      "source": "c2pa",
      "target": "provenance",
      "value": 1
    },
    {
      "source": "cryptography",
      "target": "watermarking",
      "value": 1
    },
    {
      "source": "c2pa",
      "target": "watermarking",
      "value": 1
    },
    {
      "source": "c2pa",
      "target": "cryptography",
      "value": 1
    },
    {
      "source": "limits",
      "target": "theory",
      "value": 1
    },
    {
      "source": "safety",
      "target": "theory",
      "value": 2
    },
    {
      "source": "paradox",
      "target": "theory",
      "value": 1
    },
    {
      "source": "limits",
      "target": "safety",
      "value": 1
    },
    {
      "source": "limits",
      "target": "paradox",
      "value": 1
    },
    {
      "source": "paradox",
      "target": "safety",
      "value": 1
    },
    {
      "source": "reporting",
      "target": "whistleblowing",
      "value": 2
    },
    {
      "source": "api-design",
      "target": "whistleblowing",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "whistleblowing",
      "value": 1
    },
    {
      "source": "api-design",
      "target": "reporting",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "reporting",
      "value": 1
    },
    {
      "source": "api-design",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "game-theory",
      "target": "multi-agent-systems",
      "value": 1
    },
    {
      "source": "emergent-behavior",
      "target": "multi-agent-systems",
      "value": 1
    },
    {
      "source": "evolution",
      "target": "multi-agent-systems",
      "value": 1
    },
    {
      "source": "emergent-behavior",
      "target": "game-theory",
      "value": 1
    },
    {
      "source": "evolution",
      "target": "game-theory",
      "value": 1
    },
    {
      "source": "emergent-behavior",
      "target": "evolution",
      "value": 1
    },
    {
      "source": "alignment",
      "target": "safety",
      "value": 3
    },
    {
      "source": "alignment",
      "target": "ethics",
      "value": 3
    },
    {
      "source": "alignment",
      "target": "guide",
      "value": 2
    },
    {
      "source": "alignment",
      "target": "theory",
      "value": 2
    },
    {
      "source": "ethics",
      "target": "safety",
      "value": 3
    },
    {
      "source": "guide",
      "target": "safety",
      "value": 2
    },
    {
      "source": "ethics",
      "target": "guide",
      "value": 2
    },
    {
      "source": "ethics",
      "target": "theory",
      "value": 3
    },
    {
      "source": "guide",
      "target": "theory",
      "value": 1
    },
    {
      "source": "governance",
      "target": "guide",
      "value": 2
    },
    {
      "source": "governance",
      "target": "policy",
      "value": 5
    },
    {
      "source": "governance",
      "target": "regulation",
      "value": 5
    },
    {
      "source": "ethics",
      "target": "governance",
      "value": 5
    },
    {
      "source": "ethics",
      "target": "policy",
      "value": 3
    },
    {
      "source": "ethics",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "enforcement",
      "target": "regulation",
      "value": 4
    },
    {
      "source": "jurisdiction",
      "target": "regulation",
      "value": 3
    },
    {
      "source": "enforcement",
      "target": "policy",
      "value": 2
    },
    {
      "source": "jurisdiction",
      "target": "policy",
      "value": 3
    },
    {
      "source": "enforcement",
      "target": "governance",
      "value": 4
    },
    {
      "source": "governance",
      "target": "jurisdiction",
      "value": 3
    },
    {
      "source": "eu-ai-act",
      "target": "policy",
      "value": 1
    },
    {
      "source": "eu-ai-act",
      "target": "governance",
      "value": 1
    },
    {
      "source": "enforcement",
      "target": "eu-ai-act",
      "value": 1
    },
    {
      "source": "eu-ai-act",
      "target": "jurisdiction",
      "value": 1
    },
    {
      "source": "legal-theory",
      "target": "liability",
      "value": 1
    },
    {
      "source": "governance",
      "target": "liability",
      "value": 2
    },
    {
      "source": "liability",
      "target": "regulation",
      "value": 2
    },
    {
      "source": "enforcement",
      "target": "liability",
      "value": 1
    },
    {
      "source": "governance",
      "target": "legal-theory",
      "value": 1
    },
    {
      "source": "legal-theory",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "enforcement",
      "target": "legal-theory",
      "value": 1
    },
    {
      "source": "incident-reporting",
      "target": "safety",
      "value": 1
    },
    {
      "source": "incident-reporting",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "incident-reporting",
      "value": 1
    },
    {
      "source": "incident-reporting",
      "target": "standards",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "safety",
      "value": 2
    },
    {
      "source": "safety",
      "target": "standards",
      "value": 3
    },
    {
      "source": "auditing",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "standards",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "standards",
      "value": 1
    },
    {
      "source": "transparency",
      "target": "whistleblowing",
      "value": 1
    },
    {
      "source": "safety",
      "target": "whistleblowing",
      "value": 1
    },
    {
      "source": "governance",
      "target": "whistleblowing",
      "value": 1
    },
    {
      "source": "governance",
      "target": "transparency",
      "value": 5
    },
    {
      "source": "reporting",
      "target": "transparency",
      "value": 2
    },
    {
      "source": "governance",
      "target": "safety",
      "value": 5
    },
    {
      "source": "reporting",
      "target": "safety",
      "value": 1
    },
    {
      "source": "governance",
      "target": "reporting",
      "value": 1
    },
    {
      "source": "compute",
      "target": "governance",
      "value": 1
    },
    {
      "source": "compute",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "compute",
      "target": "safety",
      "value": 1
    },
    {
      "source": "compute",
      "target": "enforcement",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "safety",
      "value": 4
    },
    {
      "source": "enforcement",
      "target": "safety",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "capability-elicitation",
      "value": 1
    },
    {
      "source": "capability-elicitation",
      "target": "deployment",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "safety",
      "value": 3
    },
    {
      "source": "auditing",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "auditing",
      "target": "deployment",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "risk-assessment",
      "value": 2
    },
    {
      "source": "agents",
      "target": "constraints",
      "value": 2
    },
    {
      "source": "constraints",
      "target": "ethics",
      "value": 4
    },
    {
      "source": "agents",
      "target": "safety",
      "value": 1
    },
    {
      "source": "agents",
      "target": "red-lines",
      "value": 1
    },
    {
      "source": "ethics",
      "target": "red-lines",
      "value": 1
    },
    {
      "source": "agents",
      "target": "ethics",
      "value": 2
    },
    {
      "source": "constraints",
      "target": "transparency",
      "value": 3
    },
    {
      "source": "agents",
      "target": "transparency",
      "value": 2
    },
    {
      "source": "machine-readable",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "ethics",
      "target": "transparency",
      "value": 5
    },
    {
      "source": "constraints",
      "target": "machine-readable",
      "value": 1
    },
    {
      "source": "ethics",
      "target": "machine-readable",
      "value": 1
    },
    {
      "source": "transparency",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "transparency",
      "target": "trust",
      "value": 1
    },
    {
      "source": "agents",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "trust",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "reporting",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "agents",
      "target": "trust",
      "value": 1
    },
    {
      "source": "agents",
      "target": "reporting",
      "value": 1
    },
    {
      "source": "reporting",
      "target": "trust",
      "value": 1
    },
    {
      "source": "healthcare",
      "target": "safety",
      "value": 1
    },
    {
      "source": "healthcare",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "healthcare",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "healthcare",
      "target": "liability",
      "value": 1
    },
    {
      "source": "liability",
      "target": "safety",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "risk-assessment",
      "value": 2
    },
    {
      "source": "liability",
      "target": "risk-assessment",
      "value": 2
    },
    {
      "source": "alignment",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "theory",
      "target": "transparency",
      "value": 2
    },
    {
      "source": "alignment",
      "target": "constraints",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "theory",
      "value": 2
    },
    {
      "source": "governance",
      "target": "theory",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "governance",
      "value": 1
    },
    {
      "source": "capability-elicitation",
      "target": "guide",
      "value": 1
    },
    {
      "source": "alignment",
      "target": "capability-elicitation",
      "value": 1
    },
    {
      "source": "policy",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "governance",
      "target": "risk-assessment",
      "value": 3
    },
    {
      "source": "alignment",
      "target": "governance",
      "value": 2
    },
    {
      "source": "cbrn",
      "target": "dual-use",
      "value": 1
    },
    {
      "source": "dual-use",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "dual-use",
      "target": "governance",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "cbrn",
      "target": "governance",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "liability",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "liability",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "risk-assessment",
      "value": 1
    },
    {
      "source": "governance",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "governance",
      "value": 2
    },
    {
      "source": "deployment",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "regulation",
      "value": 2
    },
    {
      "source": "deployment",
      "target": "policy",
      "value": 1
    },
    {
      "source": "governance",
      "target": "standards",
      "value": 2
    },
    {
      "source": "regulation",
      "target": "standards",
      "value": 3
    },
    {
      "source": "governance",
      "target": "interoperability",
      "value": 1
    },
    {
      "source": "interoperability",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "policy",
      "target": "standards",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "standards",
      "value": 1
    },
    {
      "source": "governance",
      "target": "institutional-design",
      "value": 2
    },
    {
      "source": "institutional-design",
      "target": "safety",
      "value": 2
    },
    {
      "source": "institutional-design",
      "target": "transparency",
      "value": 2
    },
    {
      "source": "impact assessment",
      "target": "risk governance",
      "value": 1
    },
    {
      "source": "impact assessment",
      "target": "implementation",
      "value": 1
    },
    {
      "source": "best practices",
      "target": "impact assessment",
      "value": 1
    },
    {
      "source": "implementation",
      "target": "risk governance",
      "value": 1
    },
    {
      "source": "best practices",
      "target": "risk governance",
      "value": 1
    },
    {
      "source": "best practices",
      "target": "implementation",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "risk assessment",
      "value": 1
    },
    {
      "source": "risk assessment",
      "target": "safety",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "risk assessment",
      "value": 1
    },
    {
      "source": "deployment",
      "target": "evaluation",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "safety",
      "value": 3
    },
    {
      "source": "data governance",
      "target": "training data",
      "value": 1
    },
    {
      "source": "data governance",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "data governance",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "privacy",
      "target": "training data",
      "value": 1
    },
    {
      "source": "training data",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "privacy",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "standards",
      "value": 1
    },
    {
      "source": "benchmarks",
      "target": "evaluation",
      "value": 1
    },
    {
      "source": "benchmarks",
      "target": "standards",
      "value": 1
    },
    {
      "source": "benchmarks",
      "target": "safety",
      "value": 1
    },
    {
      "source": "red teaming",
      "target": "security",
      "value": 1
    },
    {
      "source": "red teaming",
      "target": "safety",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "red teaming",
      "value": 1
    },
    {
      "source": "safety",
      "target": "security",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "security",
      "value": 1
    },
    {
      "source": "interpretability",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "governance",
      "target": "interpretability",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "interpretability",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "evaluation",
      "target": "governance",
      "value": 1
    },
    {
      "source": "content authentication",
      "target": "watermarking",
      "value": 1
    },
    {
      "source": "governance",
      "target": "watermarking",
      "value": 1
    },
    {
      "source": "content authentication",
      "target": "provenance",
      "value": 1
    },
    {
      "source": "governance",
      "target": "provenance",
      "value": 1
    },
    {
      "source": "content authentication",
      "target": "governance",
      "value": 1
    },
    {
      "source": "model weights",
      "target": "security",
      "value": 1
    },
    {
      "source": "infrastructure",
      "target": "security",
      "value": 1
    },
    {
      "source": "governance",
      "target": "security",
      "value": 1
    },
    {
      "source": "infrastructure",
      "target": "model weights",
      "value": 1
    },
    {
      "source": "governance",
      "target": "model weights",
      "value": 1
    },
    {
      "source": "governance",
      "target": "infrastructure",
      "value": 1
    },
    {
      "source": "api-controls",
      "target": "safety-mechanisms",
      "value": 1
    },
    {
      "source": "access-management",
      "target": "api-controls",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "api-controls",
      "value": 1
    },
    {
      "source": "api-controls",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "access-management",
      "target": "safety-mechanisms",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "safety-mechanisms",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "safety-mechanisms",
      "value": 1
    },
    {
      "source": "access-management",
      "target": "ai-governance",
      "value": 1
    },
    {
      "source": "access-management",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "abuse detection",
      "target": "rate limiting",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "rate limiting",
      "value": 1
    },
    {
      "source": "rate limiting",
      "target": "safety mechanisms",
      "value": 1
    },
    {
      "source": "rate limiting",
      "target": "trust and safety",
      "value": 1
    },
    {
      "source": "abuse detection",
      "target": "ai governance",
      "value": 1
    },
    {
      "source": "abuse detection",
      "target": "safety mechanisms",
      "value": 1
    },
    {
      "source": "abuse detection",
      "target": "trust and safety",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "safety mechanisms",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "trust and safety",
      "value": 1
    },
    {
      "source": "safety mechanisms",
      "target": "trust and safety",
      "value": 1
    },
    {
      "source": "ai-safety",
      "target": "monitoring",
      "value": 1
    },
    {
      "source": "governance",
      "target": "monitoring",
      "value": 2
    },
    {
      "source": "accountability",
      "target": "monitoring",
      "value": 1
    },
    {
      "source": "ai-safety",
      "target": "governance",
      "value": 2
    },
    {
      "source": "accountability",
      "target": "ai-safety",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "governance",
      "value": 1
    },
    {
      "source": "emergent behavior",
      "target": "post-deployment risks",
      "value": 1
    },
    {
      "source": "emergent behavior",
      "target": "governance",
      "value": 1
    },
    {
      "source": "emergent behavior",
      "target": "monitoring",
      "value": 1
    },
    {
      "source": "emergent behavior",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "governance",
      "target": "post-deployment risks",
      "value": 1
    },
    {
      "source": "monitoring",
      "target": "post-deployment risks",
      "value": 1
    },
    {
      "source": "post-deployment risks",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "governance",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "monitoring",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "rollback",
      "target": "versioning",
      "value": 1
    },
    {
      "source": "safety",
      "target": "versioning",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "versioning",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "versioning",
      "value": 1
    },
    {
      "source": "rollback",
      "target": "safety",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "rollback",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "rollback",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "safety",
      "value": 1
    },
    {
      "source": "ai-governance",
      "target": "safety",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "ai-governance",
      "value": 1
    },
    {
      "source": "governance",
      "target": "privacy",
      "value": 2
    },
    {
      "source": "ai safety",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "privacy",
      "target": "technical safeguards",
      "value": 1
    },
    {
      "source": "data security",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "governance",
      "value": 1
    },
    {
      "source": "governance",
      "target": "technical safeguards",
      "value": 1
    },
    {
      "source": "data security",
      "target": "governance",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "technical safeguards",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "data security",
      "value": 1
    },
    {
      "source": "data security",
      "target": "technical safeguards",
      "value": 1
    },
    {
      "source": "hardware",
      "target": "safety",
      "value": 1
    },
    {
      "source": "governance",
      "target": "hardware",
      "value": 1
    },
    {
      "source": "hardware",
      "target": "risk-mitigation",
      "value": 1
    },
    {
      "source": "risk-mitigation",
      "target": "safety",
      "value": 1
    },
    {
      "source": "governance",
      "target": "risk-mitigation",
      "value": 1
    },
    {
      "source": "governance",
      "target": "self-modification",
      "value": 1
    },
    {
      "source": "ai constraints",
      "target": "governance",
      "value": 1
    },
    {
      "source": "safety",
      "target": "self-modification",
      "value": 1
    },
    {
      "source": "ai constraints",
      "target": "safety",
      "value": 1
    },
    {
      "source": "ai constraints",
      "target": "self-modification",
      "value": 1
    },
    {
      "source": "ai-models",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "use-based-regulation",
      "value": 1
    },
    {
      "source": "ai-models",
      "target": "policy",
      "value": 1
    },
    {
      "source": "policy",
      "target": "use-based-regulation",
      "value": 1
    },
    {
      "source": "ai-models",
      "target": "governance",
      "value": 1
    },
    {
      "source": "governance",
      "target": "use-based-regulation",
      "value": 1
    },
    {
      "source": "ai-models",
      "target": "use-based-regulation",
      "value": 1
    },
    {
      "source": "human-in-the-loop",
      "target": "oversight",
      "value": 1
    },
    {
      "source": "automation",
      "target": "oversight",
      "value": 1
    },
    {
      "source": "governance",
      "target": "oversight",
      "value": 2
    },
    {
      "source": "oversight",
      "target": "paradox",
      "value": 1
    },
    {
      "source": "automation",
      "target": "human-in-the-loop",
      "value": 1
    },
    {
      "source": "governance",
      "target": "human-in-the-loop",
      "value": 1
    },
    {
      "source": "human-in-the-loop",
      "target": "paradox",
      "value": 1
    },
    {
      "source": "automation",
      "target": "governance",
      "value": 1
    },
    {
      "source": "automation",
      "target": "paradox",
      "value": 1
    },
    {
      "source": "governance",
      "target": "paradox",
      "value": 1
    },
    {
      "source": "ai-safety",
      "target": "worldviews",
      "value": 1
    },
    {
      "source": "ai-safety",
      "target": "alignment",
      "value": 1
    },
    {
      "source": "ai-safety",
      "target": "methodology",
      "value": 1
    },
    {
      "source": "alignment",
      "target": "worldviews",
      "value": 1
    },
    {
      "source": "governance",
      "target": "worldviews",
      "value": 1
    },
    {
      "source": "methodology",
      "target": "worldviews",
      "value": 1
    },
    {
      "source": "alignment",
      "target": "methodology",
      "value": 1
    },
    {
      "source": "governance",
      "target": "methodology",
      "value": 1
    },
    {
      "source": "attention",
      "target": "oversight",
      "value": 1
    },
    {
      "source": "attention",
      "target": "interface-design",
      "value": 1
    },
    {
      "source": "attention",
      "target": "human-factors",
      "value": 1
    },
    {
      "source": "attention",
      "target": "governance",
      "value": 1
    },
    {
      "source": "interface-design",
      "target": "oversight",
      "value": 1
    },
    {
      "source": "human-factors",
      "target": "oversight",
      "value": 1
    },
    {
      "source": "human-factors",
      "target": "interface-design",
      "value": 1
    },
    {
      "source": "governance",
      "target": "interface-design",
      "value": 1
    },
    {
      "source": "governance",
      "target": "human-factors",
      "value": 1
    },
    {
      "source": "international",
      "target": "internet-governance",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "internet-governance",
      "value": 1
    },
    {
      "source": "internet-governance",
      "target": "multistakeholder",
      "value": 1
    },
    {
      "source": "history",
      "target": "internet-governance",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "international",
      "value": 1
    },
    {
      "source": "international",
      "target": "multistakeholder",
      "value": 1
    },
    {
      "source": "history",
      "target": "international",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "multistakeholder",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "history",
      "value": 1
    },
    {
      "source": "history",
      "target": "multistakeholder",
      "value": 1
    },
    {
      "source": "game-theory",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "collective-action",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "transparency",
      "value": 1
    },
    {
      "source": "collective-action",
      "target": "game-theory",
      "value": 1
    },
    {
      "source": "disclosure",
      "target": "game-theory",
      "value": 1
    },
    {
      "source": "collective-action",
      "target": "disclosure",
      "value": 1
    },
    {
      "source": "collective-action",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "disclosure",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "failures",
      "value": 1
    },
    {
      "source": "case-studies",
      "target": "constraints",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "implementation",
      "value": 2
    },
    {
      "source": "constraints",
      "target": "lessons",
      "value": 1
    },
    {
      "source": "case-studies",
      "target": "failures",
      "value": 1
    },
    {
      "source": "failures",
      "target": "implementation",
      "value": 1
    },
    {
      "source": "failures",
      "target": "lessons",
      "value": 1
    },
    {
      "source": "case-studies",
      "target": "implementation",
      "value": 1
    },
    {
      "source": "case-studies",
      "target": "lessons",
      "value": 1
    },
    {
      "source": "implementation",
      "target": "lessons",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "semantics",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "natural-language",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "formal-verification",
      "value": 1
    },
    {
      "source": "natural-language",
      "target": "semantics",
      "value": 1
    },
    {
      "source": "formal-verification",
      "target": "semantics",
      "value": 1
    },
    {
      "source": "implementation",
      "target": "semantics",
      "value": 1
    },
    {
      "source": "formal-verification",
      "target": "natural-language",
      "value": 1
    },
    {
      "source": "implementation",
      "target": "natural-language",
      "value": 1
    },
    {
      "source": "formal-verification",
      "target": "implementation",
      "value": 1
    },
    {
      "source": "democracy",
      "target": "legitimacy",
      "value": 1
    },
    {
      "source": "democracy",
      "target": "refusals",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "democracy",
      "value": 1
    },
    {
      "source": "democracy",
      "target": "participation",
      "value": 1
    },
    {
      "source": "legitimacy",
      "target": "refusals",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "legitimacy",
      "value": 1
    },
    {
      "source": "legitimacy",
      "target": "participation",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "refusals",
      "value": 1
    },
    {
      "source": "participation",
      "target": "refusals",
      "value": 1
    },
    {
      "source": "constraints",
      "target": "participation",
      "value": 1
    },
    {
      "source": "law",
      "target": "liability",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "liability",
      "value": 1
    },
    {
      "source": "harms",
      "target": "liability",
      "value": 1
    },
    {
      "source": "legal-frameworks",
      "target": "liability",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "law",
      "value": 1
    },
    {
      "source": "harms",
      "target": "law",
      "value": 1
    },
    {
      "source": "law",
      "target": "legal-frameworks",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "harms",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "legal-frameworks",
      "value": 1
    },
    {
      "source": "harms",
      "target": "legal-frameworks",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "simulation",
      "value": 1
    },
    {
      "source": "policy-testing",
      "target": "simulation",
      "value": 1
    },
    {
      "source": "modeling",
      "target": "simulation",
      "value": 1
    },
    {
      "source": "simulation",
      "target": "unintended-consequences",
      "value": 1
    },
    {
      "source": "policy-testing",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "modeling",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "unintended-consequences",
      "value": 1
    },
    {
      "source": "modeling",
      "target": "policy-testing",
      "value": 1
    },
    {
      "source": "policy-testing",
      "target": "unintended-consequences",
      "value": 1
    },
    {
      "source": "modeling",
      "target": "unintended-consequences",
      "value": 1
    },
    {
      "source": "researcher-wellbeing",
      "target": "sustainability",
      "value": 1
    },
    {
      "source": "culture",
      "target": "researcher-wellbeing",
      "value": 1
    },
    {
      "source": "institutions",
      "target": "researcher-wellbeing",
      "value": 1
    },
    {
      "source": "meta",
      "target": "researcher-wellbeing",
      "value": 1
    },
    {
      "source": "culture",
      "target": "sustainability",
      "value": 1
    },
    {
      "source": "institutions",
      "target": "sustainability",
      "value": 1
    },
    {
      "source": "meta",
      "target": "sustainability",
      "value": 1
    },
    {
      "source": "culture",
      "target": "institutions",
      "value": 1
    },
    {
      "source": "culture",
      "target": "meta",
      "value": 1
    },
    {
      "source": "institutions",
      "target": "meta",
      "value": 1
    },
    {
      "source": "epistemic-humility",
      "target": "predictions",
      "value": 1
    },
    {
      "source": "history",
      "target": "predictions",
      "value": 1
    },
    {
      "source": "predictions",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "expert-judgment",
      "target": "predictions",
      "value": 1
    },
    {
      "source": "epistemic-humility",
      "target": "history",
      "value": 1
    },
    {
      "source": "epistemic-humility",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "epistemic-humility",
      "target": "expert-judgment",
      "value": 1
    },
    {
      "source": "history",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "expert-judgment",
      "target": "history",
      "value": 1
    },
    {
      "source": "expert-judgment",
      "target": "uncertainty",
      "value": 1
    },
    {
      "source": "market-structure",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "regulation",
      "target": "small-actors",
      "value": 1
    },
    {
      "source": "competition",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "power-concentration",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "market-structure",
      "target": "small-actors",
      "value": 1
    },
    {
      "source": "competition",
      "target": "market-structure",
      "value": 1
    },
    {
      "source": "market-structure",
      "target": "power-concentration",
      "value": 1
    },
    {
      "source": "competition",
      "target": "small-actors",
      "value": 1
    },
    {
      "source": "power-concentration",
      "target": "small-actors",
      "value": 1
    },
    {
      "source": "competition",
      "target": "power-concentration",
      "value": 1
    },
    {
      "source": "global-south",
      "target": "international",
      "value": 1
    },
    {
      "source": "development",
      "target": "global-south",
      "value": 1
    },
    {
      "source": "context",
      "target": "global-south",
      "value": 1
    },
    {
      "source": "global-south",
      "target": "priorities",
      "value": 1
    },
    {
      "source": "development",
      "target": "international",
      "value": 1
    },
    {
      "source": "context",
      "target": "international",
      "value": 1
    },
    {
      "source": "international",
      "target": "priorities",
      "value": 1
    },
    {
      "source": "context",
      "target": "development",
      "value": 1
    },
    {
      "source": "development",
      "target": "priorities",
      "value": 1
    },
    {
      "source": "context",
      "target": "priorities",
      "value": 1
    },
    {
      "source": "safety",
      "target": "speed",
      "value": 1
    },
    {
      "source": "speed",
      "target": "tradeoffs",
      "value": 1
    },
    {
      "source": "development",
      "target": "speed",
      "value": 1
    },
    {
      "source": "decision-making",
      "target": "speed",
      "value": 1
    },
    {
      "source": "safety",
      "target": "tradeoffs",
      "value": 1
    },
    {
      "source": "development",
      "target": "safety",
      "value": 1
    },
    {
      "source": "decision-making",
      "target": "safety",
      "value": 1
    },
    {
      "source": "development",
      "target": "tradeoffs",
      "value": 1
    },
    {
      "source": "decision-making",
      "target": "tradeoffs",
      "value": 1
    },
    {
      "source": "decision-making",
      "target": "development",
      "value": 1
    },
    {
      "source": "economics",
      "target": "funding",
      "value": 1
    },
    {
      "source": "economics",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "economics",
      "target": "safety",
      "value": 1
    },
    {
      "source": "economics",
      "target": "investment",
      "value": 1
    },
    {
      "source": "funding",
      "target": "incentives",
      "value": 1
    },
    {
      "source": "funding",
      "target": "safety",
      "value": 1
    },
    {
      "source": "funding",
      "target": "investment",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "safety",
      "value": 1
    },
    {
      "source": "incentives",
      "target": "investment",
      "value": 1
    },
    {
      "source": "investment",
      "target": "safety",
      "value": 1
    },
    {
      "source": "trust",
      "target": "users",
      "value": 1
    },
    {
      "source": "calibration",
      "target": "trust",
      "value": 1
    },
    {
      "source": "reliability",
      "target": "trust",
      "value": 1
    },
    {
      "source": "education",
      "target": "trust",
      "value": 1
    },
    {
      "source": "calibration",
      "target": "users",
      "value": 1
    },
    {
      "source": "reliability",
      "target": "users",
      "value": 1
    },
    {
      "source": "education",
      "target": "users",
      "value": 1
    },
    {
      "source": "calibration",
      "target": "reliability",
      "value": 1
    },
    {
      "source": "calibration",
      "target": "education",
      "value": 1
    },
    {
      "source": "education",
      "target": "reliability",
      "value": 1
    },
    {
      "source": "children",
      "target": "minors",
      "value": 1
    },
    {
      "source": "children",
      "target": "development",
      "value": 1
    },
    {
      "source": "children",
      "target": "protection",
      "value": 1
    },
    {
      "source": "children",
      "target": "education",
      "value": 1
    },
    {
      "source": "development",
      "target": "minors",
      "value": 1
    },
    {
      "source": "minors",
      "target": "protection",
      "value": 1
    },
    {
      "source": "education",
      "target": "minors",
      "value": 1
    },
    {
      "source": "development",
      "target": "protection",
      "value": 1
    },
    {
      "source": "development",
      "target": "education",
      "value": 1
    },
    {
      "source": "education",
      "target": "protection",
      "value": 1
    },
    {
      "source": "emotions",
      "target": "psychology",
      "value": 1
    },
    {
      "source": "psychology",
      "target": "relationships",
      "value": 1
    },
    {
      "source": "psychology",
      "target": "wellbeing",
      "value": 1
    },
    {
      "source": "companionship",
      "target": "psychology",
      "value": 1
    },
    {
      "source": "emotions",
      "target": "relationships",
      "value": 1
    },
    {
      "source": "emotions",
      "target": "wellbeing",
      "value": 1
    },
    {
      "source": "companionship",
      "target": "emotions",
      "value": 1
    },
    {
      "source": "relationships",
      "target": "wellbeing",
      "value": 1
    },
    {
      "source": "companionship",
      "target": "relationships",
      "value": 1
    },
    {
      "source": "companionship",
      "target": "wellbeing",
      "value": 1
    },
    {
      "source": "fragmentation",
      "target": "frameworks",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "fragmentation",
      "value": 1
    },
    {
      "source": "fragmentation",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "fragmentation",
      "target": "synthesis",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "frameworks",
      "value": 1
    },
    {
      "source": "frameworks",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "frameworks",
      "target": "synthesis",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "meta-governance",
      "value": 1
    },
    {
      "source": "coordination",
      "target": "synthesis",
      "value": 1
    },
    {
      "source": "meta-governance",
      "target": "synthesis",
      "value": 1
    },
    {
      "source": "policy",
      "target": "research",
      "value": 1
    },
    {
      "source": "ai-focused",
      "target": "research",
      "value": 1
    },
    {
      "source": "ai-focused",
      "target": "policy",
      "value": 1
    },
    {
      "source": "climate modeling",
      "target": "validation",
      "value": 1
    },
    {
      "source": "standards",
      "target": "validation",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "validation",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "validation",
      "value": 1
    },
    {
      "source": "climate modeling",
      "target": "standards",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "climate modeling",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "climate modeling",
      "value": 1
    },
    {
      "source": "ai safety",
      "target": "standards",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "standards",
      "value": 1
    },
    {
      "source": "accountability",
      "target": "ai safety",
      "value": 1
    },
    {
      "source": "personalization",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "governance",
      "target": "personalization",
      "value": 1
    },
    {
      "source": "education",
      "target": "personalization",
      "value": 1
    },
    {
      "source": "data-protection",
      "target": "personalization",
      "value": 1
    },
    {
      "source": "education",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "data-protection",
      "target": "privacy",
      "value": 1
    },
    {
      "source": "education",
      "target": "governance",
      "value": 1
    },
    {
      "source": "data-protection",
      "target": "governance",
      "value": 1
    },
    {
      "source": "data-protection",
      "target": "education",
      "value": 1
    },
    {
      "source": "agi",
      "target": "ai governance",
      "value": 1
    },
    {
      "source": "agi",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "agi",
      "target": "safety",
      "value": 1
    },
    {
      "source": "agi",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "safety",
      "value": 1
    },
    {
      "source": "ai governance",
      "target": "reflexive ai",
      "value": 1
    },
    {
      "source": "reflexive ai",
      "target": "regulation",
      "value": 1
    },
    {
      "source": "reflexive ai",
      "target": "safety",
      "value": 1
    },
    {
      "source": "multi-agent systems",
      "target": "research",
      "value": 1
    },
    {
      "source": "ai coordination",
      "target": "research",
      "value": 1
    },
    {
      "source": "research",
      "target": "safety",
      "value": 1
    },
    {
      "source": "failure modes",
      "target": "research",
      "value": 1
    },
    {
      "source": "ai coordination",
      "target": "multi-agent systems",
      "value": 1
    },
    {
      "source": "multi-agent systems",
      "target": "safety",
      "value": 1
    },
    {
      "source": "failure modes",
      "target": "multi-agent systems",
      "value": 1
    },
    {
      "source": "ai coordination",
      "target": "safety",
      "value": 1
    },
    {
      "source": "ai coordination",
      "target": "failure modes",
      "value": 1
    },
    {
      "source": "failure modes",
      "target": "safety",
      "value": 1
    }
  ]
}